{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import TransformerMixin\n",
    "from pymystem3 import Mystem\n",
    "from gensim.models import KeyedVectors\n",
    "import re\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/e.zholkovskiy/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import string\n",
    "import tempfile\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import tf_metrics\n",
    "from tensorflow.python.keras.preprocessing import sequence\n",
    "from tensorboard import summary as summary_lib\n",
    "\n",
    "# tf.logging.set_verbosity(tf.logging.INFO)\n",
    "# print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from vectorizer import Vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Работа с данными (1 балл)\n",
    "\n",
    "Загрузите датасет, с которым вы работали во время соревнования на kaggle. Преобразуйте его в формат, удобный для обучения модели. В качетсве фичей используйте эмбединги слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(to_subm=False, data_dir='../hw1/data/'):\n",
    "    train_data = pd.read_csv(os.path.join(data_dir, 'train.csv'), index_col='id')\n",
    "    test_data = pd.read_csv(os.path.join(data_dir, 'test.csv'), index_col='id')\n",
    "    y = train_data.values[:, 1].astype(int)\n",
    "    x = train_data.values[:, 0]\n",
    "    x_test = test_data.values[:, 0]\n",
    "\n",
    "    x_tokens = np.array(json.load(open(os.path.join(data_dir, 'train_tokens.json'), 'rb')))\n",
    "    x_tokens_test = np.array(json.load(open(os.path.join(data_dir, 'test_tokens.json'), 'rb')))\n",
    "\n",
    "    train_idx, val_idx = train_test_split(np.arange(len(x)), train_size=0.8, random_state=0)\n",
    "    if to_subm:\n",
    "        train_idx = np.arange(len(x))  # for final submit\n",
    "\n",
    "    return (x[train_idx], x[val_idx], x_test), \\\n",
    "           (x_tokens[train_idx], x_tokens[val_idx], x_tokens_test), \\\n",
    "           (y[train_idx], y[val_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50000\n",
    "embedding_size = 1000\n",
    "vect = Vectorizer('../hw1/data/word2vec.bin', vocab_size=vocab_size, max_len=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/e.zholkovskiy/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3be99dceaf904befad01389087ade05f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=89973), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca6c2b40cb964e789592dec624d04e28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=22494), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "(x_train, x_val, x_test), (x_tokens_train, x_tokens_val, x_tokens_test), (y_train, y_val) = get_data(0)\n",
    "x_train, x_len_train = vect.fit_transform(tqdm(x_train))\n",
    "x_val, x_len_val = vect.transform(tqdm(x_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parser(x, length, y):\n",
    "    features = {\"x\": x, \"len\": length}\n",
    "    return features, y\n",
    "\n",
    "def train_input_fn(params):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x_train, x_len_train, y_train))\n",
    "    dataset = dataset.shuffle(buffer_size=len(x_train))\n",
    "    dataset = dataset.batch(params['batch_size'])\n",
    "    dataset = dataset.map(parser)\n",
    "    dataset = dataset.repeat()\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    return iterator.get_next()\n",
    "\n",
    "def eval_input_fn(params):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x_val, x_len_val, y_val))\n",
    "    dataset = dataset.batch(params['batch_size'])\n",
    "    dataset = dataset.map(parser)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    return iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn(features, labels, mode, params):   \n",
    "    # print ('mode: {}'.format(mode))\n",
    "    logits = params['forward'](features)\n",
    "    predicted_classes = tf.argmax(logits, 1)\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        predictions = {\n",
    "            'class_ids': predicted_classes[:, tf.newaxis],\n",
    "            'probabilities': tf.nn.softmax(logits),\n",
    "            'logits': logits,\n",
    "        }\n",
    "        return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
    "\n",
    "    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "    \n",
    "    accuracy = tf.metrics.accuracy(labels=labels,\n",
    "                               predictions=predicted_classes,\n",
    "                               name='acc_op')\n",
    "    f1 = tf_metrics.f1(labels, predicted_classes, 3, average='macro')\n",
    "    metrics = {\n",
    "        'accuracy': accuracy,\n",
    "        'f1': f1\n",
    "    }\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.EVAL:\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "            mode, loss=loss, eval_metric_ops=metrics)\n",
    "    \n",
    "    tf.summary.scalar('accuracy', accuracy[1])\n",
    "    tf.summary.scalar('f1', f1[1])\n",
    "    \n",
    "    optimizer = tf.train.AdagradOptimizer(learning_rate=0.3)\n",
    "    train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())\n",
    "    \n",
    "    return tf.estimator.EstimatorSpec(\n",
    "        mode, loss=loss, train_op=train_op, eval_metric_ops=metrics)\n",
    " \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dir = './models/'\n",
    "logs_dir = './logs/'\n",
    "\n",
    "def train_model(forward_fn, name, epochs=10):\n",
    "    params = {\n",
    "        'batch_size': 1000,\n",
    "        'train_size': x_train.shape[0],\n",
    "        'val_size': x_test.shape[0],\n",
    "        'num_epochs': epochs,\n",
    "        'forward': forward_fn\n",
    "    }\n",
    "    model_dir = os.path.join(models_dir, name)\n",
    "    log_dir = os.path.join(logs_dir, name)\n",
    "    logger = logging.getLogger('tensorflow')\n",
    "    logger.handlers = []\n",
    "\n",
    "    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "    log_file = os.path.join(logs_dir, '{}.log'.format(name))\n",
    "    print ('log output: {}'.format(log_file))\n",
    "    \n",
    "    fh = logging.FileHandler(log_file)\n",
    "    fh.setLevel(logging.DEBUG)\n",
    "    fh.setFormatter(formatter)\n",
    "    logger.addHandler(fh)\n",
    "\n",
    "    sh = logging.StreamHandler(sys.stdout)\n",
    "    sh.setLevel(logging.CRITICAL)\n",
    "    logger.addHandler(sh)\n",
    "    \n",
    "    config = tf.estimator.RunConfig(\n",
    "        save_checkpoints_steps = int(params['train_size'] / params['batch_size']),\n",
    "        model_dir = model_dir\n",
    "    )\n",
    "    classifier = tf.estimator.Estimator(\n",
    "        model_fn = model_fn,\n",
    "        params = params,\n",
    "        config = config,\n",
    "    )\n",
    "    \n",
    "    train_spec = tf.estimator.TrainSpec(\n",
    "        input_fn = lambda: train_input_fn(params), \n",
    "        max_steps=int(params['train_size'] / params['batch_size'] * params['num_epochs'])\n",
    "    )\n",
    "\n",
    "    eval_spec = tf.estimator.EvalSpec(\n",
    "        input_fn = lambda: eval_input_fn(params),\n",
    "        steps = int(params['val_size'] / params['batch_size']),\n",
    "        throttle_secs = 0, \n",
    "    )\n",
    "\n",
    "    val_scores = tf.estimator.train_and_evaluate(\n",
    "        classifier, \n",
    "        train_spec, \n",
    "        eval_spec\n",
    "    )\n",
    "    val_scores = val_scores[0]\n",
    "    if val_scores:\n",
    "        print('Val metrics: Accuracy: {:.3}, f1: {:.3}, Loss: {:.3}'.format(\n",
    "            val_scores['accuracy'],\n",
    "            val_scores['f1'],\n",
    "            val_scores['loss']\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN model (1 балл)\n",
    "\n",
    "Реализуйте модель со следующими слоями, идущими в указанном порядке:\n",
    "- Conv1D, activation='relu'\n",
    "- MaxPooling1D\n",
    "- Dense\n",
    "\n",
    "Параметры модели подберите сами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_cnn(features):\n",
    "    inputs = tf.contrib.layers.embed_sequence(\n",
    "        features['x'], vocab_size, embedding_size,\n",
    "        #initializer=tf.random_uniform_initializer(-1.0, 1.0)\n",
    "        initializer=tf.constant_initializer(vect.get_embeddings()), trainable=False # pretrained embeddings\n",
    "    )\n",
    "    out = tf.layers.conv1d(\n",
    "        inputs=inputs,\n",
    "        filters = 32,\n",
    "        kernel_size = 3,\n",
    "        padding=\"same\",\n",
    "        activation=tf.nn.relu\n",
    "    )\n",
    "\n",
    "    # Global Max Pooling\n",
    "    out = tf.reduce_max(input_tensor=out, axis=1)\n",
    "    #out = tf.expand_dims(out, 1)\n",
    "    \n",
    "    logits = tf.layers.dense(inputs=out, units=3)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log output: ./logs/cnn.log\n",
      "Val metrics: Accuracy: 0.854, f1: 0.81, Loss: 0.334\n",
      "CPU times: user 1min 18s, sys: 12.3 s, total: 1min 30s\n",
      "Wall time: 1min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_model(forward_cnn, 'cnn'.format(embedding_size), epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN model (1 балл)\n",
    "\n",
    "Реализуйте модель со следующими слоями, идущими в указанном порядке:\n",
    "- RNN\n",
    "- Dense\n",
    "\n",
    "Параметры модели подберите сами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_rnn(features):\n",
    "    inputs = tf.contrib.layers.embed_sequence(\n",
    "        features['x'], vocab_size, embedding_size,\n",
    "        #initializer=tf.random_uniform_initializer(-1.0, 1.0)\n",
    "        initializer=tf.constant_initializer(vect.get_embeddings()), trainable=False # pretrained embeddings\n",
    "    )\n",
    "    \n",
    "    rnn_cell = tf.nn.rnn_cell.BasicRNNCell(10)\n",
    "\n",
    "    _, final_states = tf.nn.dynamic_rnn(\n",
    "        rnn_cell, inputs, sequence_length=features['len'], dtype=tf.float32)\n",
    "    \n",
    "    logits = tf.layers.dense(inputs=final_states, units=3)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log output: ./logs/rnn.log\n",
      "Val metrics: Accuracy: 0.798, f1: 0.678, Loss: 0.449\n",
      "CPU times: user 2min 48s, sys: 17.7 s, total: 3min 5s\n",
      "Wall time: 1min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_model(forward_rnn, 'rnn'.format(embedding_size), epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM model (1 балл)\n",
    "\n",
    "Реализуйте модель со следующими слоями, идущими в указанном порядке:\n",
    "- LSTM\n",
    "- Dense\n",
    "\n",
    "Параметры модели подберите сами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_lstm(features):\n",
    "    inputs = tf.contrib.layers.embed_sequence(\n",
    "        features['x'], vocab_size, embedding_size,\n",
    "        #initializer=tf.random_uniform_initializer(-1.0, 1.0)\n",
    "        initializer=tf.constant_initializer(vect.get_embeddings()), trainable=False # pretrained embeddings\n",
    "    )\n",
    "    \n",
    "    lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(10)\n",
    "\n",
    "    _, final_states = tf.nn.dynamic_rnn(\n",
    "        lstm_cell, inputs, sequence_length=features['len'], dtype=tf.float32)\n",
    "    out = final_states.h\n",
    "    \n",
    "    logits = tf.layers.dense(inputs=out, units=3)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log output: ./logs/lstm.log\n",
      "Val metrics: Accuracy: 0.836, f1: 0.774, Loss: 0.372\n",
      "CPU times: user 3min 29s, sys: 21.5 s, total: 3min 50s\n",
      "Wall time: 2min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_model(forward_lstm, 'lstm'.format(embedding_size), epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сравните все три реализованные модели по времени и по качеству классификации. Какая лучше? Как думаете, почему?\n",
    "\n",
    "<b>Вывод:</b>\n",
    "\n",
    "Время обучения:\n",
    "- cnn: 1min 25s\n",
    "- rnn: 1min 52s\n",
    "- lstm: 2min 9s\n",
    "\n",
    "Рекурентные сети работают медленнее из-за большого количества последовательных операций (плохо параллелить). lstm работает лучше rnn из-за более сложной архитектуры. lstm и cnn работают примерно одинаково, следовательно имеет смысл использовать cnn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline model (2 балла)\n",
    "\n",
    "Реализуйте модель со следующими слоями, идущими в указанном порядке:\n",
    "- Conv1D, activation='relu'\n",
    "- MaxPooling1D\n",
    "- RNN\n",
    "- Dense\n",
    "\n",
    "Параметры для Conv1D, MaxPooling1D и RNN подберите сами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_baseline(features):\n",
    "    inputs = tf.contrib.layers.embed_sequence(\n",
    "        features['x'], vocab_size, embedding_size,\n",
    "        #initializer=tf.random_uniform_initializer(-1.0, 1.0)\n",
    "        initializer=tf.constant_initializer(vect.get_embeddings()), trainable=False # pretrained embeddings\n",
    "    )\n",
    "    out = tf.layers.conv1d(\n",
    "        inputs=inputs,\n",
    "        filters = 32,\n",
    "        kernel_size = 3,\n",
    "        padding=\"same\",\n",
    "        activation=tf.nn.relu\n",
    "    )\n",
    "    # Global Max Pooling\n",
    "    out = tf.reduce_max(input_tensor=inputs, axis=1)\n",
    "    out = tf.expand_dims(out, 1)\n",
    "    \n",
    "    rnn_cell = tf.nn.rnn_cell.BasicRNNCell(10)\n",
    "\n",
    "    _, final_states = tf.nn.dynamic_rnn(\n",
    "        rnn_cell, inputs, sequence_length=features['len'], dtype=tf.float32)\n",
    "    out = final_states\n",
    "    \n",
    "    logits = tf.layers.dense(inputs=out, units=3)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log output: ./logs/baseline.log\n",
      "Val metrics: Accuracy: 0.795, f1: 0.678, Loss: 0.445\n"
     ]
    }
   ],
   "source": [
    "train_model(forward_baseline, 'baseline'.format(embedding_size), epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout (1 балл)\n",
    "\n",
    "Добавьте dropout к baseline моделе. Пример модефицированной модели:\n",
    "- Dropout\n",
    "- Conv1D, activation='relu'\n",
    "- MaxPooling1D\n",
    "- RNN\n",
    "- Dense\n",
    "\n",
    "Подберите параметры."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_dropout(features):\n",
    "    inputs = tf.contrib.layers.embed_sequence(\n",
    "        features['x'], vocab_size, embedding_size,\n",
    "        #initializer=tf.random_uniform_initializer(-1.0, 1.0)\n",
    "        initializer=tf.constant_initializer(vect.get_embeddings()), trainable=False # pretrained embeddings\n",
    "    )\n",
    "    out = tf.layers.dropout(inputs)\n",
    "    out = tf.layers.conv1d(\n",
    "        inputs=out,\n",
    "        filters = 32,\n",
    "        kernel_size = 3,\n",
    "        padding=\"same\",\n",
    "        activation=tf.nn.relu\n",
    "    )\n",
    "    # Global Max Pooling\n",
    "    out = tf.reduce_max(input_tensor=inputs, axis=1)\n",
    "    out = tf.expand_dims(out, 1)\n",
    "    \n",
    "    rnn_cell = tf.nn.rnn_cell.BasicRNNCell(10)\n",
    "\n",
    "    _, final_states = tf.nn.dynamic_rnn(\n",
    "        rnn_cell, inputs, sequence_length=features['len'], dtype=tf.float32)\n",
    "    out = final_states\n",
    "    \n",
    "    logits = tf.layers.dense(inputs=out, units=3)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log output: ./logs/dropout.log\n",
      "Val metrics: Accuracy: 0.807, f1: 0.722, Loss: 0.435\n"
     ]
    }
   ],
   "source": [
    "train_model(forward_dropout, 'dropout'.format(embedding_size), epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Улучшило ли результат классификации использование Dropout? Как думаете, почему?\n",
    "\n",
    "<b>Вывод:</b>\n",
    "    \n",
    "Использование dropout улучшило результат, так как это хорошая регуляризация при обучении модели (помогает бороться с переобучением)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bidirectional (1 балл)\n",
    "\n",
    "Вместо RNN в первой модели используйте biRNN. Пример модефецированной модели:\n",
    "- Conv1D, activation='relu'\n",
    "- MaxPooling1D\n",
    "- biRNN\n",
    "- Dense\n",
    "\n",
    "Подберите параметры."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_birnn(features):\n",
    "    inputs = tf.contrib.layers.embed_sequence(\n",
    "        features['x'], vocab_size, embedding_size,\n",
    "        #initializer=tf.random_uniform_initializer(-1.0, 1.0)\n",
    "        initializer=tf.constant_initializer(vect.get_embeddings()), trainable=False # pretrained embeddings\n",
    "    )\n",
    "    \n",
    "    out = tf.layers.conv1d(\n",
    "        inputs=inputs,\n",
    "        filters = 32,\n",
    "        kernel_size = 3,\n",
    "        padding=\"same\",\n",
    "        activation=tf.nn.relu\n",
    "    )\n",
    "    \n",
    "    # Global Max Pooling\n",
    "    out = tf.reduce_max(input_tensor=inputs, axis=1)\n",
    "    out = tf.expand_dims(out, 1)\n",
    "        \n",
    "    rnn_cell_fw = tf.nn.rnn_cell.BasicRNNCell(10)\n",
    "    rnn_cell_bw = tf.nn.rnn_cell.BasicRNNCell(10)\n",
    "    \n",
    "    _, final_states = tf.nn.bidirectional_dynamic_rnn(rnn_cell_fw, rnn_cell_bw, out, \n",
    "                                                      sequence_length=features['len'], dtype=tf.float32)\n",
    "    out = tf.concat(final_states, 1)\n",
    "    logits = tf.layers.dense(inputs=out, units=3)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log output: ./logs/birnn.log\n",
      "Val metrics: Accuracy: 0.827, f1: 0.773, Loss: 0.39\n"
     ]
    }
   ],
   "source": [
    "train_model(forward_birnn, 'birnn'.format(embedding_size), epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дало ли буст использовании biRNN вместо RNN? Сильный? Как думаете, почему?\n",
    "\n",
    "<b>Вывод:</b>\n",
    "    \n",
    "Использование biRNN дало хороший прирост, так как это помогает \"видеть\" слова и смысл с обеих сторон (как и свертка)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom model (2 балла)\n",
    "\n",
    "Подберите архитектуру сети, используя сверточные слои и слои макспулинга и превзойдите качество полносвязной сети.\n",
    "\n",
    "Подберите архитектуру сети, используя следующие типы слоев:\n",
    "- Conv1D\n",
    "- MaxPooling1D\n",
    "- RNN\n",
    "- LSTM\n",
    "- GRU\n",
    "- Dropout\n",
    "- Dense\n",
    "\n",
    "Можно использовать любые из описанных слоев в любом количестве и в любом порядке. Не обязательно использовать все описанные слои. \n",
    "\n",
    "Настройте параметры. Превзойдите лучшее качество, полученное раннее. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_custom(features):\n",
    "    inputs = tf.contrib.layers.embed_sequence(\n",
    "        features['x'], vocab_size, embedding_size,\n",
    "        #initializer=tf.random_uniform_initializer(-1.0, 1.0)\n",
    "        initializer=tf.constant_initializer(vect.get_embeddings()), trainable=False # pretrained embeddings\n",
    "    )\n",
    "    out = tf.layers.conv1d(\n",
    "        inputs=inputs,\n",
    "        filters = 32,\n",
    "        kernel_size = 3,\n",
    "        padding=\"same\",\n",
    "        activation=tf.nn.relu\n",
    "    )\n",
    "    out = tf.layers.conv1d(\n",
    "        inputs=out,\n",
    "        filters = 32,\n",
    "        kernel_size = 3,\n",
    "        padding=\"same\",\n",
    "        activation=tf.nn.relu\n",
    "    )\n",
    "    # Global Max Pooling\n",
    "    out = tf.reduce_max(input_tensor=out, axis=1)\n",
    "    #out = tf.expand_dims(out, 1)\n",
    "    out = tf.layers.dropout(out, rate=0.3)\n",
    "    logits = tf.layers.dense(inputs=out, units=3)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log output: ./logs/custom_2.log\n",
      "Val metrics: Accuracy: 0.846, f1: 0.799, Loss: 0.35\n"
     ]
    }
   ],
   "source": [
    "train_model(forward_custom, 'custom_2'.format(embedding_size), epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
