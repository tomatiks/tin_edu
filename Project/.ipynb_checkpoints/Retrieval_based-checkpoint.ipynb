{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.0-rc1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import string\n",
    "import tempfile\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tensorflow.python.keras.preprocessing import sequence\n",
    "from tensorboard import summary as summary_lib\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem\n",
    "from tqdm import tqdm_notebook,tqdm_pandas,tqdm\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "import nltk\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import tf_metrics\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "from multiprocessing import cpu_count, Pool\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "TqdmDeprecationWarning: Please use `tqdm.pandas(...)` instead of `tqdm_pandas(tqdm, ...)`.\n"
     ]
    }
   ],
   "source": [
    "tqdm_pandas(tqdm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading file     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('pikabu.csv',sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Убрал 600 литров мусора в реликтовом лесу. Ты...</td>\n",
       "      <td>Спасибо.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Убрал 600 литров мусора в реликтовом лесу. Ты ...</td>\n",
       "      <td>Приедь к нам в Мурманск пожалуйста.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Убрал 600 литров мусора в реликтовом лесу. Ты ...</td>\n",
       "      <td>Тебе платят за это?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ты просто большой молодец. Спасибо. Тебе платя...</td>\n",
       "      <td>За посты на Пикабу? Да, платят. Достаточно при...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Спасибо. Тебе платят за это? За посты на Пикаб...</td>\n",
       "      <td>Не нажимается сук</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Ты просто большой молодец. Спасибо. Тебе платя...</td>\n",
       "      <td>Лично Путин</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Спасибо. Тебе платят за это? Лично Путин</td>\n",
       "      <td>дарт вейдар</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Убрал 600 литров мусора в реликтовом лесу. Ты...</td>\n",
       "      <td>ну всё, теперь можно не убирать за собой, Чист...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Убрал 600 литров мусора в реликтовом лесу. Ты ...</td>\n",
       "      <td>Зачем заминусили человека? Очевидная ирония же...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Убрал 600 литров мусора в реликтовом лесу.</td>\n",
       "      <td>Герой, про которого я бы читал комиксы от dc и...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Убрал 600 литров мусора в реликтовом лесу. Ге...</td>\n",
       "      <td>да хотя бы от bubble студии)) но чет не пишут))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Убрал 600 литров мусора в реликтовом лесу. Гер...</td>\n",
       "      <td>может силами пикабу родится комикс, хехе)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Герой, про которого я бы читал комиксы от dc и...</td>\n",
       "      <td>Зовите Чилика...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>да хотя бы от bubble студии)) но чет не пишут)...</td>\n",
       "      <td>/@Chiliktolik , ищем автора комиксов про чисто...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Герой, про которого я бы читал комиксы от dc и...</td>\n",
       "      <td>Только комикс и может родится. Нет бы выйти да...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Убрал 600 литров мусора в реликтовом лесу. Гер...</td>\n",
       "      <td>ты в маске убираешь или она только для деавтор...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Герой, про которого я бы читал комиксы от dc и...</td>\n",
       "      <td>маска только для фото</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>да хотя бы от bubble студии)) но чет не пишут)...</td>\n",
       "      <td>понятно. А так молодец).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Убрал 600 литров мусора в реликтовом лесу. Гер...</td>\n",
       "      <td>Реликтовый лес? G-unit не нашел случаем?)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Убрал 600 литров мусора в реликтовом лесу. Гер...</td>\n",
       "      <td>Нужно мерчендайз организовывать</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              context  \\\n",
       "0    Убрал 600 литров мусора в реликтовом лесу. Ты...   \n",
       "1   Убрал 600 литров мусора в реликтовом лесу. Ты ...   \n",
       "2   Убрал 600 литров мусора в реликтовом лесу. Ты ...   \n",
       "3   Ты просто большой молодец. Спасибо. Тебе платя...   \n",
       "4   Спасибо. Тебе платят за это? За посты на Пикаб...   \n",
       "5   Ты просто большой молодец. Спасибо. Тебе платя...   \n",
       "6            Спасибо. Тебе платят за это? Лично Путин   \n",
       "7    Убрал 600 литров мусора в реликтовом лесу. Ты...   \n",
       "8   Убрал 600 литров мусора в реликтовом лесу. Ты ...   \n",
       "9          Убрал 600 литров мусора в реликтовом лесу.   \n",
       "10   Убрал 600 литров мусора в реликтовом лесу. Ге...   \n",
       "11  Убрал 600 литров мусора в реликтовом лесу. Гер...   \n",
       "12  Герой, про которого я бы читал комиксы от dc и...   \n",
       "13  да хотя бы от bubble студии)) но чет не пишут)...   \n",
       "14  Герой, про которого я бы читал комиксы от dc и...   \n",
       "15  Убрал 600 литров мусора в реликтовом лесу. Гер...   \n",
       "16  Герой, про которого я бы читал комиксы от dc и...   \n",
       "17  да хотя бы от bubble студии)) но чет не пишут)...   \n",
       "18  Убрал 600 литров мусора в реликтовом лесу. Гер...   \n",
       "19  Убрал 600 литров мусора в реликтовом лесу. Гер...   \n",
       "\n",
       "                                               answer  \n",
       "0                                            Спасибо.  \n",
       "1                 Приедь к нам в Мурманск пожалуйста.  \n",
       "2                                 Тебе платят за это?  \n",
       "3   За посты на Пикабу? Да, платят. Достаточно при...  \n",
       "4                                   Не нажимается сук  \n",
       "5                                         Лично Путин  \n",
       "6                                         дарт вейдар  \n",
       "7   ну всё, теперь можно не убирать за собой, Чист...  \n",
       "8   Зачем заминусили человека? Очевидная ирония же...  \n",
       "9   Герой, про которого я бы читал комиксы от dc и...  \n",
       "10    да хотя бы от bubble студии)) но чет не пишут))  \n",
       "11          может силами пикабу родится комикс, хехе)  \n",
       "12                                   Зовите Чилика...  \n",
       "13  /@Chiliktolik , ищем автора комиксов про чисто...  \n",
       "14  Только комикс и может родится. Нет бы выйти да...  \n",
       "15  ты в маске убираешь или она только для деавтор...  \n",
       "16                              маска только для фото  \n",
       "17                           понятно. А так молодец).  \n",
       "18          Реликтовый лес? G-unit не нашел случаем?)  \n",
       "19                    Нужно мерчендайз организовывать  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "context    21486165\n",
       "answer     21486162\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_train.loc[df_train['context'].apply(len)>410,'context'][271713].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{4: 'убирать_VERB'}"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "sw_ru = set(stopwords.words('russian'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mystem = Mystem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = df_train['context'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_tokenizer(tokenizer, corpus):\n",
    "    #voc_set = set()\n",
    "    res = np.fromiter(map(tokenizer,tqdm_notebook(corpus[:20]),),dtype=str)\n",
    "    voc_set = np.unique(res)\n",
    "    print(voc_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fu(x):\n",
    "    return np.apply_along_axis(text_prep_tags,1,np.reshape(x,(-1,1)))\n",
    "        \n",
    "def parallelize(data,partitions):\n",
    "\n",
    "    data_split = np.array_split(data, partitions)\n",
    "    pool = Pool(partitions)\n",
    "    #data = np.stack(pool.apply(np.apply_along_axis,text_prep_tags,1,np.reshape(data_split,(-1,1))))\n",
    "    data = np.concatenate(pool.map(fu,data_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if type(6) is not str:\n",
    "        print('1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gsd_gdsg dgsdg dfdsg fdfs'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(r'([ _!?])+',r'\\1','gsd____gdsg   dgsdg dfdsg!     fdfs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_id_vocab = {}\n",
    "id_text_vocab = {}\n",
    "prev_word = None\n",
    "\n",
    "def text_prep_tags(text):\n",
    "   \n",
    "    #print(text)\n",
    "    text = text[0]\n",
    "    if type(text) is not str:\n",
    "        text= str(text)\n",
    "    upos_map = {'A':'ADJ','ADV':'ADV','ADVPRO':'ADV','ANUM':'ADJ','APRO':'DET','COM':'ADJ','CONJ':'SCON','INTJ':'INTJ','NONLEX':'X','NUM':'NUM','PART':'PART','PR':'ADP','S':'NOUN','SPRO':'PRON','UNKN':'X' ,'V':'VERB'}\n",
    "    text = text.lower()\n",
    "    result = np.array([])\n",
    "    \n",
    "    \n",
    "    # Убираем лишние символы\n",
    "    #text = re.sub(r'[;,]',r' ',text).strip()\n",
    "    text = re.sub(r'[^\\w\\s\\.]',r'',text).strip() \n",
    "    text = re.sub(r'([ _!?])+',r'\\1',text)\n",
    "    #text = [token.text for token in razdel.tokenize(text)]\n",
    "    # Делаем лемматизацию       \n",
    "#     text = [lemma for lemma in mystem.lemmatize(text) if not lemma.isspace() and lemma not in sw_ru\n",
    "#             and lemma.strip() not in ['.','..','...']]\n",
    "    \n",
    "#     if id_text_vocab != {}:\n",
    "#         prev_word = id_text_vocab[max(id_text_vocab.keys())]\n",
    "#     else:       \n",
    "#         prev_word = None\n",
    "        \n",
    "    for item in mystem.analyze(text):\n",
    "      #  print(item)\n",
    "        token = None\n",
    "        if item.get('analysis'):\n",
    "            lemma = item['analysis'][0]['lex']\n",
    "            pos = re.split('[=,]', item['analysis'][0]['gr'])[0]\n",
    "            #and lemma not in sw_ru\n",
    "            if not lemma.isspace()  and lemma.strip() not in ['.','..','...'] and lemma not in sw_ru:\n",
    "     \n",
    "                token = f'{lemma}_{upos_map[pos]}'\n",
    "        else:\n",
    "            lem_text = item[\"text\"]\n",
    "            if not lem_text.isspace() and lem_text.strip() not in ['.','..','...'] and lem_text not in sw_ru:\n",
    "            \n",
    "                token = f'{lem_text}_UNKN'\n",
    "            \n",
    "        if token:    \n",
    "            #result.append(token)\n",
    "            result = np.append(result,token)\n",
    "            \n",
    "#             if prev_word:\n",
    "#                 text_id_vocab[token] = text_id_vocab[prev_word] + 1\n",
    "#                 id_text_vocab[text_id_vocab[prev_word] + 1] = token\n",
    "#             else:\n",
    "#                 text_vocab[token] = 4\n",
    "#                 id_text_vocab[4] = token\n",
    "#             prev_word = token\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Лемматизатор неправильно разбивает слова с дефисом, поэтому исправляем это\n",
    "#     if '-' in text:\n",
    "#         for l in range(len(text)):\n",
    "#             if text[l] == '-':\n",
    "#                 text[l] = f'{text[l-1]}-{text[l+1]}'\n",
    "#                 text[l-1] = text[l+1] = text[l]\n",
    "    zer = np.empty([40], dtype=\"U25\")\n",
    "    zer[:] = '<PAD>'\n",
    "    size = result.shape[0]\n",
    "    if size>40:\n",
    "        zer = result[:40]\n",
    "    else:\n",
    "        zer[:size] = result\n",
    "        \n",
    "    return zer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.char2idx = {'<PAD>':0,'<START>':1,'<END>':2,'<UNK>':3}\n",
    "        self.idx2char = {i:ch for ch,i in self.char2idx.items()}\n",
    "    \n",
    "    def save(self, path='.'):\n",
    "        json_ch_idx = json.dumps(self.char2idx)\n",
    "        with open(os.path.join(path,\"char2idx.json\"),\"w\") as f:\n",
    "            f.write(json_ch_idx)\n",
    "        json_idx_ch = json.dumps(self.idx2char)\n",
    "        with open(os.path.join(path,\"idx2char.json\"),\"w\") as f:\n",
    "            f.write(json_idx_ch)\n",
    "    \n",
    "    def load(self, path='.'):       \n",
    "        with open(os.path.join(path,\"char2idx.json\"),\"r\") as f:\n",
    "            self.char2idx = json.loads(f.read())        \n",
    "        with open(os.path.join(path,\"idx2char.json\"),\"r\") as f:\n",
    "            self.idx2char = json.loads(f.read())\n",
    "        \n",
    "    \n",
    "    def indx_tokenize_from_files(self, huge_parts, name):\n",
    "        if not os.path.exists(f'{name}_indexed_token_files'):\n",
    "            os.mkdir(f'{name}_indexed_token_files')\n",
    "        for i in tqdm_notebook(range(huge_parts)):\n",
    "            with np.load(f'{name}_tokenized_files/np_tokens{i}.npz') as data:\n",
    "                np_idxs = np.apply_along_axis(np.vectorize(self.char2idx.get),1,data['arr'])\n",
    "                np.savez_compressed(f'{name}_indexed_token_files/indx_tokens{i}',arr=np_idxs)\n",
    "                print(f'{name} indx_tokenize part - {i}')\n",
    "                \n",
    "        \n",
    "    \n",
    "    def indx_detokenize(self, sequence):\n",
    "        return ''.join([self.idx2char[idx] for idx in sequence])\n",
    "    \n",
    "    def tokenize(self, data, tokenizer, huge_parts, name ,save_to_file=True):\n",
    "        huge_splits = np.array_split(data, huge_parts)\n",
    "        partitions = cpu_count()\n",
    "        for split_number,huge_split in enumerate(tqdm_notebook(huge_splits)):\n",
    "                      \n",
    "            data_split = np.array_split(huge_split, partitions)\n",
    "            pool = Pool(partitions)\n",
    "\n",
    "            res_data = np.concatenate(pool.map(tokenizer,data_split))\n",
    "            pool.close()\n",
    "            pool.join()\n",
    "\n",
    "            self._extend_vocab(res_data)\n",
    "            \n",
    "            print(f'{name} first tokenize part - {split_number}')\n",
    "            \n",
    "            if save_to_file:\n",
    "                if not os.path.exists(f'{name}_tokenized_files'):\n",
    "                    os.mkdir(f'{name}_tokenized_files')\n",
    "                np.savez_compressed(f'{name}_tokenized_files/np_tokens{split_number}',arr=res_data)             \n",
    "                del res_data\n",
    "                gc.collect()\n",
    "                \n",
    "                \n",
    "    def _extend_vocab(self,token_matrix):\n",
    "        start_idx = len(self.char2idx)\n",
    "        for idx,token in np.ndenumerate(np.unique(token_matrix)):\n",
    "            if token not in self.char2idx:\n",
    "                self.char2idx[token] = start_idx + idx[0]\n",
    "        \n",
    "        self.idx2char = {i:ch for ch,i in self.char2idx.items()}\n",
    "                \n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.char2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def help_token(x):\n",
    "    return np.apply_along_axis(text_prep_tags,1,np.reshape(x,(-1,1)))\n",
    "\n",
    "voc = Vocab()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbb42a11045342128468029baaa53d93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=20), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context first tokenize part - 0\n",
      "context first tokenize part - 1\n",
      "context first tokenize part - 2\n",
      "context first tokenize part - 3\n",
      "context first tokenize part - 4\n",
      "context first tokenize part - 5\n",
      "context first tokenize part - 6\n",
      "context first tokenize part - 7\n",
      "context first tokenize part - 8\n",
      "context first tokenize part - 9\n",
      "context first tokenize part - 10\n",
      "context first tokenize part - 11\n",
      "context first tokenize part - 12\n",
      "context first tokenize part - 13\n",
      "context first tokenize part - 14\n",
      "context first tokenize part - 15\n",
      "context first tokenize part - 16\n",
      "context first tokenize part - 17\n",
      "context first tokenize part - 18\n",
      "context first tokenize part - 19\n",
      "CPU times: user 12min 59s, sys: 2min 1s, total: 15min 1s\n",
      "Wall time: 1h 23min 17s\n"
     ]
    }
   ],
   "source": [
    "%time voc.tokenize(df_train['context'].values,help_token,20,name = 'context')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dfed6982a59474f9f25ace0e7b86f80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=20), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer first tokenize part - 0\n",
      "answer first tokenize part - 1\n",
      "answer first tokenize part - 2\n",
      "answer first tokenize part - 3\n",
      "answer first tokenize part - 4\n",
      "answer first tokenize part - 5\n",
      "answer first tokenize part - 6\n",
      "answer first tokenize part - 7\n",
      "answer first tokenize part - 8\n",
      "answer first tokenize part - 9\n",
      "answer first tokenize part - 10\n",
      "answer first tokenize part - 11\n",
      "answer first tokenize part - 12\n",
      "answer first tokenize part - 13\n",
      "answer first tokenize part - 14\n",
      "answer first tokenize part - 15\n",
      "answer first tokenize part - 16\n",
      "answer first tokenize part - 17\n",
      "answer first tokenize part - 18\n",
      "answer first tokenize part - 19\n",
      "CPU times: user 12min 19s, sys: 1min 52s, total: 14min 11s\n",
      "Wall time: 42min 43s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%time voc.tokenize(df_train['answer'].values,help_token,20,name = 'answer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5172f9b0d79449da3a055ca569deb9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=20), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context indx_tokenize part - 0\n",
      "context indx_tokenize part - 1\n",
      "context indx_tokenize part - 2\n",
      "context indx_tokenize part - 3\n",
      "context indx_tokenize part - 4\n",
      "context indx_tokenize part - 5\n",
      "context indx_tokenize part - 6\n",
      "context indx_tokenize part - 7\n",
      "context indx_tokenize part - 8\n",
      "context indx_tokenize part - 9\n",
      "context indx_tokenize part - 10\n",
      "context indx_tokenize part - 11\n",
      "context indx_tokenize part - 12\n",
      "context indx_tokenize part - 13\n",
      "context indx_tokenize part - 14\n",
      "context indx_tokenize part - 15\n",
      "context indx_tokenize part - 16\n",
      "context indx_tokenize part - 17\n",
      "context indx_tokenize part - 18\n",
      "context indx_tokenize part - 19\n",
      "CPU times: user 9min 3s, sys: 22.7 s, total: 9min 26s\n",
      "Wall time: 9min 28s\n"
     ]
    }
   ],
   "source": [
    "%time voc.indx_tokenize_from_files(20,name = 'context')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16a5f7fb47be4d3dac68fb24c0ca66a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=20), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer indx_tokenize part - 0\n",
      "answer indx_tokenize part - 1\n",
      "answer indx_tokenize part - 2\n",
      "answer indx_tokenize part - 3\n",
      "answer indx_tokenize part - 4\n",
      "answer indx_tokenize part - 5\n",
      "answer indx_tokenize part - 6\n",
      "answer indx_tokenize part - 7\n",
      "answer indx_tokenize part - 8\n",
      "answer indx_tokenize part - 9\n",
      "answer indx_tokenize part - 10\n",
      "answer indx_tokenize part - 11\n",
      "answer indx_tokenize part - 12\n",
      "answer indx_tokenize part - 13\n",
      "answer indx_tokenize part - 14\n",
      "answer indx_tokenize part - 15\n",
      "answer indx_tokenize part - 16\n",
      "answer indx_tokenize part - 17\n",
      "answer indx_tokenize part - 18\n",
      "answer indx_tokenize part - 19\n",
      "CPU times: user 8min 40s, sys: 21.8 s, total: 9min 2s\n",
      "Wall time: 9min 5s\n"
     ]
    }
   ],
   "source": [
    "%time voc.indx_tokenize_from_files(20,name = 'answer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['молодец_NOUN' 'самый_DET' 'начало_NOUN']\n",
      "['это_PRON' 'человек_NOUN' 'виноватый_ADJ']\n",
      "['беда_NOUN' 'гринлайт_NOUN' 'плюсонуть_VERB']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 99766, 149023, 105369],\n",
       "       [188210, 181013,  45919],\n",
       "       [ 36253,  56266, 126309]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def yy(x):\n",
    "    print(x)\n",
    "    return np.vectorize(voc1.char2idx.get)(x)\n",
    "np.apply_along_axis(yy,1,np.array([['молодец_NOUN', 'самый_DET' ,'начало_NOUN'],\n",
    " ['это_PRON' ,'человек_NOUN', 'виноватый_ADJ'],\n",
    " ['беда_NOUN', 'гринлайт_NOUN' ,'плюсонуть_VERB']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3341ef90e824ef0a14e27bf90c3918d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[169730   6934  91754 ...      4      4      4]\n",
      " [169730   6934  91754 ...      4      4      4]\n",
      " [169730   6934  91754 ...      4      4      4]\n",
      " ...\n",
      " [ 52516  99766  57344 ...      4      4      4]\n",
      " [ 52516  99766  57344 ...      4      4      4]\n",
      " [ 52516  99766  57344 ...      4      4      4]]\n",
      "[[ 99766 149023 105369 ...      4      4      4]\n",
      " [188210 181013  45919 ...      4      4      4]\n",
      " [ 36253  56266 126309 ...      4      4      4]\n",
      " ...\n",
      " [154435 114713  71694 ...      4      4      4]\n",
      " [ 71694  97254 114713 ...      4      4      4]\n",
      " [188210 182252 152074 ...      4      4      4]]\n",
      "[[152074 164183  67894 ...      4      4      4]\n",
      " [154435 114713  71694 ...      4      4      4]\n",
      " [154435 114713  71694 ...      4      4      4]\n",
      " ...\n",
      " [126392   4256 328320 ...      4      4      4]\n",
      " [ 37280 153558 149710 ...      4      4      4]\n",
      " [138822 353514 119067 ...      4      4      4]]\n",
      "[[110355 172004 407414 ...      4      4      4]\n",
      " [110355 172004 407414 ...      4      4      4]\n",
      " [110355 172004 407414 ...      4      4      4]\n",
      " ...\n",
      " [100444 150936 132021 ...      4      4      4]\n",
      " [172618  69450 180630 ...      4      4      4]\n",
      " [150861  44902 118985 ...      4      4      4]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "voc1.indx_tokenize_from_files(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2335478"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('<U140')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['answer'].values.astype(str).dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'res_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-f6f42e46f066>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#del answer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0mres_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'res_data' is not defined"
     ]
    }
   ],
   "source": [
    "#del answer\n",
    "gc.collect()\n",
    "del res_data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_arr = None\n",
    "for i in range(1):\n",
    "    with np.load(f'tokenized_files/np_tokens{i}.npz') as data:\n",
    "        if np_arr is not None:\n",
    "            np_arr = np.append(np_arr,data['arr'],axis=0)\n",
    "        else:\n",
    "            np_arr = data['arr']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_arr = None\n",
    "for i in range(1):\n",
    "    with np.load(f'indexed_token_files/indx_tokens{i}.npz') as data:\n",
    "        if np_arr is not None:\n",
    "            np_arr = np.append(np_arr,data['arr'],axis=0)\n",
    "        else:\n",
    "            np_arr = data['arr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000000, 40)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "320.0"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_arr.nbytes/1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[128897, 155086, 150350, ...,      4,      4,      4],\n",
       "       [111158,   2771,  42423, ...,  11618, 115962,  42423],\n",
       "       [158623, 111158,  73738, ...,      4,      4,      4],\n",
       "       ...,\n",
       "       [160165, 132439,  89121, ...,      4,      4,      4],\n",
       "       [ 89121, 188210, 123192, ...,      4,      4,      4],\n",
       "       [ 54942, 170538, 172776, ...,      4,      4,      4]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 264 ms, sys: 10.4 ms, total: 274 ms\n",
      "Wall time: 260 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'139_UNKN'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time np.unique(np_context)[100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = KeyedVectors.load_word2vec_format('../../model.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = './model'\n",
    "vocab_size = len(voc)\n",
    "sentence_size = 40\n",
    "embedding_size = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_fn(x,  params, is_training):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(x)\n",
    "\n",
    "    if is_training:\n",
    "        dataset = dataset.shuffle(buffer_size=params['buffer_size'])\n",
    "        dataset = dataset.repeat(count=params['num_epochs'])\n",
    "\n",
    "    dataset = dataset.batch(params['batch_size'])\n",
    "    dataset = dataset.prefetch(buffer_size=2)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    return iterator.get_next()\n",
    "    #return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def initializer(shape=None, dtype=None, partition_info=None):    \n",
    "    vocab_dict = count_v.vocabulary_\n",
    "    embedding_matrix = np.random.uniform(-1, 1, size=(vocab_size+1, embedding_size))\n",
    "    num_loaded = 0\n",
    "    for w, i in vocab_dict.items():\n",
    "        v = None\n",
    "        try:\n",
    "            v = word2vec[w]\n",
    "        except KeyError: # не нашли такой токен в словаре\n",
    "                pass\n",
    "        if v is not None :\n",
    "            embedding_matrix[i+1] = v\n",
    "            num_loaded += 1\n",
    "   \n",
    "    embedding_matrix = embedding_matrix.astype(np.float32)\n",
    "    embedding_matrix[0] = np.zeros(300)\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn(features, labels, mode, params):  \n",
    "    \n",
    "    # Compute predictions.\n",
    "    net = params['net']\n",
    "    \n",
    "    encoded_features = {}  \n",
    "    \n",
    "    with tf.variable_scope('encoder'):\n",
    "        encoded_features['anchor'] = net(features['anchor'])\n",
    "    with tf.variable_scope('encoder', reuse=True):\n",
    "        encoded_features['positive'] = net(features['positive'])\n",
    "    with tf.variable_scope('encoder', reuse=True):\n",
    "        encoded_features['negative'] = net(features['negative'])\n",
    "    \n",
    "    \n",
    "    #predicted_classes = tf.argmax(logits, 1)\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        predictions = {\n",
    "            'class_ids': predicted_classes[:, tf.newaxis],\n",
    "            'probabilities': tf.nn.softmax(logits),\n",
    "            'logits': logits,\n",
    "        }\n",
    "        return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
    "    \n",
    "    # Compute loss.\n",
    "    loss = my_triplet_loss(encoded_features,params['margin'])\n",
    "    \n",
    "#     # Compute evaluation metrics.\n",
    "#     accuracy = tf.metrics.accuracy(labels=labels,\n",
    "#                                predictions=predicted_classes,\n",
    "#                                name='acc_op')\n",
    "#     f1 = tf_metrics.f1(labels=labels,\n",
    "#                                predictions=predicted_classes,num_classes=3,average='micro')\n",
    "    \n",
    "#     metrics = {'accuracy': accuracy,'f1':f1}\n",
    "#     tf.summary.scalar('accuracy', accuracy[1])\n",
    "    \n",
    "#     # Compute evaluation\n",
    "#     if mode == tf.estimator.ModeKeys.EVAL:\n",
    "#         return tf.estimator.EstimatorSpec(\n",
    "#             mode, loss=loss, eval_metric_ops=metrics)\n",
    "    \n",
    "    optimizer = tf.train.AdagradOptimizer(learning_rate=0.1)\n",
    "    \n",
    "    train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())\n",
    "    \n",
    "    return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
