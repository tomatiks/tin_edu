{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import string\n",
    "import tempfile\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tensorflow.python.keras.preprocessing import sequence\n",
    "from tensorboard import summary as summary_lib\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem\n",
    "from tqdm import tqdm_notebook,tqdm_pandas,tqdm\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "import nltk\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import tf_metrics\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "from multiprocessing import cpu_count, Pool\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "TqdmDeprecationWarning: Please use `tqdm.pandas(...)` instead of `tqdm_pandas(tqdm, ...)`.\n"
     ]
    }
   ],
   "source": [
    "tqdm_pandas(tqdm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context,answer\n",
      " Убрал 600 литров мусора в реликтовом лесу. Ты просто большой молодец.,Спасибо.\n",
      "Убрал 600 литров мусора в реликтовом лесу. Ты просто большой молодец. Спасибо.,Приедь к нам в Мурманск пожалуйста.\n",
      "Убрал 600 литров мусора в реликтовом лесу. Ты просто большой молодец. Спасибо.,Тебе платят за это?\n",
      "Ты просто большой молодец. Спасибо. Тебе платят за это?,\"За посты на Пикабу? Да, платят. Достаточно при написании поста всего лишь ..\"\n",
      "\"Спасибо. Тебе платят за это? За посты на Пикабу? Да, пл\n"
     ]
    }
   ],
   "source": [
    "with open('pikabu.csv') as f:\n",
    "    print(f.read(500))\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('pikabu.csv',sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Убрал 600 литров мусора в реликтовом лесу. Ты...</td>\n",
       "      <td>Спасибо.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Убрал 600 литров мусора в реликтовом лесу. Ты ...</td>\n",
       "      <td>Приедь к нам в Мурманск пожалуйста.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Убрал 600 литров мусора в реликтовом лесу. Ты ...</td>\n",
       "      <td>Тебе платят за это?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ты просто большой молодец. Спасибо. Тебе платя...</td>\n",
       "      <td>За посты на Пикабу? Да, платят. Достаточно при...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Спасибо. Тебе платят за это? За посты на Пикаб...</td>\n",
       "      <td>Не нажимается сук</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Ты просто большой молодец. Спасибо. Тебе платя...</td>\n",
       "      <td>Лично Путин</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Спасибо. Тебе платят за это? Лично Путин</td>\n",
       "      <td>дарт вейдар</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Убрал 600 литров мусора в реликтовом лесу. Ты...</td>\n",
       "      <td>ну всё, теперь можно не убирать за собой, Чист...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Убрал 600 литров мусора в реликтовом лесу. Ты ...</td>\n",
       "      <td>Зачем заминусили человека? Очевидная ирония же...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Убрал 600 литров мусора в реликтовом лесу.</td>\n",
       "      <td>Герой, про которого я бы читал комиксы от dc и...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Убрал 600 литров мусора в реликтовом лесу. Ге...</td>\n",
       "      <td>да хотя бы от bubble студии)) но чет не пишут))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Убрал 600 литров мусора в реликтовом лесу. Гер...</td>\n",
       "      <td>может силами пикабу родится комикс, хехе)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Герой, про которого я бы читал комиксы от dc и...</td>\n",
       "      <td>Зовите Чилика...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>да хотя бы от bubble студии)) но чет не пишут)...</td>\n",
       "      <td>/@Chiliktolik , ищем автора комиксов про чисто...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Герой, про которого я бы читал комиксы от dc и...</td>\n",
       "      <td>Только комикс и может родится. Нет бы выйти да...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Убрал 600 литров мусора в реликтовом лесу. Гер...</td>\n",
       "      <td>ты в маске убираешь или она только для деавтор...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Герой, про которого я бы читал комиксы от dc и...</td>\n",
       "      <td>маска только для фото</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>да хотя бы от bubble студии)) но чет не пишут)...</td>\n",
       "      <td>понятно. А так молодец).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Убрал 600 литров мусора в реликтовом лесу. Гер...</td>\n",
       "      <td>Реликтовый лес? G-unit не нашел случаем?)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Убрал 600 литров мусора в реликтовом лесу. Гер...</td>\n",
       "      <td>Нужно мерчендайз организовывать</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              context  \\\n",
       "0    Убрал 600 литров мусора в реликтовом лесу. Ты...   \n",
       "1   Убрал 600 литров мусора в реликтовом лесу. Ты ...   \n",
       "2   Убрал 600 литров мусора в реликтовом лесу. Ты ...   \n",
       "3   Ты просто большой молодец. Спасибо. Тебе платя...   \n",
       "4   Спасибо. Тебе платят за это? За посты на Пикаб...   \n",
       "5   Ты просто большой молодец. Спасибо. Тебе платя...   \n",
       "6            Спасибо. Тебе платят за это? Лично Путин   \n",
       "7    Убрал 600 литров мусора в реликтовом лесу. Ты...   \n",
       "8   Убрал 600 литров мусора в реликтовом лесу. Ты ...   \n",
       "9          Убрал 600 литров мусора в реликтовом лесу.   \n",
       "10   Убрал 600 литров мусора в реликтовом лесу. Ге...   \n",
       "11  Убрал 600 литров мусора в реликтовом лесу. Гер...   \n",
       "12  Герой, про которого я бы читал комиксы от dc и...   \n",
       "13  да хотя бы от bubble студии)) но чет не пишут)...   \n",
       "14  Герой, про которого я бы читал комиксы от dc и...   \n",
       "15  Убрал 600 литров мусора в реликтовом лесу. Гер...   \n",
       "16  Герой, про которого я бы читал комиксы от dc и...   \n",
       "17  да хотя бы от bubble студии)) но чет не пишут)...   \n",
       "18  Убрал 600 литров мусора в реликтовом лесу. Гер...   \n",
       "19  Убрал 600 литров мусора в реликтовом лесу. Гер...   \n",
       "\n",
       "                                               answer  \n",
       "0                                            Спасибо.  \n",
       "1                 Приедь к нам в Мурманск пожалуйста.  \n",
       "2                                 Тебе платят за это?  \n",
       "3   За посты на Пикабу? Да, платят. Достаточно при...  \n",
       "4                                   Не нажимается сук  \n",
       "5                                         Лично Путин  \n",
       "6                                         дарт вейдар  \n",
       "7   ну всё, теперь можно не убирать за собой, Чист...  \n",
       "8   Зачем заминусили человека? Очевидная ирония же...  \n",
       "9   Герой, про которого я бы читал комиксы от dc и...  \n",
       "10    да хотя бы от bubble студии)) но чет не пишут))  \n",
       "11          может силами пикабу родится комикс, хехе)  \n",
       "12                                   Зовите Чилика...  \n",
       "13  /@Chiliktolik , ищем автора комиксов про чисто...  \n",
       "14  Только комикс и может родится. Нет бы выйти да...  \n",
       "15  ты в маске убираешь или она только для деавтор...  \n",
       "16                              маска только для фото  \n",
       "17                           понятно. А так молодец).  \n",
       "18          Реликтовый лес? G-unit не нашел случаем?)  \n",
       "19                    Нужно мерчендайз организовывать  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "context    21486165\n",
       "answer     21486162\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_train.loc[df_train['context'].apply(len)>410,'context'][271713].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21486165,)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([' Убрал 600 литров мусора в реликтовом лесу. Ты просто большой молодец.',\n",
       "       'Убрал 600 литров мусора в реликтовом лесу. Ты просто большой молодец. Спасибо.',\n",
       "       'Убрал 600 литров мусора в реликтовом лесу. Ты просто большой молодец. Спасибо.',\n",
       "       'Ты просто большой молодец. Спасибо. Тебе платят за это?',\n",
       "       'Спасибо. Тебе платят за это? За посты на Пикабу? Да, платят. Достаточно при написании поста всего лишь ..',\n",
       "       'Ты просто большой молодец. Спасибо. Тебе платят за это?',\n",
       "       'Спасибо. Тебе платят за это? Лично Путин',\n",
       "       ' Убрал 600 литров мусора в реликтовом лесу. Ты просто большой молодец.',\n",
       "       'Убрал 600 литров мусора в реликтовом лесу. Ты просто большой молодец. ну всё, теперь можно не убирать за собой, Чистомен спасёт!',\n",
       "       '  Убрал 600 литров мусора в реликтовом лесу.',\n",
       "       ' Убрал 600 литров мусора в реликтовом лесу. Герой, про которого я бы читал комиксы от dc или марвел!',\n",
       "       'Убрал 600 литров мусора в реликтовом лесу. Герой, про которого я бы читал комиксы от dc или марвел! да хотя бы от bubble студии)) но чет не пишут))',\n",
       "       'Герой, про которого я бы читал комиксы от dc или марвел! да хотя бы от bubble студии)) но чет не пишут)) может силами пикабу родится комикс, хехе)',\n",
       "       'да хотя бы от bubble студии)) но чет не пишут)) может силами пикабу родится комикс, хехе) Зовите Чилика...',\n",
       "       'Герой, про которого я бы читал комиксы от dc или марвел! да хотя бы от bubble студии)) но чет не пишут)) может силами пикабу родится комикс, хехе)',\n",
       "       'Убрал 600 литров мусора в реликтовом лесу. Герой, про которого я бы читал комиксы от dc или марвел! да хотя бы от bubble студии)) но чет не пишут))',\n",
       "       'Герой, про которого я бы читал комиксы от dc или марвел! да хотя бы от bubble студии)) но чет не пишут)) ты в маске убираешь или она только для деавторизации на фото?',\n",
       "       'да хотя бы от bubble студии)) но чет не пишут)) ты в маске убираешь или она только для деавторизации на фото? маска только для фото',\n",
       "       'Убрал 600 литров мусора в реликтовом лесу. Герой, про которого я бы читал комиксы от dc или марвел! да хотя бы от bubble студии)) но чет не пишут))',\n",
       "       'Убрал 600 литров мусора в реликтовом лесу. Герой, про которого я бы читал комиксы от dc или марвел! да хотя бы от bubble студии)) но чет не пишут))'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "802d451bc38f4de5847efe2fe7d29eb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=20), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyError",
     "evalue": "'убирать_VERB'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-107-b35a23888ac5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_prep_tags\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-106-3c1650a2fc15>\u001b[0m in \u001b[0;36mtext_prep_tags\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mprev_word\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m                 \u001b[0mtext_id_vocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_id_vocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprev_word\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m                 \u001b[0mid_text_vocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtext_id_vocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprev_word\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'убирать_VERB'"
     ]
    }
   ],
   "source": [
    "k = np.array(list(map(text_prep_tags,tqdm_notebook(context[:20]))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{4: 'убирать_VERB'}"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_text_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "sw_ru = set(stopwords.words('russian'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "mystem = Mystem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = df_train['context'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_tokenizer(tokenizer, corpus):\n",
    "    #voc_set = set()\n",
    "    res = np.fromiter(map(tokenizer,tqdm_notebook(corpus[:20]),),dtype=str)\n",
    "    voc_set = np.unique(res)\n",
    "    print(voc_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fu(x):\n",
    "    return np.apply_along_axis(text_prep_tags,1,np.reshape(x,(-1,1)))\n",
    "        \n",
    "def parallelize(data,partitions):\n",
    "\n",
    "    data_split = np.array_split(data, partitions)\n",
    "    pool = Pool(partitions)\n",
    "    #data = np.stack(pool.apply(np.apply_along_axis,text_prep_tags,1,np.reshape(data_split,(-1,1))))\n",
    "    data = np.concatenate(pool.map(fu,data_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-153064d275cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mvoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mvoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m200000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtext_prep_tags\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-72-016fc939a790>\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, data, tokenizer, huge_parts, save_to_file)\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msave_to_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-72-016fc939a790>\u001b[0m in \u001b[0;36m_extend_vocab\u001b[0;34m(self, token_matrix)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_extend_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtoken_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mstart_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchar2idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndenumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchar2idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart_idx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 173 ms, sys: 342 ms, total: 515 ms\n",
      "Wall time: 13.2 s\n"
     ]
    }
   ],
   "source": [
    "%time np_context = parallelize(context[:20000],4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.char2idx = {'<PAD>':0,'<START>':1,'<END>':2,'<UNK>':3}\n",
    "#         self.char2idx.update({ch:i+4 for i,ch in enumerate(set(tokenize_string(data)))})\n",
    "        self.idx2char = {i:ch for ch,i in self.char2idx.items()}\n",
    "    \n",
    "    def save(path='.'):\n",
    "        json_ch_idx = json.dumps(voc.char2idx)\n",
    "        with open(os.path.join(path,\"char2idx.json\"),\"w\") as f:\n",
    "            f.write(json_ch_idx)\n",
    "        json_idx_ch = json.dumps(voc.idx2char)\n",
    "        with open(os.path.join(path,\"idx2char.json\"),\"w\") as f:\n",
    "            f.write(json_idx_ch)\n",
    "    \n",
    "    def load(path='.'):\n",
    "        \n",
    "    \n",
    "    def indx_tokenize(self, sequence):\n",
    "        sequence = tokenize_string(sequence)\n",
    "        return [self.char2idx[char] for char in sequence]\n",
    "    \n",
    "    def indx_detokenize(self, sequence):\n",
    "        return ''.join([self.idx2char[idx] for idx in sequence])\n",
    "    \n",
    "    def tokenize(self, data, tokenizer, huge_parts, save_to_file=True):\n",
    "        huge_splits = np.array_split(data, huge_parts)\n",
    "        partitions = cpu_count()\n",
    "        for split_number,huge_split in enumerate(tqdm_notebook(huge_splits)):\n",
    "            print(huge_split.shape)          \n",
    "            data_split = np.array_split(huge_split, partitions)\n",
    "            pool = Pool(partitions)\n",
    "\n",
    "            res_data = np.concatenate(pool.map(tokenizer,data_split))\n",
    "            pool.close()\n",
    "            pool.join()\n",
    "            \n",
    "            print(res_data.shape)\n",
    "            self._extend_vocab(res_data)\n",
    "            \n",
    "            print(res_data)\n",
    "            \n",
    "            if save_to_file:\n",
    "                np.savez_compressed(f'tokenized_files/np_tokens{split_number}',arr=res_data)             \n",
    "                del res_data\n",
    "                gc.collect()\n",
    "                \n",
    "                \n",
    "    def _extend_vocab(self,token_matrix):\n",
    "        start_idx = len(self.char2idx)\n",
    "        for idx,token in np.ndenumerate(np.unique(token_matrix)):\n",
    "            if token not in self.char2idx:\n",
    "                self.char2idx[token] = start_idx + idx[0]\n",
    "        \n",
    "        self.idx2char = {i:ch for ch,i in self.char2idx.items()}\n",
    "                \n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.char2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8297baf19b8c4546b7e42bea840ff392",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000000,)\n",
      "(1000000, 40)\n",
      "[['убирать_VERB' '600_UNKN' 'литр_NOUN' ... '' '' '']\n",
      " ['убирать_VERB' '600_UNKN' 'литр_NOUN' ... '' '' '']\n",
      " ['убирать_VERB' '600_UNKN' 'литр_NOUN' ... '' '' '']\n",
      " ...\n",
      " ['гейб_NOUN' 'молодец_NOUN' 'давно_ADV' ... '' '' '']\n",
      " ['гейб_NOUN' 'молодец_NOUN' 'давно_ADV' ... '' '' '']\n",
      " ['гейб_NOUN' 'молодец_NOUN' 'давно_ADV' ... '' '' '']]\n",
      "(1000000,)\n",
      "(1000000, 40)\n",
      "[['молодец_NOUN' 'самый_DET' 'начало_NOUN' ... '' '' '']\n",
      " ['это_PRON' 'человек_NOUN' 'виноватый_ADJ' ... '' '' '']\n",
      " ['беда_NOUN' 'гринлайт_NOUN' 'плюсонуть_VERB' ... '' '' '']\n",
      " ...\n",
      " ['сложно_ADV' 'олдфаг_NOUN' 'зарегистрировать_VERB' ... '' '' '']\n",
      " ['зарегистрировать_VERB' 'месяц_NOUN' 'олдфаг_NOUN' ... '' '' '']\n",
      " ['это_PRON' 'читать_VERB' 'сидеть_VERB' ... '' '' '']]\n",
      "CPU times: user 1min 33s, sys: 1min 7s, total: 2min 40s\n",
      "Wall time: 18min 59s\n"
     ]
    }
   ],
   "source": [
    "def fu(x):\n",
    "    return np.apply_along_axis(text_prep_tags,1,np.reshape(x,(-1,1)))\n",
    "voc = Vocab()\n",
    "%time voc.tokenize(context[:2000000],fu,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '<PAD>',\n",
       " 1: '<START>',\n",
       " 2: '<END>',\n",
       " 3: '<UNK>',\n",
       " 4: '',\n",
       " 5: '    _ _UNKN',\n",
       " 6: '    __   _UNKN',\n",
       " 7: '   _ _UNKN',\n",
       " 8: '   __\\n_UNKN',\n",
       " 9: '   __ __UNKN',\n",
       " 10: '  _\\n_UNKN',\n",
       " 11: '  _  _UNKN',\n",
       " 12: '  _ _\\n_UNKN',\n",
       " 13: '  _ _UNKN',\n",
       " 14: '  __\\n_UNKN',\n",
       " 15: '  __  _UNKN',\n",
       " 16: '  __  __UNKN',\n",
       " 17: '  __ _UNKN',\n",
       " 18: '  __UNKN',\n",
       " 19: '  ___\\n_UNKN',\n",
       " 20: '  ___UNKN',\n",
       " 21: '  ヽ\\n_UNKN',\n",
       " 22: '  ヽ _UNKN',\n",
       " 23: '  ヽ ー__UNKN',\n",
       " 24: '  ヽ_UNKN',\n",
       " 25: ' _\\n_UNKN',\n",
       " 26: ' _   _UNKN',\n",
       " 27: ' _  _UNKN',\n",
       " 28: ' _ _ _\\n_UNKN',\n",
       " 29: ' _ _ _ _ _ _\\n_UNKN',\n",
       " 30: ' _ _ _ _ _ _ _ _ _ _ _ _ ',\n",
       " 31: ' _ _ _ _ _ _ _UNKN',\n",
       " 32: ' _ _ _ _UNKN',\n",
       " 33: ' _ _ _UNKN',\n",
       " 34: ' _ _UNKN',\n",
       " 35: ' _ __UNKN',\n",
       " 36: ' _ ヽ _UNKN',\n",
       " 37: ' __\\n_UNKN',\n",
       " 38: ' __  _UNKN',\n",
       " 39: ' __ _ _\\n_UNKN',\n",
       " 40: ' __ _UNKN',\n",
       " 41: ' __ __UNKN',\n",
       " 42: ' __UNKN',\n",
       " 43: ' ___\\n_UNKN',\n",
       " 44: ' ___ _UNKN',\n",
       " 45: ' ___UNKN',\n",
       " 46: ' ____\\n_UNKN',\n",
       " 47: ' ____ _UNKN',\n",
       " 48: ' ____UNKN',\n",
       " 49: ' _____\\n_UNKN',\n",
       " 50: ' _____ _ _UNKN',\n",
       " 51: ' _____ _UNKN',\n",
       " 52: ' ______\\n_UNKN',\n",
       " 53: ' ______ _UNKN',\n",
       " 54: ' ______UNKN',\n",
       " 55: ' _______\\n_UNKN',\n",
       " 56: ' _______ _UNKN',\n",
       " 57: ' ________ _UNKN',\n",
       " 58: ' _________\\n_UNKN',\n",
       " 59: ' _________ _UNKN',\n",
       " 60: ' __________\\n_UNKN',\n",
       " 61: ' __________        _UNKN',\n",
       " 62: ' __________    _UNKN',\n",
       " 63: ' ___________ _UNKN',\n",
       " 64: ' ___________UNKN',\n",
       " 65: ' ____________    ____ _UN',\n",
       " 66: ' ____________UNKN',\n",
       " 67: ' _____________ _UNKN',\n",
       " 68: ' ________________ _UNKN',\n",
       " 69: ' __________________ _UNKN',\n",
       " 70: ' ____________________ ___',\n",
       " 71: ' _____________________ _U',\n",
       " 72: ' ______________________ _',\n",
       " 73: ' ________________________',\n",
       " 74: ' ゞ _UNKN',\n",
       " 75: ' ー _UNKN',\n",
       " 76: ' ヽ     _UNKN',\n",
       " 77: ' ヽ _UNKN',\n",
       " 78: ' ヽ_UNKN',\n",
       " 79: ' ヽー\\n_UNKN',\n",
       " 80: '.   .   _UNKN',\n",
       " 81: '.  .  _UNKN',\n",
       " 82: '. . . . . . . . . . . _UN',\n",
       " 83: '. . . _UNKN',\n",
       " 84: '. . _UNKN',\n",
       " 85: '. ... _UNKN',\n",
       " 86: '. ._UNKN',\n",
       " 87: '. __UNKN',\n",
       " 88: '.. ... _UNKN',\n",
       " 89: '.. ..._UNKN',\n",
       " 90: '.. ._UNKN',\n",
       " 91: '... ....._UNKN',\n",
       " 92: '... ..._UNKN',\n",
       " 93: '... .._UNKN',\n",
       " 94: '.... ..._UNKN',\n",
       " 95: '.... _UNKN',\n",
       " 96: '.....  _UNKN',\n",
       " 97: '..... _UNKN',\n",
       " 98: '...... _UNKN',\n",
       " 99: '........ _UNKN',\n",
       " 100: '...............   _UNKN',\n",
       " 101: '.........................',\n",
       " 102: '........................_',\n",
       " 103: '......................_UN',\n",
       " 104: '....................._UNK',\n",
       " 105: '...................._UNKN',\n",
       " 106: '..................._UNKN',\n",
       " 107: '.................._UNKN',\n",
       " 108: '................._UNKN',\n",
       " 109: '................_UNKN',\n",
       " 110: '..............._UNKN',\n",
       " 111: '.............._UNKN',\n",
       " 112: '............._UNKN',\n",
       " 113: '............_UNKN',\n",
       " 114: '..........._UNKN',\n",
       " 115: '.........._UNKN',\n",
       " 116: '........._UNKN',\n",
       " 117: '........_UNKN',\n",
       " 118: '......._UNKN',\n",
       " 119: '......_UNKN',\n",
       " 120: '....._UNKN',\n",
       " 121: '...._UNKN',\n",
       " 122: '0.000000000000001_UNKN',\n",
       " 123: '0.000000000001_UNKN',\n",
       " 124: '0.0000001_UNKN',\n",
       " 125: '0.000000_UNKN',\n",
       " 126: '0.000010_UNKN',\n",
       " 127: '0.00003_UNKN',\n",
       " 128: '0.0001_UNKN',\n",
       " 129: '0.000250_UNKN',\n",
       " 130: '0.0025_UNKN',\n",
       " 131: '0.0034_UNKN',\n",
       " 132: '0.003_UNKN',\n",
       " 133: '0.004_UNKN',\n",
       " 134: '0.0076_UNKN',\n",
       " 135: '0.0081_UNKN',\n",
       " 136: '0.0088_UNKN',\n",
       " 137: '0.009_UNKN',\n",
       " 138: '0.00_UNKN',\n",
       " 139: '0.0127_UNKN',\n",
       " 140: '0.015625_UNKN',\n",
       " 141: '0.01_UNKN',\n",
       " 142: '0.023_UNKN',\n",
       " 143: '0.03125_UNKN',\n",
       " 144: '0.03_UNKN',\n",
       " 145: '0.04_UNKN',\n",
       " 146: '0.051_UNKN',\n",
       " 147: '0.05_UNKN',\n",
       " 148: '0.0625_UNKN',\n",
       " 149: '0.08_UNKN',\n",
       " 150: '0.09_UNKN',\n",
       " 151: '0.0_UNKN',\n",
       " 152: '0.100_UNKN',\n",
       " 153: '0.10_UNKN',\n",
       " 154: '0.119_UNKN',\n",
       " 155: '0.125_UNKN',\n",
       " 156: '0.1428571429_UNKN',\n",
       " 157: '0.18_UNKN',\n",
       " 158: '0.1_UNKN',\n",
       " 159: '0.223_UNKN',\n",
       " 160: '0.24_UNKN',\n",
       " 161: '0.255_UNKN',\n",
       " 162: '0.25_UNKN',\n",
       " 163: '0.26_UNKN',\n",
       " 164: '0.270_UNKN',\n",
       " 165: '0.2_UNKN',\n",
       " 166: '0.30_UNKN',\n",
       " 167: '0.31_UNKN',\n",
       " 168: '0.32_UNKN',\n",
       " 169: '0.33_UNKN',\n",
       " 170: '0.35_UNKN',\n",
       " 171: '0.375_UNKN',\n",
       " 172: '0.3_UNKN',\n",
       " 173: '0.40_UNKN',\n",
       " 174: '0.41_UNKN',\n",
       " 175: '0.44490_UNKN',\n",
       " 176: '0.449_UNKN',\n",
       " 177: '0.44_UNKN',\n",
       " 178: '0.45_UNKN',\n",
       " 179: '0.47_UNKN',\n",
       " 180: '0.485_UNKN',\n",
       " 181: '0.49_UNKN',\n",
       " 182: '0.4_UNKN',\n",
       " 183: '0.50_UNKN',\n",
       " 184: '0.52_UNKN',\n",
       " 185: '0.53_UNKN',\n",
       " 186: '0.5553535_UNKN',\n",
       " 187: '0.55_UNKN',\n",
       " 188: '0.568_UNKN',\n",
       " 189: '0.57_UNKN',\n",
       " 190: '0.586_UNKN',\n",
       " 191: '0.5_UNKN',\n",
       " 192: '0.6666_UNKN',\n",
       " 193: '0.68_UNKN',\n",
       " 194: '0.6_UNKN',\n",
       " 195: '0.71_UNKN',\n",
       " 196: '0.75_UNKN',\n",
       " 197: '0.76_UNKN',\n",
       " 198: '0.783183_UNKN',\n",
       " 199: '0.7931505_UNKN',\n",
       " 200: '0.7_UNKN',\n",
       " 201: '0.850_UNKN',\n",
       " 202: '0.87_UNKN',\n",
       " 203: '0.89_UNKN',\n",
       " 204: '0.8_UNKN',\n",
       " 205: '0.9012_UNKN',\n",
       " 206: '0.91_UNKN',\n",
       " 207: '0.942505_UNKN',\n",
       " 208: '0.94_UNKN',\n",
       " 209: '0.95_UNKN',\n",
       " 210: '0.98_UNKN',\n",
       " 211: '0.99100_UNKN',\n",
       " 212: '0.9999999999_UNKN',\n",
       " 213: '0.99_UNKN',\n",
       " 214: '0.9_UNKN',\n",
       " 215: '00.000000000000_UNKN',\n",
       " 216: '00.00_UNKN',\n",
       " 217: '00.01_UNKN',\n",
       " 218: '00.49_UNKN',\n",
       " 219: '00.95_UNKN',\n",
       " 220: '000.000_UNKN',\n",
       " 221: '00000000000000000000001_U',\n",
       " 222: '0000000000000001_UNKN',\n",
       " 223: '0000000000002_UNKN',\n",
       " 224: '0000000000010_UNKN',\n",
       " 225: '00000000018_UNKN',\n",
       " 226: '000000000_UNKN',\n",
       " 227: '00000000_UNKN',\n",
       " 228: '00000001_UNKN',\n",
       " 229: '0000001_UNKN',\n",
       " 230: '000000_UNKN',\n",
       " 231: '000001_UNKN',\n",
       " 232: '00000_UNKN',\n",
       " 233: '00001.00000002_UNKN',\n",
       " 234: '00002_UNKN',\n",
       " 235: '000030_UNKN',\n",
       " 236: '00004_UNKN',\n",
       " 237: '0000_UNKN',\n",
       " 238: '000100_UNKN',\n",
       " 239: '00010_UNKN',\n",
       " 240: '000110101100011101_UNKN',\n",
       " 241: '00017_UNKN',\n",
       " 242: '0001_UNKN',\n",
       " 243: '00026185185185185_UNKN',\n",
       " 244: '00035_UNKN',\n",
       " 245: '0003_UNKN',\n",
       " 246: '0005_UNKN',\n",
       " 247: '0006_UNKN',\n",
       " 248: '000_UNKN',\n",
       " 249: '000k_UNKN',\n",
       " 250: '000баксов_UNKN',\n",
       " 251: '000км_UNKN',\n",
       " 252: '000р_UNKN',\n",
       " 253: '000руб_UNKN',\n",
       " 254: '000сум_UNKN',\n",
       " 255: '001002_UNKN',\n",
       " 256: '0010_UNKN',\n",
       " 257: '0011001000110000001100010',\n",
       " 258: '0011159_UNKN',\n",
       " 259: '0015_UNKN',\n",
       " 260: '001626002048_UNKN',\n",
       " 261: '001_UNKN',\n",
       " 262: '001мм_UNKN',\n",
       " 263: '0024_UNKN',\n",
       " 264: '0026_UNKN',\n",
       " 265: '00288_UNKN',\n",
       " 266: '002_UNKN',\n",
       " 267: '002frev_UNKN',\n",
       " 268: '0030_UNKN',\n",
       " 269: '003_UNKN',\n",
       " 270: '00495_UNKN',\n",
       " 271: '004_UNKN',\n",
       " 272: '005553535_UNKN',\n",
       " 273: '00555_UNKN',\n",
       " 274: '005_UNKN',\n",
       " 275: '0060с7o_UNKN',\n",
       " 276: '0066_UNKN',\n",
       " 277: '006_UNKN',\n",
       " 278: '0078_UNKN',\n",
       " 279: '007_UNKN',\n",
       " 280: '0081_UNKN',\n",
       " 281: '0099к_UNKN',\n",
       " 282: '009_UNKN',\n",
       " 283: '00_UNKN',\n",
       " 284: '00е_UNKN',\n",
       " 285: '00ольноль_UNKN',\n",
       " 286: '00оо_UNKN',\n",
       " 287: '00х_UNKN',\n",
       " 288: '00ых_UNKN',\n",
       " 289: '01.00_UNKN',\n",
       " 290: '01.01_UNKN',\n",
       " 291: '01.02_UNKN',\n",
       " 292: '01.03_UNKN',\n",
       " 293: '01.04_UNKN',\n",
       " 294: '01000_UNKN',\n",
       " 295: '01001103_UNKN',\n",
       " 296: '0100_UNKN',\n",
       " 297: '010103_UNKN',\n",
       " 298: '010111001010110001001_UNK',\n",
       " 299: '0103_UNKN',\n",
       " 300: '010455_UNKN',\n",
       " 301: '010600_UNKN',\n",
       " 302: '010_UNKN',\n",
       " 303: '011000111_UNKN',\n",
       " 304: '0110_UNKN',\n",
       " 305: '01200_UNKN',\n",
       " 306: '0123456789_UNKN',\n",
       " 307: '0125_UNKN',\n",
       " 308: '0125мм_UNKN',\n",
       " 309: '012_UNKN',\n",
       " 310: '0138_UNKN',\n",
       " 311: '013_UNKN',\n",
       " 312: '0147_UNKN',\n",
       " 313: '0148_UNKN',\n",
       " 314: '014_UNKN',\n",
       " 315: '0150_UNKN',\n",
       " 316: '0151_UNKN',\n",
       " 317: '015325_UNKN',\n",
       " 318: '01571111111111111_UNKN',\n",
       " 319: '0160_UNKN',\n",
       " 320: '0169_UNKN',\n",
       " 321: '016_UNKN',\n",
       " 322: '016м3_UNKN',\n",
       " 323: '017_UNKN',\n",
       " 324: '017ый_UNKN',\n",
       " 325: '018_UNKN',\n",
       " 326: '019_UNKN',\n",
       " 327: '01_UNKN',\n",
       " 328: '01пожар69гейклубпожар_UNK',\n",
       " 329: '0200_UNKN',\n",
       " 330: '0206mail_UNKN',\n",
       " 331: '0216_UNKN',\n",
       " 332: '0220_UNKN',\n",
       " 333: '0220в_UNKN',\n",
       " 334: '0234_UNKN',\n",
       " 335: '023_UNKN',\n",
       " 336: '024_UNKN',\n",
       " 337: '0250_UNKN',\n",
       " 338: '0254_UNKN',\n",
       " 339: '0257_UNKN',\n",
       " 340: '025_UNKN',\n",
       " 341: '02_UNKN',\n",
       " 342: '02b_UNKN',\n",
       " 343: '02c_UNKN',\n",
       " 344: '02i_UNKN',\n",
       " 345: '02pi_UNKN',\n",
       " 346: '02а_UNKN',\n",
       " 347: '03.01_UNKN',\n",
       " 348: '03.04_UNKN',\n",
       " 349: '03.09_UNKN',\n",
       " 350: '030_UNKN',\n",
       " 351: '0313_UNKN',\n",
       " 352: '031_UNKN',\n",
       " 353: '0321_UNKN',\n",
       " 354: '032р_UNKN',\n",
       " 355: '0330_UNKN',\n",
       " 356: '03334764645354384354л_UNK',\n",
       " 357: '033_UNKN',\n",
       " 358: '035_UNKN',\n",
       " 359: '0367_UNKN',\n",
       " 360: '036_UNKN',\n",
       " 361: '037_UNKN',\n",
       " 362: '0396723536_UNKN',\n",
       " 363: '03_UNKN',\n",
       " 364: '03х01_UNKN',\n",
       " 365: '04.12_UNKN',\n",
       " 366: '042_UNKN',\n",
       " 367: '043b_UNKN',\n",
       " 368: '04449_UNKN',\n",
       " 369: '044_UNKN',\n",
       " 370: '0450_UNKN',\n",
       " 371: '0451_UNKN',\n",
       " 372: '045_UNKN',\n",
       " 373: '046_UNKN',\n",
       " 374: '0473_UNKN',\n",
       " 375: '047_UNKN',\n",
       " 376: '048052_UNKN',\n",
       " 377: '048_UNKN',\n",
       " 378: '04_UNKN',\n",
       " 379: '04y_UNKN',\n",
       " 380: '05.05_UNKN',\n",
       " 381: '05.07_UNKN',\n",
       " 382: '05.10_UNKN',\n",
       " 383: '05.12_UNKN',\n",
       " 384: '0505050125_UNKN',\n",
       " 385: '0506846672_UNKN',\n",
       " 386: '0508_UNKN',\n",
       " 387: '050_UNKN',\n",
       " 388: '0515_UNKN',\n",
       " 389: '0515л_UNKN',\n",
       " 390: '051_UNKN',\n",
       " 391: '052016_UNKN',\n",
       " 392: '052020_UNKN',\n",
       " 393: '053_UNKN',\n",
       " 394: '05568687_UNKN',\n",
       " 395: '055_UNKN',\n",
       " 396: '0567_UNKN',\n",
       " 397: '056826125_UNKN',\n",
       " 398: '05683_UNKN',\n",
       " 399: '0568_UNKN',\n",
       " 400: '056_UNKN',\n",
       " 401: '057_UNKN',\n",
       " 402: '059_UNKN',\n",
       " 403: '05_UNKN',\n",
       " 404: '05rus_UNKN',\n",
       " 405: '05xmr_UNKN',\n",
       " 406: '05к_UNKN',\n",
       " 407: '05л_UNKN',\n",
       " 408: '05мм_UNKN',\n",
       " 409: '05см_UNKN',\n",
       " 410: '05тые_UNKN',\n",
       " 411: '06.02_UNKN',\n",
       " 412: '06.03_UNKN',\n",
       " 413: '06.05_UNKN',\n",
       " 414: '06.2015_UNKN',\n",
       " 415: '0611_UNKN',\n",
       " 416: '0634_UNKN',\n",
       " 417: '0666_UNKN',\n",
       " 418: '066_UNKN',\n",
       " 419: '0672_UNKN',\n",
       " 420: '068_UNKN',\n",
       " 421: '06_UNKN',\n",
       " 422: '06tolik86_UNKN',\n",
       " 423: '07.00_UNKN',\n",
       " 424: '07.01_UNKN',\n",
       " 425: '07.03_UNKN',\n",
       " 426: '07.05_UNKN',\n",
       " 427: '07.11_UNKN',\n",
       " 428: '07082018_UNKN',\n",
       " 429: '071_UNKN',\n",
       " 430: '0726_UNKN',\n",
       " 431: '0745_UNKN',\n",
       " 432: '075_UNKN',\n",
       " 433: '075l_UNKN',\n",
       " 434: '07734_UNKN',\n",
       " 435: '07831505_UNKN',\n",
       " 436: '0783_UNKN',\n",
       " 437: '0785d2_UNKN',\n",
       " 438: '07931505_UNKN',\n",
       " 439: '0793_UNKN',\n",
       " 440: '07_UNKN',\n",
       " 441: '07mail_UNKN',\n",
       " 442: '07го_UNKN',\n",
       " 443: '07л_UNKN',\n",
       " 444: '07ю_UNKN',\n",
       " 445: '08.00_UNKN',\n",
       " 446: '08.02_UNKN',\n",
       " 447: '08.03_UNKN',\n",
       " 448: '08.07_UNKN',\n",
       " 449: '08.08_UNKN',\n",
       " 450: '08.09_UNKN',\n",
       " 451: '08.30_UNKN',\n",
       " 452: '0800_UNKN',\n",
       " 453: '0808_UNKN',\n",
       " 454: '0811_UNKN',\n",
       " 455: '082_UNKN',\n",
       " 456: '085_UNKN',\n",
       " 457: '089_UNKN',\n",
       " 458: '08_UNKN',\n",
       " 459: '08mail_UNKN',\n",
       " 460: '08xi16г_UNKN',\n",
       " 461: '08мбс_UNKN',\n",
       " 462: '09.0018_UNKN',\n",
       " 463: '09.05_UNKN',\n",
       " 464: '0900_UNKN',\n",
       " 465: '09051945_UNKN',\n",
       " 466: '090_UNKN',\n",
       " 467: '0910_UNKN',\n",
       " 468: '091_UNKN',\n",
       " 469: '0927_UNKN',\n",
       " 470: '09281918_UNKN',\n",
       " 471: '092_UNKN',\n",
       " 472: '093_UNKN',\n",
       " 473: '095_UNKN',\n",
       " 474: '097_UNKN',\n",
       " 475: '099_UNKN',\n",
       " 476: '099s_UNKN',\n",
       " 477: '09_UNKN',\n",
       " 478: '09xi16г_UNKN',\n",
       " 479: '09го_UNKN',\n",
       " 480: '09кто_UNKN',\n",
       " 481: '09л_UNKN',\n",
       " 482: '0_UNKN',\n",
       " 483: '0a1_UNKN',\n",
       " 484: '0b11111100001_UNKN',\n",
       " 485: '0bh0_UNKN',\n",
       " 486: '0bho_UNKN',\n",
       " 487: '0ddzum0rj1bjmail_UNKN',\n",
       " 488: '0dimetrius0_UNKN',\n",
       " 489: '0drt0r_UNKN',\n",
       " 490: '0exm68_UNKN',\n",
       " 491: '0h_UNKN',\n",
       " 492: '0jxrgdc70lgg0lrqvtc80ymg0',\n",
       " 493: '0khqv9cw0yhquncx0l4sinc00',\n",
       " 494: '0qp46l_UNKN',\n",
       " 495: '0r_UNKN',\n",
       " 496: '0tpeжyt_UNKN',\n",
       " 497: '0varda0_UNKN',\n",
       " 498: '0w30_UNKN',\n",
       " 499: '0x00000012_UNKN',\n",
       " 500: '0x00_UNKN',\n",
       " 501: '0x1_UNKN',\n",
       " 502: '0x20_UNKN',\n",
       " 503: '0x2923143_UNKN',\n",
       " 504: '0x50_UNKN',\n",
       " 505: '0x5f3759df_UNKN',\n",
       " 506: '0x7e1_UNKN',\n",
       " 507: '0xff_UNKN',\n",
       " 508: '0а_UNKN',\n",
       " 509: '0азнь_UNKN',\n",
       " 510: '0й_UNKN',\n",
       " 511: '0лень_UNKN',\n",
       " 512: '0м_UNKN',\n",
       " 513: '0не_UNKN',\n",
       " 514: '0о_UNKN',\n",
       " 515: '0ошники_UNKN',\n",
       " 516: '0р_UNKN',\n",
       " 517: '0руб_UNKN',\n",
       " 518: '0ттбьч_UNKN',\n",
       " 519: '0х00_UNKN',\n",
       " 520: '0х2_UNKN',\n",
       " 521: '0х500_UNKN',\n",
       " 522: '0х50_UNKN',\n",
       " 523: '0ых_UNKN',\n",
       " 524: '1.000_UNKN',\n",
       " 525: '1.002_UNKN',\n",
       " 526: '1.01_UNKN',\n",
       " 527: '1.02_UNKN',\n",
       " 528: '1.07_UNKN',\n",
       " 529: '1.0_UNKN',\n",
       " 530: '1.11_UNKN',\n",
       " 531: '1.19175_UNKN',\n",
       " 532: '1.198_UNKN',\n",
       " 533: '1.1_UNKN',\n",
       " 534: '1.20_UNKN',\n",
       " 535: '1.216451_UNKN',\n",
       " 536: '1.22_UNKN',\n",
       " 537: '1.24_UNKN',\n",
       " 538: '1.25_UNKN',\n",
       " 539: '1.27_UNKN',\n",
       " 540: '1.2_UNKN',\n",
       " 541: '1.30_UNKN',\n",
       " 542: '1.35_UNKN',\n",
       " 543: '1.3763753_UNKN',\n",
       " 544: '1.38_UNKN',\n",
       " 545: '1.3_UNKN',\n",
       " 546: '1.42_UNKN',\n",
       " 547: '1.45_UNKN',\n",
       " 548: '1.498_UNKN',\n",
       " 549: '1.4_UNKN',\n",
       " 550: '1.50_UNKN',\n",
       " 551: '1.511_UNKN',\n",
       " 552: '1.51_UNKN',\n",
       " 553: '1.52_UNKN',\n",
       " 554: '1.53_UNKN',\n",
       " 555: '1.57_UNKN',\n",
       " 556: '1.5_UNKN',\n",
       " 557: '1.600_UNKN',\n",
       " 558: '1.602_UNKN',\n",
       " 559: '1.60_UNKN',\n",
       " 560: '1.65_UNKN',\n",
       " 561: '1.68_UNKN',\n",
       " 562: '1.69_UNKN',\n",
       " 563: '1.6_UNKN',\n",
       " 564: '1.70_UNKN',\n",
       " 565: '1.71_UNKN',\n",
       " 566: '1.72_UNKN',\n",
       " 567: '1.75_UNKN',\n",
       " 568: '1.783_UNKN',\n",
       " 569: '1.78_UNKN',\n",
       " 570: '1.7_UNKN',\n",
       " 571: '1.80_UNKN',\n",
       " 572: '1.82_UNKN',\n",
       " 573: '1.85_UNKN',\n",
       " 574: '1.8745_UNKN',\n",
       " 575: '1.87_UNKN',\n",
       " 576: '1.892_UNKN',\n",
       " 577: '1.8_UNKN',\n",
       " 578: '1.90_UNKN',\n",
       " 579: '1.93_UNKN',\n",
       " 580: '1.95_UNKN',\n",
       " 581: '1.96_UNKN',\n",
       " 582: '1.99_UNKN',\n",
       " 583: '1.9_UNKN',\n",
       " 584: '10.000_UNKN',\n",
       " 585: '10.00_UNKN',\n",
       " 586: '10.01_UNKN',\n",
       " 587: '10.0_UNKN',\n",
       " 588: '10.10_UNKN',\n",
       " 589: '10.12_UNKN',\n",
       " 590: '10.16_UNKN',\n",
       " 591: '10.1_UNKN',\n",
       " 592: '10.259_UNKN',\n",
       " 593: '10.25_UNKN',\n",
       " 594: '10.366_UNKN',\n",
       " 595: '10.42_UNKN',\n",
       " 596: '10.45_UNKN',\n",
       " 597: '10.4_UNKN',\n",
       " 598: '10.5_UNKN',\n",
       " 599: '10.6_UNKN',\n",
       " 600: '10.99100_UNKN',\n",
       " 601: '10.9_UNKN',\n",
       " 602: '100.000_UNKN',\n",
       " 603: '100.75_UNKN',\n",
       " 604: '10000000000_UNKN',\n",
       " 605: '1000000000_UNKN',\n",
       " 606: '1000000001_UNKN',\n",
       " 607: '100000000_UNKN',\n",
       " 608: '10000000_UNKN',\n",
       " 609: '10000001_UNKN',\n",
       " 610: '1000000_UNKN',\n",
       " 611: '1000000к_UNKN',\n",
       " 612: '10000010_UNKN',\n",
       " 613: '1000005_UNKN',\n",
       " 614: '100000640093600_UNKN',\n",
       " 615: '100000_UNKN',\n",
       " 616: '100000й_UNKN',\n",
       " 617: '100007038_UNKN',\n",
       " 618: '10000_UNKN',\n",
       " 619: '10000к_UNKN',\n",
       " 620: '10000л_UNKN',\n",
       " 621: '10000р_UNKN',\n",
       " 622: '10000т_UNKN',\n",
       " 623: '100010001000_UNKN',\n",
       " 624: '10001000_UNKN',\n",
       " 625: '1000111110100100100000110',\n",
       " 626: '10001200_UNKN',\n",
       " 627: '10001200р_UNKN',\n",
       " 628: '10001500_UNKN',\n",
       " 629: '10001500м_UNKN',\n",
       " 630: '1000192_UNKN',\n",
       " 631: '100019_UNKN',\n",
       " 632: '10001_UNKN',\n",
       " 633: '10002000_UNKN',\n",
       " 634: '10003000р_UNKN',\n",
       " 635: '10007013185_UNKN',\n",
       " 636: '1000800р_UNKN',\n",
       " 637: '1000_UNKN',\n",
       " 638: '1000btc_UNKN',\n",
       " 639: '1000в_UNKN',\n",
       " 640: '1000гр_UNKN',\n",
       " 641: '1000грн_UNKN',\n",
       " 642: '1000й_UNKN',\n",
       " 643: '1000к_UNKN',\n",
       " 644: '1000км_UNKN',\n",
       " 645: '1000кмстараюсь_UNKN',\n",
       " 646: '1000мб_UNKN',\n",
       " 647: '1000мин_UNKN',\n",
       " 648: '1000мм_UNKN',\n",
       " 649: '1000ни_UNKN',\n",
       " 650: '1000но_UNKN',\n",
       " 651: '1000р_UNKN',\n",
       " 652: '1000рмесяц_UNKN',\n",
       " 653: '1000руб_UNKN',\n",
       " 654: '1000угольное_UNKN',\n",
       " 655: '1000х2100_UNKN',\n",
       " 656: '1000х750_UNKN',\n",
       " 657: '1000х90х90мм_UNKN',\n",
       " 658: '1000ысячи_UNKN',\n",
       " 659: '10010010000_UNKN',\n",
       " 660: '100104_UNKN',\n",
       " 661: '100110_UNKN',\n",
       " 662: '100120_UNKN',\n",
       " 663: '1001230_UNKN',\n",
       " 664: '1001233_UNKN',\n",
       " 665: '100130_UNKN',\n",
       " 666: '100130т_UNKN',\n",
       " 667: '100150_UNKN',\n",
       " 668: '100150р_UNKN',\n",
       " 669: '100170_UNKN',\n",
       " 670: '1001_UNKN',\n",
       " 671: '1001мем_UNKN',\n",
       " 672: '100200_UNKN',\n",
       " 673: '100200к_UNKN',\n",
       " 674: '100200л_UNKN',\n",
       " 675: '100250_UNKN',\n",
       " 676: '1002_UNKN',\n",
       " 677: '100300_UNKN',\n",
       " 678: '1004_UNKN',\n",
       " 679: '1005000_UNKN',\n",
       " 680: '100500_UNKN',\n",
       " 681: '100500кг_UNKN',\n",
       " 682: '100500ый_UNKN',\n",
       " 683: '100500этажки_UNKN',\n",
       " 684: '100500я_UNKN',\n",
       " 685: '1005100_UNKN',\n",
       " 686: '1005_UNKN',\n",
       " 687: '10060_UNKN',\n",
       " 688: '1006_UNKN',\n",
       " 689: '1008317_UNKN',\n",
       " 690: '1009.332622_UNKN',\n",
       " 691: '100_UNKN',\n",
       " 692: '100hp_UNKN',\n",
       " 693: '100k_UNKN',\n",
       " 694: '100kg_UNKN',\n",
       " 695: '100kk_UNKN',\n",
       " 696: '100leshnica_UNKN',\n",
       " 697: '100lvl_UNKN',\n",
       " 698: '100rub_UNKN',\n",
       " 699: '100rubley_UNKN',\n",
       " 700: '100rublеy_UNKN',\n",
       " 701: '100rublеу_UNKN',\n",
       " 702: '100su_UNKN',\n",
       " 703: '100алиби_UNKN',\n",
       " 704: '100в_UNKN',\n",
       " 705: '100г100м_UNKN',\n",
       " 706: '100г500м_UNKN',\n",
       " 707: '100г_UNKN',\n",
       " 708: '100гб_UNKN',\n",
       " 709: '100главый_UNKN',\n",
       " 710: '100го_UNKN',\n",
       " 711: '100гр_UNKN',\n",
       " 712: '100градусного_UNKN',\n",
       " 713: '100грамм_UNKN',\n",
       " 714: '100грн_UNKN',\n",
       " 715: '100джек_UNKN',\n",
       " 716: '100евр_UNKN',\n",
       " 717: '100евроночь_UNKN',\n",
       " 718: '100жужжит_UNKN',\n",
       " 719: '100изменение_UNKN',\n",
       " 720: '100й_UNKN',\n",
       " 721: '100к1_UNKN',\n",
       " 722: '100к200к_UNKN',\n",
       " 723: '100к_UNKN',\n",
       " 724: '100ка_UNKN',\n",
       " 725: '100кб_UNKN',\n",
       " 726: '100кг_UNKN',\n",
       " 727: '100кило_UNKN',\n",
       " 728: '100килограммовому_UNKN',\n",
       " 729: '100килограмовая_UNKN',\n",
       " 730: '100кк_UNKN',\n",
       " 731: '100км_UNKN',\n",
       " 732: '100кмес_UNKN',\n",
       " 733: '100кмин_UNKN',\n",
       " 734: '100кмч_UNKN',\n",
       " 735: '100крат_UNKN',\n",
       " 736: '100л_UNKN',\n",
       " 737: '100лет_UNKN',\n",
       " 738: '100летие_UNKN',\n",
       " 739: '100летию_UNKN',\n",
       " 740: '100летними_UNKN',\n",
       " 741: '100литров_UNKN',\n",
       " 742: '100м2_UNKN',\n",
       " 743: '100м36524200057м_UNKN',\n",
       " 744: '100м_UNKN',\n",
       " 745: '100максимум_UNKN',\n",
       " 746: '100мб_UNKN',\n",
       " 747: '100мбит_UNKN',\n",
       " 748: '100мбс_UNKN',\n",
       " 749: '100мбсек_UNKN',\n",
       " 750: '100мегатонных_UNKN',\n",
       " 751: '100местное_UNKN',\n",
       " 752: '100метров_UNKN',\n",
       " 753: '100мигабитный_UNKN',\n",
       " 754: '100мл_UNKN',\n",
       " 755: '100млн_UNKN',\n",
       " 756: '100мм_UNKN',\n",
       " 757: '100ная_UNKN',\n",
       " 758: '100но_UNKN',\n",
       " 759: '100ный_UNKN',\n",
       " 760: '100ого_UNKN',\n",
       " 761: '100отка_UNKN',\n",
       " 762: '100пакетиков_UNKN',\n",
       " 763: '100прикинь_UNKN',\n",
       " 764: '100пудов_UNKN',\n",
       " 765: '100пудово_UNKN',\n",
       " 766: '100р_UNKN',\n",
       " 767: '100ркг_UNKN',\n",
       " 768: '100рсутки_UNKN',\n",
       " 769: '100руб_UNKN',\n",
       " 770: '100рублей_UNKN',\n",
       " 771: '100рштука_UNKN',\n",
       " 772: '100с_UNKN',\n",
       " 773: '100см100см_UNKN',\n",
       " 774: '100см_UNKN',\n",
       " 775: '100сработает_UNKN',\n",
       " 776: '100стопроцентной_UNKN',\n",
       " 777: '100т_UNKN',\n",
       " 778: '100тб_UNKN',\n",
       " 779: '100то_UNKN',\n",
       " 780: '100тонн_UNKN',\n",
       " 781: '100тыс_UNKN',\n",
       " 782: '100тыщ_UNKN',\n",
       " 783: '100тыщъпятистаэташки_UNKN',\n",
       " 784: '100уровень_UNKN',\n",
       " 785: '100хп_UNKN',\n",
       " 786: '100шт_UNKN',\n",
       " 787: '100штук_UNKN',\n",
       " 788: '100я_UNKN',\n",
       " 789: '10100111_UNKN',\n",
       " 790: '10100_UNKN',\n",
       " 791: '10101010101010101010_UNKN',\n",
       " 792: '10101030_UNKN',\n",
       " 793: '1010400153_UNKN',\n",
       " 794: '1010_UNKN',\n",
       " 795: '10110000_UNKN',\n",
       " 796: '10110_UNKN',\n",
       " 797: '10111110_UNKN',\n",
       " 798: '101111_UNKN',\n",
       " 799: '1011_UNKN',\n",
       " 800: '1011к_UNKN',\n",
       " 801: '1011лет_UNKN',\n",
       " 802: '1012_UNKN',\n",
       " 803: '1013_UNKN',\n",
       " 804: '1013к_UNKN',\n",
       " 805: '1014_UNKN',\n",
       " 806: '10155_UNKN',\n",
       " 807: '1015_UNKN',\n",
       " 808: '1015к_UNKN',\n",
       " 809: '1015кг_UNKN',\n",
       " 810: '1015р_UNKN',\n",
       " 811: '1016_UNKN',\n",
       " 812: '1017_UNKN',\n",
       " 813: '10180_UNKN',\n",
       " 814: '1018_UNKN',\n",
       " 815: '1019_UNKN',\n",
       " 816: '101_UNKN',\n",
       " 817: '101вый_UNKN',\n",
       " 818: '101й_UNKN',\n",
       " 819: '101р_UNKN',\n",
       " 820: '10200_UNKN',\n",
       " 821: '1020_UNKN',\n",
       " 822: '1020вт_UNKN',\n",
       " 823: '1020к_UNKN',\n",
       " 824: '1020кбсек_UNKN',\n",
       " 825: '1021_UNKN',\n",
       " 826: '1023_UNKN',\n",
       " 827: '1024450_UNKN',\n",
       " 828: '102456_UNKN',\n",
       " 829: '1024_UNKN',\n",
       " 830: '1024гб_UNKN',\n",
       " 831: '1024х768_UNKN',\n",
       " 832: '10251220_UNKN',\n",
       " 833: '10255_UNKN',\n",
       " 834: '1025_UNKN',\n",
       " 835: '1028_UNKN',\n",
       " 836: '102_UNKN',\n",
       " 837: '102х77_UNKN',\n",
       " 838: '102х_UNKN',\n",
       " 839: '103.4_UNKN',\n",
       " 840: '1030_UNKN',\n",
       " 841: '1030²24500метров_UNKN',\n",
       " 842: '1030к_UNKN',\n",
       " 843: '103101.34_UNKN',\n",
       " 844: '10320_UNKN',\n",
       " 845: '1032_UNKN',\n",
       " 846: '1034_UNKN',\n",
       " 847: '10350_UNKN',\n",
       " 848: '10376_UNKN',\n",
       " 849: '1039_UNKN',\n",
       " 850: '103_UNKN',\n",
       " 851: '103й_UNKN',\n",
       " 852: '1040_UNKN',\n",
       " 853: '1040к_UNKN',\n",
       " 854: '1045_UNKN',\n",
       " 855: '1048576_UNKN',\n",
       " 856: '1048_UNKN',\n",
       " 857: '104_UNKN',\n",
       " 858: '104а_UNKN',\n",
       " 859: '105.1_UNKN',\n",
       " 860: '105.3_UNKN',\n",
       " 861: '1050.3_UNKN',\n",
       " 862: '10500_UNKN',\n",
       " 863: '10500рсмены_UNKN',\n",
       " 864: '10501060_UNKN',\n",
       " 865: '1050_UNKN',\n",
       " 866: '1050ti_UNKN',\n",
       " 867: '1050ти_UNKN',\n",
       " 868: '1052_UNKN',\n",
       " 869: '10532_UNKN',\n",
       " 870: '105520_UNKN',\n",
       " 871: '105570773_UNKN',\n",
       " 872: '1055_UNKN',\n",
       " 873: '1056090_UNKN',\n",
       " 874: '10568_UNKN',\n",
       " 875: '1056_UNKN',\n",
       " 876: '1058_UNKN',\n",
       " 877: '10597_UNKN',\n",
       " 878: '105_UNKN',\n",
       " 879: '105кг_UNKN',\n",
       " 880: '105ст_UNKN',\n",
       " 881: '10609_UNKN',\n",
       " 882: '1060_UNKN',\n",
       " 883: '1060ti_UNKN',\n",
       " 884: '106400_UNKN',\n",
       " 885: '10660_UNKN',\n",
       " 886: '106_UNKN',\n",
       " 887: '106к_UNKN',\n",
       " 888: '106сильный_UNKN',\n",
       " 889: '1070_UNKN',\n",
       " 890: '1070ti_UNKN',\n",
       " 891: '107218кмч_UNKN',\n",
       " 892: '1074_UNKN',\n",
       " 893: '1075932mail_UNKN',\n",
       " 894: '1075_UNKN',\n",
       " 895: '10770_UNKN',\n",
       " 896: '1078620368940859_UNKN',\n",
       " 897: '1079_UNKN',\n",
       " 898: '107_UNKN',\n",
       " 899: '107ми_UNKN',\n",
       " 900: '10800_UNKN',\n",
       " 901: '1080_UNKN',\n",
       " 902: '1080gtx_UNKN',\n",
       " 903: '1080i_UNKN',\n",
       " 904: '1080p_UNKN',\n",
       " 905: '1080ti_UNKN',\n",
       " 906: '1080р_UNKN',\n",
       " 907: '1080ти_UNKN',\n",
       " 908: '1081_UNKN',\n",
       " 909: '1084885492244773410223_UN',\n",
       " 910: '10862_UNKN',\n",
       " 911: '108659_UNKN',\n",
       " 912: '1086_UNKN',\n",
       " 913: '1089_UNKN',\n",
       " 914: '108_UNKN',\n",
       " 915: '109110_UNKN',\n",
       " 916: '1093_UNKN',\n",
       " 917: '1096_UNKN',\n",
       " 918: '109_UNKN',\n",
       " 919: '109к_UNKN',\n",
       " 920: '109лс_UNKN',\n",
       " 921: '10_UNKN',\n",
       " 922: '10g_UNKN',\n",
       " 923: '10h_UNKN',\n",
       " 924: '10k_UNKN',\n",
       " 925: '10minutemail_UNKN',\n",
       " 926: '10p_UNKN',\n",
       " 927: '10x8_UNKN',\n",
       " 928: '10xx_UNKN',\n",
       " 929: '10³⁰⁰_UNKN',\n",
       " 930: '10¹⁸_UNKN',\n",
       " 931: '10а_UNKN',\n",
       " 932: '10б_UNKN',\n",
       " 933: '10бальной_UNKN',\n",
       " 934: '10бесконечность_UNKN',\n",
       " 935: '10воды10песка1цемент_UNKN',\n",
       " 936: '10вт_UNKN',\n",
       " 937: '10га_UNKN',\n",
       " 938: '10гб_UNKN',\n",
       " 939: '10го_UNKN',\n",
       " 940: '10грамм_UNKN',\n",
       " 941: '10десять_UNKN',\n",
       " 942: '10е_UNKN',\n",
       " 943: '10есяток_UNKN',\n",
       " 944: '10есять_UNKN',\n",
       " 945: '10есятью_UNKN',\n",
       " 946: '10и15и_UNKN',\n",
       " 947: '10и_UNKN',\n",
       " 948: '10из10_UNKN',\n",
       " 949: '10й_UNKN',\n",
       " 950: '10к_UNKN',\n",
       " 951: '10ка_UNKN',\n",
       " 952: '10кбитс_UNKN',\n",
       " 953: '10кв_UNKN',\n",
       " 954: '10кг_UNKN',\n",
       " 955: '10кгсм³_UNKN',\n",
       " 956: '10ке_UNKN',\n",
       " 957: '10ки_UNKN',\n",
       " 958: '10кило_UNKN',\n",
       " 959: '10кк_UNKN',\n",
       " 960: '10ккк_UNKN',\n",
       " 961: '10км_UNKN',\n",
       " 962: '10кой_UNKN',\n",
       " 963: '10коп_UNKN',\n",
       " 964: '10копеечные_UNKN',\n",
       " 965: '10копеечными_UNKN',\n",
       " 966: '10которые_UNKN',\n",
       " 967: '10кратную_UNKN',\n",
       " 968: '10ку_UNKN',\n",
       " 969: '10л_UNKN',\n",
       " 970: '10лет_UNKN',\n",
       " 971: '10летие_UNKN',\n",
       " 972: '10летия_UNKN',\n",
       " 973: '10летнего_UNKN',\n",
       " 974: '10летней_UNKN',\n",
       " 975: '10летний_UNKN',\n",
       " 976: '10летним_UNKN',\n",
       " 977: '10летних_UNKN',\n",
       " 978: '10летняя_UNKN',\n",
       " 979: '10м1атм_UNKN',\n",
       " 980: '10м3_UNKN',\n",
       " 981: '10м_UNKN',\n",
       " 982: '10мб_UNKN',\n",
       " 983: '10мбитс_UNKN',\n",
       " 984: '10мбс_UNKN',\n",
       " 985: '10мг_UNKN',\n",
       " 986: '10месячный_UNKN',\n",
       " 987: '10метров_UNKN',\n",
       " 988: '10метровую_UNKN',\n",
       " 989: '10метровый_UNKN',\n",
       " 990: '10миль_UNKN',\n",
       " 991: '10мин_UNKN',\n",
       " 992: '10минут_UNKN',\n",
       " 993: '10минутах_UNKN',\n",
       " 994: '10млн_UNKN',\n",
       " 995: '10мм_UNKN',\n",
       " 996: '10можно_UNKN',\n",
       " 997: '10нм_UNKN',\n",
       " 998: '10но_UNKN',\n",
       " 999: '10ну_UNKN',\n",
       " ...}"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voc.idx2char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_arr = None\n",
    "for i in range(2):\n",
    "    with np.load(f'tokenized_files/np_tokens{i}.npz') as data:\n",
    "        if np_arr is not None:\n",
    "            np_arr = np.append(np_arr,data['arr'],axis=0)\n",
    "        else:\n",
    "            np_arr = data['arr']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000000, 40)"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8000.0"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_arr.nbytes/1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['убирать_VERB', '600_UNKN', 'литр_NOUN', ..., '', '', ''],\n",
       "       ['убирать_VERB', '600_UNKN', 'литр_NOUN', ..., '', '', ''],\n",
       "       ['убирать_VERB', '600_UNKN', 'литр_NOUN', ..., '', '', ''],\n",
       "       ...,\n",
       "       ['сложно_ADV', 'олдфаг_NOUN', 'зарегистрировать_VERB', ..., '',\n",
       "        '', ''],\n",
       "       ['зарегистрировать_VERB', 'месяц_NOUN', 'олдфаг_NOUN', ..., '',\n",
       "        '', ''],\n",
       "       ['это_PRON', 'читать_VERB', 'сидеть_VERB', ..., '', '', '']],\n",
       "      dtype='<U25')"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 264 ms, sys: 10.4 ms, total: 274 ms\n",
      "Wall time: 260 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'139_UNKN'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time np.unique(np_context)[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    " \n",
    "json1 = json.dumps(voc.idx2char)\n",
    "f = open(\"idx2char.json\",\"w\")\n",
    "f.write(json1)\n",
    "f.close()\n",
    "json2 = json.dumps(voc.char2idx)\n",
    "f = open(\"char2idx.json\",\"w\")\n",
    "f.write(json2)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0,) \n",
      "(1,) 600_UNKN\n",
      "(2,) большой_ADJ\n",
      "(3,) лес_NOUN\n",
      "(4,) литр_NOUN\n",
      "(5,) молодец_NOUN\n",
      "(6,) мусор_NOUN\n",
      "(7,) просто_PART\n",
      "(8,) реликтовый_ADJ\n",
      "(9,) спасибо_NOUN\n",
      "(10,) убирать_VERB\n"
     ]
    }
   ],
   "source": [
    "for idx,token in np.ndenumerate(np.unique(np_context[:2])):\n",
    "    print(idx,token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np_context' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-c57f35f272d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0mnp_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np_context' is not defined"
     ]
    }
   ],
   "source": [
    "del np_context\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = np.array([3,4,3,6])\n",
    "r.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['3', '4', '3', '6', '', '', '', '', '', '', '', '', '', '', '', '',\n",
       "       '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '',\n",
       "       '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '',\n",
       "       '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '',\n",
       "       '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '',\n",
       "       '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '',\n",
       "       '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '',\n",
       "       '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '',\n",
       "       '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '',\n",
       "       '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '',\n",
       "       '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '',\n",
       "       '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '',\n",
       "       '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '',\n",
       "       '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '',\n",
       "       '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '',\n",
       "       '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '',\n",
       "       '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '',\n",
       "       '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '',\n",
       "       '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '',\n",
       "       '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '',\n",
       "       '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '',\n",
       "       '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '',\n",
       "       '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '',\n",
       "       '', '', '', '', '', '', '', '', '', ''], dtype='<U1')"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.empty([400], dtype=\"str\")\n",
    "y[:r.shape[0]] = r\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_id_vocab = {}\n",
    "id_text_vocab = {}\n",
    "prev_word = None\n",
    "\n",
    "def text_prep_tags(text):\n",
    "   \n",
    "    #print(text)\n",
    "    text = text[0]\n",
    "    upos_map = {'A':'ADJ','ADV':'ADV','ADVPRO':'ADV','ANUM':'ADJ','APRO':'DET','COM':'ADJ','CONJ':'SCON','INTJ':'INTJ','NONLEX':'X','NUM':'NUM','PART':'PART','PR':'ADP','S':'NOUN','SPRO':'PRON','UNKN':'X' ,'V':'VERB'}\n",
    "    text = text.lower()\n",
    "    result = np.array([])\n",
    "    \n",
    "    \n",
    "    # Убираем лишние символы\n",
    "    #text = re.sub(r'[;,]',r' ',text).strip()\n",
    "    text = re.sub(r'[^\\w\\s\\.]',r'',text).strip()   \n",
    "    #text = [token.text for token in razdel.tokenize(text)]\n",
    "    # Делаем лемматизацию       \n",
    "#     text = [lemma for lemma in mystem.lemmatize(text) if not lemma.isspace() and lemma not in sw_ru\n",
    "#             and lemma.strip() not in ['.','..','...']]\n",
    "    \n",
    "#     if id_text_vocab != {}:\n",
    "#         prev_word = id_text_vocab[max(id_text_vocab.keys())]\n",
    "#     else:       \n",
    "#         prev_word = None\n",
    "        \n",
    "    for item in mystem.analyze(text):\n",
    "      #  print(item)\n",
    "        token = None\n",
    "        if item.get('analysis'):\n",
    "            lemma = item['analysis'][0]['lex']\n",
    "            pos = re.split('[=,]', item['analysis'][0]['gr'])[0]\n",
    "            #and lemma not in sw_ru\n",
    "            if not lemma.isspace()  and lemma.strip() not in ['.','..','...'] and lemma not in sw_ru:\n",
    "     \n",
    "                token = f'{lemma}_{upos_map[pos]}'\n",
    "        else:\n",
    "            lem_text = item[\"text\"]\n",
    "            if not lem_text.isspace() and lem_text.strip() not in ['.','..','...'] and lem_text not in sw_ru:\n",
    "            \n",
    "                token = f'{lem_text}_UNKN'\n",
    "            \n",
    "        if token:    \n",
    "            #result.append(token)\n",
    "            result = np.append(result,token)\n",
    "            \n",
    "#             if prev_word:\n",
    "#                 text_id_vocab[token] = text_id_vocab[prev_word] + 1\n",
    "#                 id_text_vocab[text_id_vocab[prev_word] + 1] = token\n",
    "#             else:\n",
    "#                 text_vocab[token] = 4\n",
    "#                 id_text_vocab[4] = token\n",
    "#             prev_word = token\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Лемматизатор неправильно разбивает слова с дефисом, поэтому исправляем это\n",
    "#     if '-' in text:\n",
    "#         for l in range(len(text)):\n",
    "#             if text[l] == '-':\n",
    "#                 text[l] = f'{text[l-1]}-{text[l+1]}'\n",
    "#                 text[l-1] = text[l+1] = text[l]\n",
    "    zer = np.empty([40], dtype=\"U25\")\n",
    "    size = result.shape[0]\n",
    "    if size>40:\n",
    "        zer = result[:40]\n",
    "    else:\n",
    "        zer[:size] = result\n",
    "        \n",
    "    return zer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = KeyedVectors.load_word2vec_format('../../model.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100001/100001 [02:23<00:00, 698.76it/s]\n"
     ]
    }
   ],
   "source": [
    "train_token_text = df_train.loc[:100000,'context'].progress_apply(text_prep_tags)\n",
    "#test_token_text = df_test['text'].progress_apply(text_prep_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 148710/21486165 [03:47<8:19:13, 712.36it/s] "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-84-787dacba18ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_token_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'context'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogress_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_prep_tags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tqdm/_tqdm.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(df, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    658\u001b[0m                 \u001b[0;31m# Apply the provided function (in **kwargs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    659\u001b[0m                 \u001b[0;31m# on the df using our wrapper (which provides bar updating)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 660\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    662\u001b[0m                 \u001b[0;31m# Close bar and return pandas calculation result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   3192\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3193\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3194\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3196\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/src/inference.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tqdm/_tqdm.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    654\u001b[0m                     \u001b[0;31m# take a fast or slow code path; so stop when t.total==t.n\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m                     \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 656\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m                 \u001b[0;31m# Apply the provided function (in **kwargs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-83-10edf92feabb>\u001b[0m in \u001b[0;36mtext_prep_tags\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m#         prev_word = None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmystem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m       \u001b[0;31m#  print(item)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pymystem3/mystem.py\u001b[0m in \u001b[0;36manalyze\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_analyze_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pymystem3/mystem.py\u001b[0m in \u001b[0;36m_analyze_impl\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    286\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m             \u001b[0mselect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_procout_no\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_token_text = df_train['context'].progress_apply(text_prep_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_v = CountVectorizer(tokenizer=text_prep_tags)\n",
    "count_v.fit(tqdm_notebook(pd.concat([df_train['text'],df_test['text']])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         [None, None, None, None, None, None, None, Non...\n",
       "1         [None, None, None, None, None, None, None, Non...\n",
       "2         [None, None, None, None, None, None, None, Non...\n",
       "3         [т_NOUN, None, None, None, None, None, None, N...\n",
       "4         [None, None, None, None, None, None, None, Non...\n",
       "5         [т_NOUN, None, None, None, None, None, None, N...\n",
       "6         [None, None, None, None, None, None, None, Non...\n",
       "7         [None, None, None, None, None, None, None, Non...\n",
       "8         [None, None, None, None, None, None, None, Non...\n",
       "9         [None, None, None, None, None, None, None, Non...\n",
       "10        [None, None, None, None, None, None, None, Non...\n",
       "11        [None, None, None, None, None, None, None, Non...\n",
       "12        [г_NOUN, None, None, None, None, None, None, N...\n",
       "13        [д_NOUN, None, None, None, None, None, None, N...\n",
       "14        [г_NOUN, None, None, None, None, None, None, N...\n",
       "15        [None, None, None, None, None, None, None, Non...\n",
       "16        [г_NOUN, None, None, None, None, None, None, N...\n",
       "17        [д_NOUN, None, None, None, None, None, None, N...\n",
       "18        [None, None, None, None, None, None, None, Non...\n",
       "19        [None, None, None, None, None, None, None, Non...\n",
       "20        [None, None, None, None, None, None, None, Non...\n",
       "21        [None, None, None, None, None, None, None, Non...\n",
       "22        [г_NOUN, None, None, None, None, None, None, N...\n",
       "23        [г_NOUN, None, None, None, None, None, None, N...\n",
       "24        [None, None, None, None, None, None, None, Non...\n",
       "25        [None, None, None, None, None, None, None, Non...\n",
       "26        [None, None, None, None, None, None, None, Non...\n",
       "27        [None, None, None, None, None, None, None, Non...\n",
       "28        [None, None, None, None, None, None, None, Non...\n",
       "29        [None, None, None, None, None, None, None, Non...\n",
       "                                ...                        \n",
       "99971     [None, None, None, None, None, None, None, Non...\n",
       "99972     [None, None, None, None, None, None, None, Non...\n",
       "99973     [None, None, None, None, None, None, None, Non...\n",
       "99974     [р_NOUN, None, None, None, None, None, None, N...\n",
       "99975     [None, None, None, None, None, None, None, Non...\n",
       "99976     [None, None, None, None, None, None, None, Non...\n",
       "99977     [д_NOUN, None, None, None, None, None, None, N...\n",
       "99978     [т_NOUN, None, None, None, None, None, None, N...\n",
       "99979     [None, None, None, None, None, None, None, Non...\n",
       "99980     [н_NOUN, None, None, None, None, None, None, N...\n",
       "99981     [None, None, None, None, None, None, None, Non...\n",
       "99982     [None, None, None, None, None, None, None, Non...\n",
       "99983     [None, None, None, None, None, None, None, Non...\n",
       "99984     [н_NOUN, None, None, None, None, None, None, N...\n",
       "99985     [н_NOUN, None, None, None, None, None, None, N...\n",
       "99986     [None, None, None, None, None, None, None, Non...\n",
       "99987     [ч_NOUN, None, None, None, None, None, None, N...\n",
       "99988     [None, None, None, None, None, None, None, Non...\n",
       "99989     [н_NOUN, None, None, None, None, None, None, N...\n",
       "99990     [None, None, None, None, None, None, None, Non...\n",
       "99991     [ч_NOUN, None, None, None, None, None, None, N...\n",
       "99992     [None, None, None, None, None, None, None, Non...\n",
       "99993     [т_NOUN, None, None, None, None, None, None, N...\n",
       "99994     [н_NOUN, None, None, None, None, None, None, N...\n",
       "99995     [н_NOUN, None, None, None, None, None, None, N...\n",
       "99996     [н_NOUN, None, None, None, None, None, None, N...\n",
       "99997     [None, None, None, None, None, None, None, Non...\n",
       "99998     [None, None, None, None, None, None, None, Non...\n",
       "99999     [н_NOUN, None, None, None, None, None, None, N...\n",
       "100000    [None, None, None, None, None, None, None, Non...\n",
       "Name: context, Length: 100001, dtype: object"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_token_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
