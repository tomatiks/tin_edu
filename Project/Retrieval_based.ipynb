{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.0-rc1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import string\n",
    "import tempfile\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tensorflow.python.keras.preprocessing import sequence\n",
    "from tensorboard import summary as summary_lib\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem\n",
    "from tqdm import tqdm_notebook,tqdm_pandas,tqdm\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "import nltk\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import tf_metrics\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "from multiprocessing import cpu_count, Pool\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "TqdmDeprecationWarning: Please use `tqdm.pandas(...)` instead of `tqdm_pandas(tqdm, ...)`.\n"
     ]
    }
   ],
   "source": [
    "tqdm_pandas(tqdm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading file     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('pikabu.csv',sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Убрал 600 литров мусора в реликтовом лесу. Ты...</td>\n",
       "      <td>Спасибо.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Убрал 600 литров мусора в реликтовом лесу. Ты ...</td>\n",
       "      <td>Приедь к нам в Мурманск пожалуйста.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Убрал 600 литров мусора в реликтовом лесу. Ты ...</td>\n",
       "      <td>Тебе платят за это?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ты просто большой молодец. Спасибо. Тебе платя...</td>\n",
       "      <td>За посты на Пикабу? Да, платят. Достаточно при...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Спасибо. Тебе платят за это? За посты на Пикаб...</td>\n",
       "      <td>Не нажимается сук</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Ты просто большой молодец. Спасибо. Тебе платя...</td>\n",
       "      <td>Лично Путин</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Спасибо. Тебе платят за это? Лично Путин</td>\n",
       "      <td>дарт вейдар</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Убрал 600 литров мусора в реликтовом лесу. Ты...</td>\n",
       "      <td>ну всё, теперь можно не убирать за собой, Чист...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Убрал 600 литров мусора в реликтовом лесу. Ты ...</td>\n",
       "      <td>Зачем заминусили человека? Очевидная ирония же...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Убрал 600 литров мусора в реликтовом лесу.</td>\n",
       "      <td>Герой, про которого я бы читал комиксы от dc и...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Убрал 600 литров мусора в реликтовом лесу. Ге...</td>\n",
       "      <td>да хотя бы от bubble студии)) но чет не пишут))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Убрал 600 литров мусора в реликтовом лесу. Гер...</td>\n",
       "      <td>может силами пикабу родится комикс, хехе)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Герой, про которого я бы читал комиксы от dc и...</td>\n",
       "      <td>Зовите Чилика...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>да хотя бы от bubble студии)) но чет не пишут)...</td>\n",
       "      <td>/@Chiliktolik , ищем автора комиксов про чисто...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Герой, про которого я бы читал комиксы от dc и...</td>\n",
       "      <td>Только комикс и может родится. Нет бы выйти да...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Убрал 600 литров мусора в реликтовом лесу. Гер...</td>\n",
       "      <td>ты в маске убираешь или она только для деавтор...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Герой, про которого я бы читал комиксы от dc и...</td>\n",
       "      <td>маска только для фото</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>да хотя бы от bubble студии)) но чет не пишут)...</td>\n",
       "      <td>понятно. А так молодец).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Убрал 600 литров мусора в реликтовом лесу. Гер...</td>\n",
       "      <td>Реликтовый лес? G-unit не нашел случаем?)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Убрал 600 литров мусора в реликтовом лесу. Гер...</td>\n",
       "      <td>Нужно мерчендайз организовывать</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              context  \\\n",
       "0    Убрал 600 литров мусора в реликтовом лесу. Ты...   \n",
       "1   Убрал 600 литров мусора в реликтовом лесу. Ты ...   \n",
       "2   Убрал 600 литров мусора в реликтовом лесу. Ты ...   \n",
       "3   Ты просто большой молодец. Спасибо. Тебе платя...   \n",
       "4   Спасибо. Тебе платят за это? За посты на Пикаб...   \n",
       "5   Ты просто большой молодец. Спасибо. Тебе платя...   \n",
       "6            Спасибо. Тебе платят за это? Лично Путин   \n",
       "7    Убрал 600 литров мусора в реликтовом лесу. Ты...   \n",
       "8   Убрал 600 литров мусора в реликтовом лесу. Ты ...   \n",
       "9          Убрал 600 литров мусора в реликтовом лесу.   \n",
       "10   Убрал 600 литров мусора в реликтовом лесу. Ге...   \n",
       "11  Убрал 600 литров мусора в реликтовом лесу. Гер...   \n",
       "12  Герой, про которого я бы читал комиксы от dc и...   \n",
       "13  да хотя бы от bubble студии)) но чет не пишут)...   \n",
       "14  Герой, про которого я бы читал комиксы от dc и...   \n",
       "15  Убрал 600 литров мусора в реликтовом лесу. Гер...   \n",
       "16  Герой, про которого я бы читал комиксы от dc и...   \n",
       "17  да хотя бы от bubble студии)) но чет не пишут)...   \n",
       "18  Убрал 600 литров мусора в реликтовом лесу. Гер...   \n",
       "19  Убрал 600 литров мусора в реликтовом лесу. Гер...   \n",
       "\n",
       "                                               answer  \n",
       "0                                            Спасибо.  \n",
       "1                 Приедь к нам в Мурманск пожалуйста.  \n",
       "2                                 Тебе платят за это?  \n",
       "3   За посты на Пикабу? Да, платят. Достаточно при...  \n",
       "4                                   Не нажимается сук  \n",
       "5                                         Лично Путин  \n",
       "6                                         дарт вейдар  \n",
       "7   ну всё, теперь можно не убирать за собой, Чист...  \n",
       "8   Зачем заминусили человека? Очевидная ирония же...  \n",
       "9   Герой, про которого я бы читал комиксы от dc и...  \n",
       "10    да хотя бы от bubble студии)) но чет не пишут))  \n",
       "11          может силами пикабу родится комикс, хехе)  \n",
       "12                                   Зовите Чилика...  \n",
       "13  /@Chiliktolik , ищем автора комиксов про чисто...  \n",
       "14  Только комикс и может родится. Нет бы выйти да...  \n",
       "15  ты в маске убираешь или она только для деавтор...  \n",
       "16                              маска только для фото  \n",
       "17                           понятно. А так молодец).  \n",
       "18          Реликтовый лес? G-unit не нашел случаем?)  \n",
       "19                    Нужно мерчендайз организовывать  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "context    21486165\n",
       "answer     21486162\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_train.loc[df_train['context'].apply(len)>410,'context'][271713].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{4: 'убирать_VERB'}"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "sw_ru = set(stopwords.words('russian'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "mystem = Mystem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = df_train['context'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_tokenizer(tokenizer, corpus):\n",
    "    #voc_set = set()\n",
    "    res = np.fromiter(map(tokenizer,tqdm_notebook(corpus[:20]),),dtype=str)\n",
    "    voc_set = np.unique(res)\n",
    "    print(voc_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fu(x):\n",
    "    return np.apply_along_axis(text_prep_tags,1,np.reshape(x,(-1,1)))\n",
    "        \n",
    "def parallelize(data,partitions):\n",
    "\n",
    "    data_split = np.array_split(data, partitions)\n",
    "    pool = Pool(partitions)\n",
    "    #data = np.stack(pool.apply(np.apply_along_axis,text_prep_tags,1,np.reshape(data_split,(-1,1))))\n",
    "    data = np.concatenate(pool.map(fu,data_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if type(6) is not str:\n",
    "        print('1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gsd_gdsg dgsdg dfdsg fdfs'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(r'([ _!?])+',r'\\1','gsd____gdsg   dgsdg dfdsg!     fdfs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_id_vocab = {}\n",
    "id_text_vocab = {}\n",
    "prev_word = None\n",
    "\n",
    "def text_prep_tags(text):\n",
    "   \n",
    "    #print(text)\n",
    "    text = text[0]\n",
    "    if type(text) is not str:\n",
    "        text= str(text)\n",
    "    upos_map = {'A':'ADJ','ADV':'ADV','ADVPRO':'ADV','ANUM':'ADJ','APRO':'DET','COM':'ADJ','CONJ':'SCON','INTJ':'INTJ','NONLEX':'X','NUM':'NUM','PART':'PART','PR':'ADP','S':'NOUN','SPRO':'PRON','UNKN':'X' ,'V':'VERB'}\n",
    "    text = text.lower()\n",
    "    result = np.array([])\n",
    "    \n",
    "    \n",
    "    # Убираем лишние символы\n",
    "    #text = re.sub(r'[;,]',r' ',text).strip()\n",
    "    text = re.sub(r'[^\\w\\s\\.]',r'',text).strip() \n",
    "    text = re.sub(r'([ _!?])+',r'\\1',text)\n",
    "    #text = [token.text for token in razdel.tokenize(text)]\n",
    "    # Делаем лемматизацию       \n",
    "#     text = [lemma for lemma in mystem.lemmatize(text) if not lemma.isspace() and lemma not in sw_ru\n",
    "#             and lemma.strip() not in ['.','..','...']]\n",
    "    \n",
    "#     if id_text_vocab != {}:\n",
    "#         prev_word = id_text_vocab[max(id_text_vocab.keys())]\n",
    "#     else:       \n",
    "#         prev_word = None\n",
    "        \n",
    "    for item in mystem.analyze(text):\n",
    "      #  print(item)\n",
    "        token = None\n",
    "        if item.get('analysis'):\n",
    "            lemma = item['analysis'][0]['lex']\n",
    "            pos = re.split('[=,]', item['analysis'][0]['gr'])[0]\n",
    "            #and lemma not in sw_ru\n",
    "            if not lemma.isspace()  and lemma.strip() not in ['.','..','...'] and lemma not in sw_ru:\n",
    "     \n",
    "                token = f'{lemma}_{upos_map[pos]}'\n",
    "        else:\n",
    "            lem_text = item[\"text\"]\n",
    "            if not lem_text.isspace() and lem_text.strip() not in ['.','..','...'] and lem_text not in sw_ru:\n",
    "            \n",
    "                token = f'{lem_text}_UNKN'\n",
    "            \n",
    "        if token:    \n",
    "            #result.append(token)\n",
    "            result = np.append(result,token)\n",
    "            \n",
    "#             if prev_word:\n",
    "#                 text_id_vocab[token] = text_id_vocab[prev_word] + 1\n",
    "#                 id_text_vocab[text_id_vocab[prev_word] + 1] = token\n",
    "#             else:\n",
    "#                 text_vocab[token] = 4\n",
    "#                 id_text_vocab[4] = token\n",
    "#             prev_word = token\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Лемматизатор неправильно разбивает слова с дефисом, поэтому исправляем это\n",
    "#     if '-' in text:\n",
    "#         for l in range(len(text)):\n",
    "#             if text[l] == '-':\n",
    "#                 text[l] = f'{text[l-1]}-{text[l+1]}'\n",
    "#                 text[l-1] = text[l+1] = text[l]\n",
    "    zer = np.empty([40], dtype=\"U25\")\n",
    "    zer[:] = '<PAD>'\n",
    "    size = result.shape[0]\n",
    "    if size>40:\n",
    "        zer = result[:40]\n",
    "    else:\n",
    "        zer[:size] = result\n",
    "        \n",
    "    return zer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.char2idx = {'<PAD>':0,'<START>':1,'<END>':2,'<UNK>':3}\n",
    "        self.idx2char = {i:ch for ch,i in self.char2idx.items()}\n",
    "        self.last_index = 4\n",
    "    \n",
    "    def save(self, path='.'):\n",
    "        json_ch_idx = json.dumps(self.char2idx)\n",
    "        with open(os.path.join(path,\"char2idx.json\"),\"w\") as f:\n",
    "            f.write(json_ch_idx)\n",
    "        json_idx_ch = json.dumps(self.idx2char)\n",
    "        with open(os.path.join(path,\"idx2char.json\"),\"w\") as f:\n",
    "            f.write(json_idx_ch)\n",
    "    \n",
    "    def load(self, path='.'):       \n",
    "        with open(os.path.join(path,\"char2idx.json\"),\"r\") as f:\n",
    "            self.char2idx = json.loads(f.read())        \n",
    "        with open(os.path.join(path,\"idx2char.json\"),\"r\") as f:\n",
    "            self.idx2char = json.loads(f.read())\n",
    "        \n",
    "    \n",
    "    def indx_tokenize_from_files(self, huge_parts, name):\n",
    "        if not os.path.exists(f'{name}_indexed_token_files'):\n",
    "            os.mkdir(f'{name}_indexed_token_files')\n",
    "        for i in tqdm_notebook(range(huge_parts)):\n",
    "            with np.load(f'{name}_tokenized_files/np_tokens{i}.npz') as data:\n",
    "                np_idxs = np.apply_along_axis(np.vectorize(self.char2idx.get),1,data['arr'])\n",
    "                np.savez_compressed(f'{name}_indexed_token_files/indx_tokens{i}',arr=np_idxs)\n",
    "                print(f'{name} indx_tokenize part - {i}')\n",
    "                \n",
    "        \n",
    "    \n",
    "    def indx_detokenize(self, sequence):\n",
    "        return ''.join([self.idx2char[idx] for idx in sequence])\n",
    "    \n",
    "    def tokenize(self, data, tokenizer, huge_parts, name ,save_to_file=True):\n",
    "        huge_splits = np.array_split(data, huge_parts)\n",
    "        partitions = cpu_count()\n",
    "        for split_number,huge_split in enumerate(tqdm_notebook(huge_splits)):\n",
    "                      \n",
    "            data_split = np.array_split(huge_split, partitions)\n",
    "            pool = Pool(partitions)\n",
    "\n",
    "            res_data = np.concatenate(pool.map(tokenizer,data_split))\n",
    "            pool.close()\n",
    "            pool.join()\n",
    "\n",
    "            self._extend_vocab(res_data)\n",
    "            \n",
    "            print(f'{name} first tokenize part - {split_number}')\n",
    "            \n",
    "            if save_to_file:\n",
    "                if not os.path.exists(f'{name}_tokenized_files'):\n",
    "                    os.mkdir(f'{name}_tokenized_files')\n",
    "                np.savez_compressed(f'{name}_tokenized_files/np_tokens{split_number}',arr=res_data)             \n",
    "                del res_data\n",
    "                gc.collect()\n",
    "                \n",
    "                \n",
    "    def _extend_vocab(self,token_matrix):\n",
    "        print(f'enter {self.last_index}')\n",
    "        start_idx = self.last_index\n",
    "        next_indx = 0\n",
    "        for idx,token in np.ndenumerate(np.unique(token_matrix)):\n",
    "            if token not in self.char2idx:\n",
    "                next_indx = start_idx + idx[0]\n",
    "                self.char2idx[token] = next_indx\n",
    "                \n",
    "        if next_indx > self.last_index:\n",
    "            self.last_index = next_indx+1\n",
    "        print(f'exit {self.last_index}')\n",
    "        self.idx2char = {i:ch for ch,i in self.char2idx.items()}\n",
    "                \n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.char2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def help_token(x):\n",
    "    return np.apply_along_axis(text_prep_tags,1,np.reshape(x,(-1,1)))\n",
    "\n",
    "voc = Vocab()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d89881a8e24249e0a66319056d158452",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=20), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter 4\n",
      "exit 199505\n",
      "context first tokenize part - 0\n",
      "enter 199505\n",
      "exit 372784\n",
      "context first tokenize part - 1\n",
      "enter 372784\n",
      "exit 546224\n",
      "context first tokenize part - 2\n",
      "enter 546224\n",
      "exit 721705\n",
      "context first tokenize part - 3\n",
      "enter 721705\n",
      "exit 899093\n",
      "context first tokenize part - 4\n",
      "enter 899093\n",
      "exit 1067692\n",
      "context first tokenize part - 5\n",
      "enter 1067692\n",
      "exit 1238877\n",
      "context first tokenize part - 6\n",
      "enter 1238877\n",
      "exit 1419549\n",
      "context first tokenize part - 7\n",
      "enter 1419549\n",
      "exit 1600915\n",
      "context first tokenize part - 8\n",
      "enter 1600915\n",
      "exit 1781883\n",
      "context first tokenize part - 9\n",
      "enter 1781883\n",
      "exit 1960969\n",
      "context first tokenize part - 10\n",
      "enter 1960969\n",
      "exit 2139124\n",
      "context first tokenize part - 11\n",
      "enter 2139124\n",
      "exit 2321055\n",
      "context first tokenize part - 12\n",
      "enter 2321055\n",
      "exit 2499659\n",
      "context first tokenize part - 13\n",
      "enter 2499659\n",
      "exit 2685939\n",
      "context first tokenize part - 14\n",
      "enter 2685939\n",
      "exit 2863019\n",
      "context first tokenize part - 15\n",
      "enter 2863019\n",
      "exit 3034366\n",
      "context first tokenize part - 16\n",
      "enter 3034366\n",
      "exit 3207463\n",
      "context first tokenize part - 17\n",
      "enter 3207463\n",
      "exit 3377110\n",
      "context first tokenize part - 18\n",
      "enter 3377110\n",
      "exit 3545778\n",
      "context first tokenize part - 19\n",
      "CPU times: user 13min 2s, sys: 2min 3s, total: 15min 5s\n",
      "Wall time: 1h 23min 7s\n"
     ]
    }
   ],
   "source": [
    "%time voc.tokenize(df_train['context'].values,help_token,20,name = 'context')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee5e0d4685c24c0385e1f363fc18449f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=20), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter 3545778\n",
      "exit 3806923\n",
      "answer first tokenize part - 0\n",
      "enter 3806923\n",
      "exit 4064291\n",
      "answer first tokenize part - 1\n",
      "enter 4064291\n",
      "exit 4305576\n",
      "answer first tokenize part - 2\n",
      "enter 4305576\n",
      "exit 4456098\n",
      "answer first tokenize part - 3\n",
      "enter 4456098\n",
      "exit 4607016\n",
      "answer first tokenize part - 4\n",
      "enter 4607016\n",
      "exit 4753000\n",
      "answer first tokenize part - 5\n",
      "enter 4753000\n",
      "exit 4899703\n",
      "answer first tokenize part - 6\n",
      "enter 4899703\n",
      "exit 5055819\n",
      "answer first tokenize part - 7\n",
      "enter 5055819\n",
      "exit 5213058\n",
      "answer first tokenize part - 8\n",
      "enter 5213058\n",
      "exit 5368148\n",
      "answer first tokenize part - 9\n",
      "enter 5368148\n",
      "exit 5521843\n",
      "answer first tokenize part - 10\n",
      "enter 5521843\n",
      "exit 5675205\n",
      "answer first tokenize part - 11\n",
      "enter 5675205\n",
      "exit 5830724\n",
      "answer first tokenize part - 12\n",
      "enter 5830724\n",
      "exit 5984022\n",
      "answer first tokenize part - 13\n",
      "enter 5984022\n",
      "exit 6141680\n",
      "answer first tokenize part - 14\n",
      "enter 6141680\n",
      "exit 6293327\n",
      "answer first tokenize part - 15\n",
      "enter 6293327\n",
      "exit 6440368\n",
      "answer first tokenize part - 16\n",
      "enter 6440368\n",
      "exit 6589551\n",
      "answer first tokenize part - 17\n",
      "enter 6589551\n",
      "exit 6735080\n",
      "answer first tokenize part - 18\n",
      "enter 6735080\n",
      "exit 6903548\n",
      "answer first tokenize part - 19\n",
      "CPU times: user 12min 17s, sys: 1min 56s, total: 14min 14s\n",
      "Wall time: 42min 25s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%time voc.tokenize(df_train['answer'].values,help_token,20,name = 'answer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0abc3fb72a14778815009ecb81f6d9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=20), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context indx_tokenize part - 0\n",
      "context indx_tokenize part - 1\n",
      "context indx_tokenize part - 2\n",
      "context indx_tokenize part - 3\n",
      "context indx_tokenize part - 4\n",
      "context indx_tokenize part - 5\n",
      "context indx_tokenize part - 6\n",
      "context indx_tokenize part - 7\n",
      "context indx_tokenize part - 8\n",
      "context indx_tokenize part - 9\n",
      "context indx_tokenize part - 10\n",
      "context indx_tokenize part - 11\n",
      "context indx_tokenize part - 12\n",
      "context indx_tokenize part - 13\n",
      "context indx_tokenize part - 14\n",
      "context indx_tokenize part - 15\n",
      "context indx_tokenize part - 16\n",
      "context indx_tokenize part - 17\n",
      "context indx_tokenize part - 18\n",
      "context indx_tokenize part - 19\n",
      "CPU times: user 9min 5s, sys: 22.9 s, total: 9min 28s\n",
      "Wall time: 9min 30s\n"
     ]
    }
   ],
   "source": [
    "%time voc.indx_tokenize_from_files(20,name = 'context')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "022daa5922b74367a9ce9a013c2b67a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=20), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer indx_tokenize part - 0\n",
      "answer indx_tokenize part - 1\n",
      "answer indx_tokenize part - 2\n",
      "answer indx_tokenize part - 3\n",
      "answer indx_tokenize part - 4\n",
      "answer indx_tokenize part - 5\n",
      "answer indx_tokenize part - 6\n",
      "answer indx_tokenize part - 7\n",
      "answer indx_tokenize part - 8\n",
      "answer indx_tokenize part - 9\n",
      "answer indx_tokenize part - 10\n",
      "answer indx_tokenize part - 11\n",
      "answer indx_tokenize part - 12\n",
      "answer indx_tokenize part - 13\n",
      "answer indx_tokenize part - 14\n",
      "answer indx_tokenize part - 15\n",
      "answer indx_tokenize part - 16\n",
      "answer indx_tokenize part - 17\n",
      "answer indx_tokenize part - 18\n",
      "answer indx_tokenize part - 19\n",
      "CPU times: user 8min 40s, sys: 22.3 s, total: 9min 3s\n",
      "Wall time: 9min 5s\n"
     ]
    }
   ],
   "source": [
    "%time voc.indx_tokenize_from_files(20,name = 'answer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['молодец_NOUN' 'самый_DET' 'начало_NOUN']\n",
      "['это_PRON' 'человек_NOUN' 'виноватый_ADJ']\n",
      "['беда_NOUN' 'гринлайт_NOUN' 'плюсонуть_VERB']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 99766, 149023, 105369],\n",
       "       [188210, 181013,  45919],\n",
       "       [ 36253,  56266, 126309]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def yy(x):\n",
    "    print(x)\n",
    "    return np.vectorize(voc1.char2idx.get)(x)\n",
    "np.apply_along_axis(yy,1,np.array([['молодец_NOUN', 'самый_DET' ,'начало_NOUN'],\n",
    " ['это_PRON' ,'человек_NOUN', 'виноватый_ADJ'],\n",
    " ['беда_NOUN', 'гринлайт_NOUN' ,'плюсонуть_VERB']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df4ea49fadb94507938a6ff353c96c7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter 4\n",
      "exit 46395\n",
      "test first tokenize part - 0\n",
      "enter 46395\n",
      "exit 92885\n",
      "test first tokenize part - 1\n",
      "enter 92885\n",
      "exit 139261\n",
      "test first tokenize part - 2\n",
      "enter 139261\n",
      "exit 185802\n",
      "test first tokenize part - 3\n",
      "enter 185802\n",
      "exit 231676\n",
      "test first tokenize part - 4\n",
      "enter 231676\n",
      "exit 277314\n",
      "test first tokenize part - 5\n",
      "enter 277314\n",
      "exit 321918\n",
      "test first tokenize part - 6\n",
      "enter 321918\n",
      "exit 367098\n",
      "test first tokenize part - 7\n",
      "enter 367098\n",
      "exit 411912\n",
      "test first tokenize part - 8\n",
      "enter 411912\n",
      "exit 455786\n",
      "test first tokenize part - 9\n",
      "CPU times: user 32.8 s, sys: 8.84 s, total: 41.6 s\n",
      "Wall time: 3min 8s\n"
     ]
    }
   ],
   "source": [
    "voc_test = Vocab()\n",
    "%time voc_test.tokenize(df_train['context'].values[:1000000],help_token,10,name = 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2335478"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(voc.char2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2335478"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('<U140')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['answer'].values.astype(str).dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'res_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-f6f42e46f066>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#del answer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0mres_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'res_data' is not defined"
     ]
    }
   ],
   "source": [
    "#del answer\n",
    "gc.collect()\n",
    "del res_data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_token_files(name , number):\n",
    "    np_arr = None\n",
    "    for i in range(number):\n",
    "        with np.load(f'{name}_tokenized_files/np_tokens{i}.npz') as data:\n",
    "            if np_arr is not None:\n",
    "                np_arr = np.append(np_arr,data['arr'],axis=0)\n",
    "            else:\n",
    "                np_arr = data['arr']\n",
    "    return np_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_context = read_token_files('context', 1)\n",
    "test_answer = read_token_files('answer', 1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# del test_context\n",
    "# del test_answer\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_arr = None\n",
    "for i in range(1):\n",
    "    with np.load(f'indexed_token_files/indx_tokens{i}.npz') as data:\n",
    "        if np_arr is not None:\n",
    "            np_arr = np.append(np_arr,data['arr'],axis=0)\n",
    "        else:\n",
    "            np_arr = data['arr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000000, 40)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "320.0"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_arr.nbytes/1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[128897, 155086, 150350, ...,      4,      4,      4],\n",
       "       [111158,   2771,  42423, ...,  11618, 115962,  42423],\n",
       "       [158623, 111158,  73738, ...,      4,      4,      4],\n",
       "       ...,\n",
       "       [160165, 132439,  89121, ...,      4,      4,      4],\n",
       "       [ 89121, 188210, 123192, ...,      4,      4,      4],\n",
       "       [ 54942, 170538, 172776, ...,      4,      4,      4]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 264 ms, sys: 10.4 ms, total: 274 ms\n",
      "Wall time: 260 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'139_UNKN'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time np.unique(np_context)[100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = KeyedVectors.load_word2vec_format('../../model.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = './model'\n",
    "vocab_size = len(voc)\n",
    "sentence_size = 40\n",
    "embedding_size = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_fn(x,  params, is_training):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(x)\n",
    "\n",
    "    if is_training:\n",
    "        dataset = dataset.shuffle(buffer_size=params['buffer_size'])\n",
    "        dataset = dataset.repeat(count=params['num_epochs'])\n",
    "\n",
    "    dataset = dataset.batch(params['batch_size'])\n",
    "    dataset = dataset.prefetch(buffer_size=2)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    return iterator.get_next()\n",
    "    #return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def initializer(shape=None, dtype=None, partition_info=None):    \n",
    "    vocab_dict = voc.char2idx\n",
    "    embedding_matrix = np.random.uniform(-1, 1, size=(vocab_size, embedding_size))\n",
    "    num_loaded = 0\n",
    "    for w, i in vocab_dict.items():\n",
    "        v = None\n",
    "        try:\n",
    "            v = word2vec[w]\n",
    "        except KeyError: # не нашли такой токен в словаре\n",
    "                pass\n",
    "        if v is not None :\n",
    "            embedding_matrix[i] = v\n",
    "            num_loaded += 1\n",
    "   \n",
    "    embedding_matrix = embedding_matrix.astype(np.float32)\n",
    "    embedding_matrix[0] = np.zeros(300)\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn(features, labels, mode, params):  \n",
    "    \n",
    "    # Compute predictions.\n",
    "    net = params['net']\n",
    "    \n",
    "    encoded_features = {}  \n",
    "    \n",
    "    with tf.variable_scope('encoder'):\n",
    "        encoded_features['anchor'] = net(features['anchor'])\n",
    "    with tf.variable_scope('encoder', reuse=True):\n",
    "        encoded_features['positive'] = net(features['positive'])\n",
    "    with tf.variable_scope('encoder', reuse=True):\n",
    "        encoded_features['negative'] = net(features['negative'])\n",
    "    \n",
    "    \n",
    "    #predicted_classes = tf.argmax(logits, 1)\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        predictions = {\n",
    "            'class_ids': predicted_classes[:, tf.newaxis],\n",
    "            'probabilities': tf.nn.softmax(logits),\n",
    "            'logits': logits,\n",
    "        }\n",
    "        return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
    "    \n",
    "    # Compute loss.\n",
    "    loss = my_triplet_loss(encoded_features,params['margin'])\n",
    "    \n",
    "#     # Compute evaluation metrics.\n",
    "#     accuracy = tf.metrics.accuracy(labels=labels,\n",
    "#                                predictions=predicted_classes,\n",
    "#                                name='acc_op')\n",
    "#     f1 = tf_metrics.f1(labels=labels,\n",
    "#                                predictions=predicted_classes,num_classes=3,average='micro')\n",
    "    \n",
    "#     metrics = {'accuracy': accuracy,'f1':f1}\n",
    "#     tf.summary.scalar('accuracy', accuracy[1])\n",
    "    \n",
    "#     # Compute evaluation\n",
    "#     if mode == tf.estimator.ModeKeys.EVAL:\n",
    "#         return tf.estimator.EstimatorSpec(\n",
    "#             mode, loss=loss, eval_metric_ops=metrics)\n",
    "    \n",
    "    optimizer = tf.train.AdagradOptimizer(learning_rate=0.1)\n",
    "    \n",
    "    train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())\n",
    "    \n",
    "    return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
