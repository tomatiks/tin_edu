{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import string\n",
    "import tempfile\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tensorflow.python.keras.preprocessing import sequence\n",
    "from tensorboard import summary as summary_lib\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem\n",
    "from tqdm import tqdm_notebook,tqdm_pandas,tqdm\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "import nltk\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import tf_metrics\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "from multiprocessing import cpu_count, Pool\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "TqdmDeprecationWarning: Please use `tqdm.pandas(...)` instead of `tqdm_pandas(tqdm, ...)`.\n"
     ]
    }
   ],
   "source": [
    "tqdm_pandas(tqdm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading file     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('pikabu.csv',sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Убрал 600 литров мусора в реликтовом лесу. Ты...</td>\n",
       "      <td>Спасибо.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Убрал 600 литров мусора в реликтовом лесу. Ты ...</td>\n",
       "      <td>Приедь к нам в Мурманск пожалуйста.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Убрал 600 литров мусора в реликтовом лесу. Ты ...</td>\n",
       "      <td>Тебе платят за это?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ты просто большой молодец. Спасибо. Тебе платя...</td>\n",
       "      <td>За посты на Пикабу? Да, платят. Достаточно при...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Спасибо. Тебе платят за это? За посты на Пикаб...</td>\n",
       "      <td>Не нажимается сук</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Ты просто большой молодец. Спасибо. Тебе платя...</td>\n",
       "      <td>Лично Путин</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Спасибо. Тебе платят за это? Лично Путин</td>\n",
       "      <td>дарт вейдар</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Убрал 600 литров мусора в реликтовом лесу. Ты...</td>\n",
       "      <td>ну всё, теперь можно не убирать за собой, Чист...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Убрал 600 литров мусора в реликтовом лесу. Ты ...</td>\n",
       "      <td>Зачем заминусили человека? Очевидная ирония же...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Убрал 600 литров мусора в реликтовом лесу.</td>\n",
       "      <td>Герой, про которого я бы читал комиксы от dc и...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Убрал 600 литров мусора в реликтовом лесу. Ге...</td>\n",
       "      <td>да хотя бы от bubble студии)) но чет не пишут))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Убрал 600 литров мусора в реликтовом лесу. Гер...</td>\n",
       "      <td>может силами пикабу родится комикс, хехе)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Герой, про которого я бы читал комиксы от dc и...</td>\n",
       "      <td>Зовите Чилика...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>да хотя бы от bubble студии)) но чет не пишут)...</td>\n",
       "      <td>/@Chiliktolik , ищем автора комиксов про чисто...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Герой, про которого я бы читал комиксы от dc и...</td>\n",
       "      <td>Только комикс и может родится. Нет бы выйти да...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Убрал 600 литров мусора в реликтовом лесу. Гер...</td>\n",
       "      <td>ты в маске убираешь или она только для деавтор...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Герой, про которого я бы читал комиксы от dc и...</td>\n",
       "      <td>маска только для фото</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>да хотя бы от bubble студии)) но чет не пишут)...</td>\n",
       "      <td>понятно. А так молодец).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Убрал 600 литров мусора в реликтовом лесу. Гер...</td>\n",
       "      <td>Реликтовый лес? G-unit не нашел случаем?)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Убрал 600 литров мусора в реликтовом лесу. Гер...</td>\n",
       "      <td>Нужно мерчендайз организовывать</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              context  \\\n",
       "0    Убрал 600 литров мусора в реликтовом лесу. Ты...   \n",
       "1   Убрал 600 литров мусора в реликтовом лесу. Ты ...   \n",
       "2   Убрал 600 литров мусора в реликтовом лесу. Ты ...   \n",
       "3   Ты просто большой молодец. Спасибо. Тебе платя...   \n",
       "4   Спасибо. Тебе платят за это? За посты на Пикаб...   \n",
       "5   Ты просто большой молодец. Спасибо. Тебе платя...   \n",
       "6            Спасибо. Тебе платят за это? Лично Путин   \n",
       "7    Убрал 600 литров мусора в реликтовом лесу. Ты...   \n",
       "8   Убрал 600 литров мусора в реликтовом лесу. Ты ...   \n",
       "9          Убрал 600 литров мусора в реликтовом лесу.   \n",
       "10   Убрал 600 литров мусора в реликтовом лесу. Ге...   \n",
       "11  Убрал 600 литров мусора в реликтовом лесу. Гер...   \n",
       "12  Герой, про которого я бы читал комиксы от dc и...   \n",
       "13  да хотя бы от bubble студии)) но чет не пишут)...   \n",
       "14  Герой, про которого я бы читал комиксы от dc и...   \n",
       "15  Убрал 600 литров мусора в реликтовом лесу. Гер...   \n",
       "16  Герой, про которого я бы читал комиксы от dc и...   \n",
       "17  да хотя бы от bubble студии)) но чет не пишут)...   \n",
       "18  Убрал 600 литров мусора в реликтовом лесу. Гер...   \n",
       "19  Убрал 600 литров мусора в реликтовом лесу. Гер...   \n",
       "\n",
       "                                               answer  \n",
       "0                                            Спасибо.  \n",
       "1                 Приедь к нам в Мурманск пожалуйста.  \n",
       "2                                 Тебе платят за это?  \n",
       "3   За посты на Пикабу? Да, платят. Достаточно при...  \n",
       "4                                   Не нажимается сук  \n",
       "5                                         Лично Путин  \n",
       "6                                         дарт вейдар  \n",
       "7   ну всё, теперь можно не убирать за собой, Чист...  \n",
       "8   Зачем заминусили человека? Очевидная ирония же...  \n",
       "9   Герой, про которого я бы читал комиксы от dc и...  \n",
       "10    да хотя бы от bubble студии)) но чет не пишут))  \n",
       "11          может силами пикабу родится комикс, хехе)  \n",
       "12                                   Зовите Чилика...  \n",
       "13  /@Chiliktolik , ищем автора комиксов про чисто...  \n",
       "14  Только комикс и может родится. Нет бы выйти да...  \n",
       "15  ты в маске убираешь или она только для деавтор...  \n",
       "16                              маска только для фото  \n",
       "17                           понятно. А так молодец).  \n",
       "18          Реликтовый лес? G-unit не нашел случаем?)  \n",
       "19                    Нужно мерчендайз организовывать  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "context    21486165\n",
       "answer     21486162\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_train.loc[df_train['context'].apply(len)>410,'context'][271713].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{4: 'убирать_VERB'}"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "sw_ru = set(stopwords.words('russian'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mystem = Mystem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = df_train['context'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_tokenizer(tokenizer, corpus):\n",
    "    #voc_set = set()\n",
    "    res = np.fromiter(map(tokenizer,tqdm_notebook(corpus[:20]),),dtype=str)\n",
    "    voc_set = np.unique(res)\n",
    "    print(voc_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fu(x):\n",
    "    return np.apply_along_axis(text_prep_tags,1,np.reshape(x,(-1,1)))\n",
    "        \n",
    "def parallelize(data,partitions):\n",
    "\n",
    "    data_split = np.array_split(data, partitions)\n",
    "    pool = Pool(partitions)\n",
    "    #data = np.stack(pool.apply(np.apply_along_axis,text_prep_tags,1,np.reshape(data_split,(-1,1))))\n",
    "    data = np.concatenate(pool.map(fu,data_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-153064d275cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mvoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mvoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m200000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtext_prep_tags\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-72-016fc939a790>\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, data, tokenizer, huge_parts, save_to_file)\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msave_to_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-72-016fc939a790>\u001b[0m in \u001b[0;36m_extend_vocab\u001b[0;34m(self, token_matrix)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_extend_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtoken_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mstart_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchar2idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndenumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchar2idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart_idx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 173 ms, sys: 342 ms, total: 515 ms\n",
      "Wall time: 13.2 s\n"
     ]
    }
   ],
   "source": [
    "%time np_context = parallelize(context[:20000],4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_id_vocab = {}\n",
    "id_text_vocab = {}\n",
    "prev_word = None\n",
    "\n",
    "def text_prep_tags(text):\n",
    "   \n",
    "    #print(text)\n",
    "    text = text[0]\n",
    "    upos_map = {'A':'ADJ','ADV':'ADV','ADVPRO':'ADV','ANUM':'ADJ','APRO':'DET','COM':'ADJ','CONJ':'SCON','INTJ':'INTJ','NONLEX':'X','NUM':'NUM','PART':'PART','PR':'ADP','S':'NOUN','SPRO':'PRON','UNKN':'X' ,'V':'VERB'}\n",
    "    text = text.lower()\n",
    "    result = np.array([])\n",
    "    \n",
    "    \n",
    "    # Убираем лишние символы\n",
    "    #text = re.sub(r'[;,]',r' ',text).strip()\n",
    "    text = re.sub(r'[^\\w\\s\\.]',r'',text).strip()   \n",
    "    #text = [token.text for token in razdel.tokenize(text)]\n",
    "    # Делаем лемматизацию       \n",
    "#     text = [lemma for lemma in mystem.lemmatize(text) if not lemma.isspace() and lemma not in sw_ru\n",
    "#             and lemma.strip() not in ['.','..','...']]\n",
    "    \n",
    "#     if id_text_vocab != {}:\n",
    "#         prev_word = id_text_vocab[max(id_text_vocab.keys())]\n",
    "#     else:       \n",
    "#         prev_word = None\n",
    "        \n",
    "    for item in mystem.analyze(text):\n",
    "      #  print(item)\n",
    "        token = None\n",
    "        if item.get('analysis'):\n",
    "            lemma = item['analysis'][0]['lex']\n",
    "            pos = re.split('[=,]', item['analysis'][0]['gr'])[0]\n",
    "            #and lemma not in sw_ru\n",
    "            if not lemma.isspace()  and lemma.strip() not in ['.','..','...'] and lemma not in sw_ru:\n",
    "     \n",
    "                token = f'{lemma}_{upos_map[pos]}'\n",
    "        else:\n",
    "            lem_text = item[\"text\"]\n",
    "            if not lem_text.isspace() and lem_text.strip() not in ['.','..','...'] and lem_text not in sw_ru:\n",
    "            \n",
    "                token = f'{lem_text}_UNKN'\n",
    "            \n",
    "        if token:    \n",
    "            #result.append(token)\n",
    "            result = np.append(result,token)\n",
    "            \n",
    "#             if prev_word:\n",
    "#                 text_id_vocab[token] = text_id_vocab[prev_word] + 1\n",
    "#                 id_text_vocab[text_id_vocab[prev_word] + 1] = token\n",
    "#             else:\n",
    "#                 text_vocab[token] = 4\n",
    "#                 id_text_vocab[4] = token\n",
    "#             prev_word = token\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Лемматизатор неправильно разбивает слова с дефисом, поэтому исправляем это\n",
    "#     if '-' in text:\n",
    "#         for l in range(len(text)):\n",
    "#             if text[l] == '-':\n",
    "#                 text[l] = f'{text[l-1]}-{text[l+1]}'\n",
    "#                 text[l-1] = text[l+1] = text[l]\n",
    "    zer = np.empty([40], dtype=\"U25\")\n",
    "    zer[:] = '<PAD>'\n",
    "    size = result.shape[0]\n",
    "    if size>40:\n",
    "        zer = result[:40]\n",
    "    else:\n",
    "        zer[:size] = result\n",
    "        \n",
    "    return zer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.char2idx = {'<PAD>':0,'<START>':1,'<END>':2,'<UNK>':3}\n",
    "        self.idx2char = {i:ch for ch,i in self.char2idx.items()}\n",
    "    \n",
    "    def save(self, path='.'):\n",
    "        json_ch_idx = json.dumps(self.char2idx)\n",
    "        with open(os.path.join(path,\"char2idx.json\"),\"w\") as f:\n",
    "            f.write(json_ch_idx)\n",
    "        json_idx_ch = json.dumps(self.idx2char)\n",
    "        with open(os.path.join(path,\"idx2char.json\"),\"w\") as f:\n",
    "            f.write(json_idx_ch)\n",
    "    \n",
    "    def load(self, path='.'):       \n",
    "        with open(os.path.join(path,\"char2idx.json\"),\"r\") as f:\n",
    "            self.char2idx = json.loads(f.read())        \n",
    "        with open(os.path.join(path,\"idx2char.json\"),\"r\") as f:\n",
    "            self.idx2char = json.loads(f.read())\n",
    "        \n",
    "    \n",
    "    def indx_tokenize_from_files(self, huge_parts):\n",
    "        if not os.path.exists('indexed_token_files'):\n",
    "            os.mkdir('indexed_token_files')\n",
    "        for i in tqdm_notebook(range(huge_parts)):\n",
    "            with np.load(f'tokenized_files/np_tokens{i}.npz') as data:\n",
    "                np_idxs = np.apply_along_axis(np.vectorize(self.char2idx.get),1,data['arr'])\n",
    "                print(np_idxs)\n",
    "                np.savez_compressed(f'indexed_token_files/indx_tokens{i}',arr=np_idxs)\n",
    "                \n",
    "        \n",
    "    \n",
    "    def indx_detokenize(self, sequence):\n",
    "        return ''.join([self.idx2char[idx] for idx in sequence])\n",
    "    \n",
    "    def tokenize(self, data, tokenizer, huge_parts, save_to_file=True):\n",
    "        huge_splits = np.array_split(data, huge_parts)\n",
    "        partitions = cpu_count()\n",
    "        for split_number,huge_split in enumerate(tqdm_notebook(huge_splits)):\n",
    "            print(huge_split.shape)          \n",
    "            data_split = np.array_split(huge_split, partitions)\n",
    "            pool = Pool(partitions)\n",
    "\n",
    "            res_data = np.concatenate(pool.map(tokenizer,data_split))\n",
    "            pool.close()\n",
    "            pool.join()\n",
    "            \n",
    "            print(res_data.shape)\n",
    "            self._extend_vocab(res_data)\n",
    "            \n",
    "            print(res_data)\n",
    "            \n",
    "            if save_to_file:\n",
    "                np.savez_compressed(f'tokenized_files/np_tokens{split_number}',arr=res_data)             \n",
    "                del res_data\n",
    "                gc.collect()\n",
    "                \n",
    "                \n",
    "    def _extend_vocab(self,token_matrix):\n",
    "        start_idx = len(self.char2idx)\n",
    "        for idx,token in np.ndenumerate(np.unique(token_matrix)):\n",
    "            if token not in self.char2idx:\n",
    "                self.char2idx[token] = start_idx + idx[0]\n",
    "        \n",
    "        self.idx2char = {i:ch for ch,i in self.char2idx.items()}\n",
    "                \n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.char2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24cc7bb2a11a4440ab2f0497d0963ed9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000000,)\n",
      "(1000000, 40)\n",
      "[['убирать_VERB' '600_UNKN' 'литр_NOUN' ... '' '' '']\n",
      " ['убирать_VERB' '600_UNKN' 'литр_NOUN' ... '' '' '']\n",
      " ['убирать_VERB' '600_UNKN' 'литр_NOUN' ... '' '' '']\n",
      " ...\n",
      " ['гейб_NOUN' 'молодец_NOUN' 'давно_ADV' ... '' '' '']\n",
      " ['гейб_NOUN' 'молодец_NOUN' 'давно_ADV' ... '' '' '']\n",
      " ['гейб_NOUN' 'молодец_NOUN' 'давно_ADV' ... '' '' '']]\n",
      "(1000000,)\n",
      "(1000000, 40)\n",
      "[['молодец_NOUN' 'самый_DET' 'начало_NOUN' ... '' '' '']\n",
      " ['это_PRON' 'человек_NOUN' 'виноватый_ADJ' ... '' '' '']\n",
      " ['беда_NOUN' 'гринлайт_NOUN' 'плюсонуть_VERB' ... '' '' '']\n",
      " ...\n",
      " ['сложно_ADV' 'олдфаг_NOUN' 'зарегистрировать_VERB' ... '' '' '']\n",
      " ['зарегистрировать_VERB' 'месяц_NOUN' 'олдфаг_NOUN' ... '' '' '']\n",
      " ['это_PRON' 'читать_VERB' 'сидеть_VERB' ... '' '' '']]\n",
      "(1000000,)\n",
      "(1000000, 40)\n",
      "[['сидеть_VERB' 'тег_NOUN' 'жесть_NOUN' ... '' '' '']\n",
      " ['сложно_ADV' 'олдфаг_NOUN' 'зарегистрировать_VERB' ... '' '' '']\n",
      " ['сложно_ADV' 'олдфаг_NOUN' 'зарегистрировать_VERB' ... '' '' '']\n",
      " ...\n",
      " ['пн_UNKN' '2_UNKN' 'геома_NOUN' ... '' '' '']\n",
      " ['бесить_VERB' 'скоро_ADV' 'сваливать_VERB' ... '' '' '']\n",
      " ['просто_PART' 'клнфликт_NOUN' 'оч_UNKN' ... '' '' '']]\n",
      "(1000000,)\n",
      "(1000000, 40)\n",
      "[['новый_ADJ' 'уровень_NOUN' 'родственность_NOUN' ... '' '' '']\n",
      " ['новый_ADJ' 'уровень_NOUN' 'родственность_NOUN' ... '' '' '']\n",
      " ['новый_ADJ' 'уровень_NOUN' 'родственность_NOUN' ... '' '' '']\n",
      " ...\n",
      " ['москва_NOUN' 'сей_DET' 'пора_NOUN' ... '' '' '']\n",
      " ['утро_NOUN' 'завтра_ADV' 'час_NOUN' ... '' '' '']\n",
      " ['сегодня_ADV' 'вечер_NOUN' 'охуенный_ADJ' ... '' '' '']]\n",
      "\n",
      "CPU times: user 2min 9s, sys: 23.5 s, total: 2min 33s\n",
      "Wall time: 12min 27s\n"
     ]
    }
   ],
   "source": [
    "def fu(x):\n",
    "    return np.apply_along_axis(text_prep_tags,1,np.reshape(x,(-1,1)))\n",
    "voc = Vocab()\n",
    "%time voc.tokenize(context[:4000000],fu,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc1=Vocab()\n",
    "voc1.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['молодец_NOUN' 'самый_DET' 'начало_NOUN']\n",
      "['это_PRON' 'человек_NOUN' 'виноватый_ADJ']\n",
      "['беда_NOUN' 'гринлайт_NOUN' 'плюсонуть_VERB']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 99766, 149023, 105369],\n",
       "       [188210, 181013,  45919],\n",
       "       [ 36253,  56266, 126309]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def yy(x):\n",
    "    print(x)\n",
    "    return np.vectorize(voc1.char2idx.get)(x)\n",
    "np.apply_along_axis(yy,1,np.array([['молодец_NOUN', 'самый_DET' ,'начало_NOUN'],\n",
    " ['это_PRON' ,'человек_NOUN', 'виноватый_ADJ'],\n",
    " ['беда_NOUN', 'гринлайт_NOUN' ,'плюсонуть_VERB']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<PAD>': 0,\n",
       " '<START>': 1,\n",
       " '<END>': 2,\n",
       " '<UNK>': 3,\n",
       " '': 4,\n",
       " '    _ _UNKN': 5,\n",
       " '    __   _UNKN': 6,\n",
       " '   _ _UNKN': 7,\n",
       " '   __\\n_UNKN': 8,\n",
       " '   __ __UNKN': 9,\n",
       " '  _\\n_UNKN': 10,\n",
       " '  _  _UNKN': 11,\n",
       " '  _ _\\n_UNKN': 12,\n",
       " '  _ _UNKN': 13,\n",
       " '  __\\n_UNKN': 14,\n",
       " '  __  _UNKN': 15,\n",
       " '  __  __UNKN': 16,\n",
       " '  __ _UNKN': 17,\n",
       " '  __UNKN': 18,\n",
       " '  ___\\n_UNKN': 19,\n",
       " '  ___UNKN': 20,\n",
       " '  ヽ\\n_UNKN': 21,\n",
       " '  ヽ _UNKN': 22,\n",
       " '  ヽ ー__UNKN': 23,\n",
       " '  ヽ_UNKN': 24,\n",
       " ' _\\n_UNKN': 25,\n",
       " ' _   _UNKN': 26,\n",
       " ' _  _UNKN': 27,\n",
       " ' _ _ _\\n_UNKN': 28,\n",
       " ' _ _ _ _ _ _\\n_UNKN': 29,\n",
       " ' _ _ _ _ _ _ _ _ _ _ _ _ ': 30,\n",
       " ' _ _ _ _ _ _ _UNKN': 31,\n",
       " ' _ _ _ _UNKN': 32,\n",
       " ' _ _ _UNKN': 33,\n",
       " ' _ _UNKN': 34,\n",
       " ' _ __UNKN': 35,\n",
       " ' _ ヽ _UNKN': 36,\n",
       " ' __\\n_UNKN': 37,\n",
       " ' __  _UNKN': 38,\n",
       " ' __ _ _\\n_UNKN': 39,\n",
       " ' __ _UNKN': 40,\n",
       " ' __ __UNKN': 41,\n",
       " ' __UNKN': 42,\n",
       " ' ___\\n_UNKN': 43,\n",
       " ' ___ _UNKN': 44,\n",
       " ' ___UNKN': 45,\n",
       " ' ____\\n_UNKN': 46,\n",
       " ' ____ _UNKN': 47,\n",
       " ' ____UNKN': 48,\n",
       " ' _____\\n_UNKN': 49,\n",
       " ' _____ _ _UNKN': 50,\n",
       " ' _____ _UNKN': 51,\n",
       " ' ______\\n_UNKN': 52,\n",
       " ' ______ _UNKN': 53,\n",
       " ' ______UNKN': 54,\n",
       " ' _______\\n_UNKN': 55,\n",
       " ' _______ _UNKN': 56,\n",
       " ' ________ _UNKN': 57,\n",
       " ' _________\\n_UNKN': 58,\n",
       " ' _________ _UNKN': 59,\n",
       " ' __________\\n_UNKN': 60,\n",
       " ' __________        _UNKN': 61,\n",
       " ' __________    _UNKN': 62,\n",
       " ' ___________ _UNKN': 63,\n",
       " ' ___________UNKN': 64,\n",
       " ' ____________    ____ _UN': 65,\n",
       " ' ____________UNKN': 66,\n",
       " ' _____________ _UNKN': 67,\n",
       " ' ________________ _UNKN': 68,\n",
       " ' __________________ _UNKN': 69,\n",
       " ' ____________________ ___': 70,\n",
       " ' _____________________ _U': 71,\n",
       " ' ______________________ _': 72,\n",
       " ' ________________________': 73,\n",
       " ' ゞ _UNKN': 74,\n",
       " ' ー _UNKN': 75,\n",
       " ' ヽ     _UNKN': 76,\n",
       " ' ヽ _UNKN': 77,\n",
       " ' ヽ_UNKN': 78,\n",
       " ' ヽー\\n_UNKN': 79,\n",
       " '.   .   _UNKN': 80,\n",
       " '.  .  _UNKN': 81,\n",
       " '. . . . . . . . . . . _UN': 82,\n",
       " '. . . _UNKN': 83,\n",
       " '. . _UNKN': 84,\n",
       " '. ... _UNKN': 85,\n",
       " '. ._UNKN': 86,\n",
       " '. __UNKN': 87,\n",
       " '.. ... _UNKN': 88,\n",
       " '.. ..._UNKN': 89,\n",
       " '.. ._UNKN': 90,\n",
       " '... ....._UNKN': 91,\n",
       " '... ..._UNKN': 92,\n",
       " '... .._UNKN': 93,\n",
       " '.... ..._UNKN': 94,\n",
       " '.... _UNKN': 95,\n",
       " '.....  _UNKN': 96,\n",
       " '..... _UNKN': 97,\n",
       " '...... _UNKN': 98,\n",
       " '........ _UNKN': 99,\n",
       " '...............   _UNKN': 100,\n",
       " '.........................': 101,\n",
       " '........................_': 102,\n",
       " '......................_UN': 103,\n",
       " '....................._UNK': 104,\n",
       " '...................._UNKN': 105,\n",
       " '..................._UNKN': 106,\n",
       " '.................._UNKN': 107,\n",
       " '................._UNKN': 108,\n",
       " '................_UNKN': 109,\n",
       " '..............._UNKN': 110,\n",
       " '.............._UNKN': 111,\n",
       " '............._UNKN': 112,\n",
       " '............_UNKN': 113,\n",
       " '..........._UNKN': 114,\n",
       " '.........._UNKN': 115,\n",
       " '........._UNKN': 116,\n",
       " '........_UNKN': 117,\n",
       " '......._UNKN': 118,\n",
       " '......_UNKN': 119,\n",
       " '....._UNKN': 120,\n",
       " '...._UNKN': 121,\n",
       " '0.000000000000001_UNKN': 122,\n",
       " '0.000000000001_UNKN': 123,\n",
       " '0.0000001_UNKN': 124,\n",
       " '0.000000_UNKN': 125,\n",
       " '0.000010_UNKN': 126,\n",
       " '0.00003_UNKN': 127,\n",
       " '0.0001_UNKN': 128,\n",
       " '0.000250_UNKN': 129,\n",
       " '0.0025_UNKN': 130,\n",
       " '0.0034_UNKN': 131,\n",
       " '0.003_UNKN': 132,\n",
       " '0.004_UNKN': 133,\n",
       " '0.0076_UNKN': 134,\n",
       " '0.0081_UNKN': 135,\n",
       " '0.0088_UNKN': 136,\n",
       " '0.009_UNKN': 137,\n",
       " '0.00_UNKN': 138,\n",
       " '0.0127_UNKN': 139,\n",
       " '0.015625_UNKN': 140,\n",
       " '0.01_UNKN': 141,\n",
       " '0.023_UNKN': 142,\n",
       " '0.03125_UNKN': 143,\n",
       " '0.03_UNKN': 144,\n",
       " '0.04_UNKN': 145,\n",
       " '0.051_UNKN': 146,\n",
       " '0.05_UNKN': 147,\n",
       " '0.0625_UNKN': 148,\n",
       " '0.08_UNKN': 149,\n",
       " '0.09_UNKN': 150,\n",
       " '0.0_UNKN': 151,\n",
       " '0.100_UNKN': 152,\n",
       " '0.10_UNKN': 153,\n",
       " '0.119_UNKN': 154,\n",
       " '0.125_UNKN': 155,\n",
       " '0.1428571429_UNKN': 156,\n",
       " '0.18_UNKN': 157,\n",
       " '0.1_UNKN': 158,\n",
       " '0.223_UNKN': 159,\n",
       " '0.24_UNKN': 160,\n",
       " '0.255_UNKN': 161,\n",
       " '0.25_UNKN': 162,\n",
       " '0.26_UNKN': 163,\n",
       " '0.270_UNKN': 164,\n",
       " '0.2_UNKN': 165,\n",
       " '0.30_UNKN': 166,\n",
       " '0.31_UNKN': 167,\n",
       " '0.32_UNKN': 168,\n",
       " '0.33_UNKN': 169,\n",
       " '0.35_UNKN': 170,\n",
       " '0.375_UNKN': 171,\n",
       " '0.3_UNKN': 172,\n",
       " '0.40_UNKN': 173,\n",
       " '0.41_UNKN': 174,\n",
       " '0.44490_UNKN': 175,\n",
       " '0.449_UNKN': 176,\n",
       " '0.44_UNKN': 177,\n",
       " '0.45_UNKN': 178,\n",
       " '0.47_UNKN': 179,\n",
       " '0.485_UNKN': 180,\n",
       " '0.49_UNKN': 181,\n",
       " '0.4_UNKN': 182,\n",
       " '0.50_UNKN': 183,\n",
       " '0.52_UNKN': 184,\n",
       " '0.53_UNKN': 185,\n",
       " '0.5553535_UNKN': 186,\n",
       " '0.55_UNKN': 187,\n",
       " '0.568_UNKN': 188,\n",
       " '0.57_UNKN': 189,\n",
       " '0.586_UNKN': 190,\n",
       " '0.5_UNKN': 191,\n",
       " '0.6666_UNKN': 192,\n",
       " '0.68_UNKN': 193,\n",
       " '0.6_UNKN': 194,\n",
       " '0.71_UNKN': 195,\n",
       " '0.75_UNKN': 196,\n",
       " '0.76_UNKN': 197,\n",
       " '0.783183_UNKN': 198,\n",
       " '0.7931505_UNKN': 199,\n",
       " '0.7_UNKN': 200,\n",
       " '0.850_UNKN': 201,\n",
       " '0.87_UNKN': 202,\n",
       " '0.89_UNKN': 203,\n",
       " '0.8_UNKN': 204,\n",
       " '0.9012_UNKN': 205,\n",
       " '0.91_UNKN': 206,\n",
       " '0.942505_UNKN': 207,\n",
       " '0.94_UNKN': 208,\n",
       " '0.95_UNKN': 209,\n",
       " '0.98_UNKN': 210,\n",
       " '0.99100_UNKN': 211,\n",
       " '0.9999999999_UNKN': 212,\n",
       " '0.99_UNKN': 213,\n",
       " '0.9_UNKN': 214,\n",
       " '00.000000000000_UNKN': 215,\n",
       " '00.00_UNKN': 216,\n",
       " '00.01_UNKN': 217,\n",
       " '00.49_UNKN': 218,\n",
       " '00.95_UNKN': 219,\n",
       " '000.000_UNKN': 220,\n",
       " '00000000000000000000001_U': 221,\n",
       " '0000000000000001_UNKN': 222,\n",
       " '0000000000002_UNKN': 223,\n",
       " '0000000000010_UNKN': 224,\n",
       " '00000000018_UNKN': 225,\n",
       " '000000000_UNKN': 226,\n",
       " '00000000_UNKN': 227,\n",
       " '00000001_UNKN': 228,\n",
       " '0000001_UNKN': 229,\n",
       " '000000_UNKN': 230,\n",
       " '000001_UNKN': 231,\n",
       " '00000_UNKN': 232,\n",
       " '00001.00000002_UNKN': 233,\n",
       " '00002_UNKN': 234,\n",
       " '000030_UNKN': 235,\n",
       " '00004_UNKN': 236,\n",
       " '0000_UNKN': 237,\n",
       " '000100_UNKN': 238,\n",
       " '00010_UNKN': 239,\n",
       " '000110101100011101_UNKN': 240,\n",
       " '00017_UNKN': 241,\n",
       " '0001_UNKN': 242,\n",
       " '00026185185185185_UNKN': 243,\n",
       " '00035_UNKN': 244,\n",
       " '0003_UNKN': 245,\n",
       " '0005_UNKN': 246,\n",
       " '0006_UNKN': 247,\n",
       " '000_UNKN': 248,\n",
       " '000k_UNKN': 249,\n",
       " '000баксов_UNKN': 250,\n",
       " '000км_UNKN': 251,\n",
       " '000р_UNKN': 252,\n",
       " '000руб_UNKN': 253,\n",
       " '000сум_UNKN': 254,\n",
       " '001002_UNKN': 255,\n",
       " '0010_UNKN': 256,\n",
       " '0011001000110000001100010': 257,\n",
       " '0011159_UNKN': 258,\n",
       " '0015_UNKN': 259,\n",
       " '001626002048_UNKN': 260,\n",
       " '001_UNKN': 261,\n",
       " '001мм_UNKN': 262,\n",
       " '0024_UNKN': 263,\n",
       " '0026_UNKN': 264,\n",
       " '00288_UNKN': 265,\n",
       " '002_UNKN': 266,\n",
       " '002frev_UNKN': 267,\n",
       " '0030_UNKN': 268,\n",
       " '003_UNKN': 269,\n",
       " '00495_UNKN': 270,\n",
       " '004_UNKN': 271,\n",
       " '005553535_UNKN': 272,\n",
       " '00555_UNKN': 273,\n",
       " '005_UNKN': 274,\n",
       " '0060с7o_UNKN': 275,\n",
       " '0066_UNKN': 276,\n",
       " '006_UNKN': 277,\n",
       " '0078_UNKN': 278,\n",
       " '007_UNKN': 279,\n",
       " '0081_UNKN': 280,\n",
       " '0099к_UNKN': 281,\n",
       " '009_UNKN': 282,\n",
       " '00_UNKN': 283,\n",
       " '00е_UNKN': 284,\n",
       " '00ольноль_UNKN': 285,\n",
       " '00оо_UNKN': 286,\n",
       " '00х_UNKN': 287,\n",
       " '00ых_UNKN': 288,\n",
       " '01.00_UNKN': 289,\n",
       " '01.01_UNKN': 290,\n",
       " '01.02_UNKN': 291,\n",
       " '01.03_UNKN': 292,\n",
       " '01.04_UNKN': 293,\n",
       " '01000_UNKN': 294,\n",
       " '01001103_UNKN': 295,\n",
       " '0100_UNKN': 296,\n",
       " '010103_UNKN': 297,\n",
       " '010111001010110001001_UNK': 298,\n",
       " '0103_UNKN': 299,\n",
       " '010455_UNKN': 300,\n",
       " '010600_UNKN': 301,\n",
       " '010_UNKN': 302,\n",
       " '011000111_UNKN': 303,\n",
       " '0110_UNKN': 304,\n",
       " '01200_UNKN': 305,\n",
       " '0123456789_UNKN': 306,\n",
       " '0125_UNKN': 307,\n",
       " '0125мм_UNKN': 308,\n",
       " '012_UNKN': 309,\n",
       " '0138_UNKN': 310,\n",
       " '013_UNKN': 311,\n",
       " '0147_UNKN': 312,\n",
       " '0148_UNKN': 313,\n",
       " '014_UNKN': 314,\n",
       " '0150_UNKN': 315,\n",
       " '0151_UNKN': 316,\n",
       " '015325_UNKN': 317,\n",
       " '01571111111111111_UNKN': 318,\n",
       " '0160_UNKN': 319,\n",
       " '0169_UNKN': 320,\n",
       " '016_UNKN': 321,\n",
       " '016м3_UNKN': 322,\n",
       " '017_UNKN': 323,\n",
       " '017ый_UNKN': 324,\n",
       " '018_UNKN': 325,\n",
       " '019_UNKN': 326,\n",
       " '01_UNKN': 327,\n",
       " '01пожар69гейклубпожар_UNK': 328,\n",
       " '0200_UNKN': 329,\n",
       " '0206mail_UNKN': 330,\n",
       " '0216_UNKN': 331,\n",
       " '0220_UNKN': 332,\n",
       " '0220в_UNKN': 333,\n",
       " '0234_UNKN': 334,\n",
       " '023_UNKN': 335,\n",
       " '024_UNKN': 336,\n",
       " '0250_UNKN': 337,\n",
       " '0254_UNKN': 338,\n",
       " '0257_UNKN': 339,\n",
       " '025_UNKN': 340,\n",
       " '02_UNKN': 341,\n",
       " '02b_UNKN': 342,\n",
       " '02c_UNKN': 343,\n",
       " '02i_UNKN': 344,\n",
       " '02pi_UNKN': 345,\n",
       " '02а_UNKN': 346,\n",
       " '03.01_UNKN': 347,\n",
       " '03.04_UNKN': 348,\n",
       " '03.09_UNKN': 349,\n",
       " '030_UNKN': 350,\n",
       " '0313_UNKN': 351,\n",
       " '031_UNKN': 352,\n",
       " '0321_UNKN': 353,\n",
       " '032р_UNKN': 354,\n",
       " '0330_UNKN': 355,\n",
       " '03334764645354384354л_UNK': 356,\n",
       " '033_UNKN': 357,\n",
       " '035_UNKN': 358,\n",
       " '0367_UNKN': 359,\n",
       " '036_UNKN': 360,\n",
       " '037_UNKN': 361,\n",
       " '0396723536_UNKN': 362,\n",
       " '03_UNKN': 363,\n",
       " '03х01_UNKN': 364,\n",
       " '04.12_UNKN': 365,\n",
       " '042_UNKN': 366,\n",
       " '043b_UNKN': 367,\n",
       " '04449_UNKN': 368,\n",
       " '044_UNKN': 369,\n",
       " '0450_UNKN': 370,\n",
       " '0451_UNKN': 371,\n",
       " '045_UNKN': 372,\n",
       " '046_UNKN': 373,\n",
       " '0473_UNKN': 374,\n",
       " '047_UNKN': 375,\n",
       " '048052_UNKN': 376,\n",
       " '048_UNKN': 377,\n",
       " '04_UNKN': 378,\n",
       " '04y_UNKN': 379,\n",
       " '05.05_UNKN': 380,\n",
       " '05.07_UNKN': 381,\n",
       " '05.10_UNKN': 382,\n",
       " '05.12_UNKN': 383,\n",
       " '0505050125_UNKN': 384,\n",
       " '0506846672_UNKN': 385,\n",
       " '0508_UNKN': 386,\n",
       " '050_UNKN': 387,\n",
       " '0515_UNKN': 388,\n",
       " '0515л_UNKN': 389,\n",
       " '051_UNKN': 390,\n",
       " '052016_UNKN': 391,\n",
       " '052020_UNKN': 392,\n",
       " '053_UNKN': 393,\n",
       " '05568687_UNKN': 394,\n",
       " '055_UNKN': 395,\n",
       " '0567_UNKN': 396,\n",
       " '056826125_UNKN': 397,\n",
       " '05683_UNKN': 398,\n",
       " '0568_UNKN': 399,\n",
       " '056_UNKN': 400,\n",
       " '057_UNKN': 401,\n",
       " '059_UNKN': 402,\n",
       " '05_UNKN': 403,\n",
       " '05rus_UNKN': 404,\n",
       " '05xmr_UNKN': 405,\n",
       " '05к_UNKN': 406,\n",
       " '05л_UNKN': 407,\n",
       " '05мм_UNKN': 408,\n",
       " '05см_UNKN': 409,\n",
       " '05тые_UNKN': 410,\n",
       " '06.02_UNKN': 411,\n",
       " '06.03_UNKN': 412,\n",
       " '06.05_UNKN': 413,\n",
       " '06.2015_UNKN': 414,\n",
       " '0611_UNKN': 415,\n",
       " '0634_UNKN': 416,\n",
       " '0666_UNKN': 417,\n",
       " '066_UNKN': 418,\n",
       " '0672_UNKN': 419,\n",
       " '068_UNKN': 420,\n",
       " '06_UNKN': 421,\n",
       " '06tolik86_UNKN': 422,\n",
       " '07.00_UNKN': 423,\n",
       " '07.01_UNKN': 424,\n",
       " '07.03_UNKN': 425,\n",
       " '07.05_UNKN': 426,\n",
       " '07.11_UNKN': 427,\n",
       " '07082018_UNKN': 428,\n",
       " '071_UNKN': 429,\n",
       " '0726_UNKN': 430,\n",
       " '0745_UNKN': 431,\n",
       " '075_UNKN': 432,\n",
       " '075l_UNKN': 433,\n",
       " '07734_UNKN': 434,\n",
       " '07831505_UNKN': 435,\n",
       " '0783_UNKN': 436,\n",
       " '0785d2_UNKN': 437,\n",
       " '07931505_UNKN': 438,\n",
       " '0793_UNKN': 439,\n",
       " '07_UNKN': 440,\n",
       " '07mail_UNKN': 441,\n",
       " '07го_UNKN': 442,\n",
       " '07л_UNKN': 443,\n",
       " '07ю_UNKN': 444,\n",
       " '08.00_UNKN': 445,\n",
       " '08.02_UNKN': 446,\n",
       " '08.03_UNKN': 447,\n",
       " '08.07_UNKN': 448,\n",
       " '08.08_UNKN': 449,\n",
       " '08.09_UNKN': 450,\n",
       " '08.30_UNKN': 451,\n",
       " '0800_UNKN': 452,\n",
       " '0808_UNKN': 453,\n",
       " '0811_UNKN': 454,\n",
       " '082_UNKN': 455,\n",
       " '085_UNKN': 456,\n",
       " '089_UNKN': 457,\n",
       " '08_UNKN': 458,\n",
       " '08mail_UNKN': 459,\n",
       " '08xi16г_UNKN': 460,\n",
       " '08мбс_UNKN': 461,\n",
       " '09.0018_UNKN': 462,\n",
       " '09.05_UNKN': 463,\n",
       " '0900_UNKN': 464,\n",
       " '09051945_UNKN': 465,\n",
       " '090_UNKN': 466,\n",
       " '0910_UNKN': 467,\n",
       " '091_UNKN': 468,\n",
       " '0927_UNKN': 469,\n",
       " '09281918_UNKN': 470,\n",
       " '092_UNKN': 471,\n",
       " '093_UNKN': 472,\n",
       " '095_UNKN': 473,\n",
       " '097_UNKN': 474,\n",
       " '099_UNKN': 475,\n",
       " '099s_UNKN': 476,\n",
       " '09_UNKN': 477,\n",
       " '09xi16г_UNKN': 478,\n",
       " '09го_UNKN': 479,\n",
       " '09кто_UNKN': 480,\n",
       " '09л_UNKN': 481,\n",
       " '0_UNKN': 482,\n",
       " '0a1_UNKN': 483,\n",
       " '0b11111100001_UNKN': 484,\n",
       " '0bh0_UNKN': 485,\n",
       " '0bho_UNKN': 486,\n",
       " '0ddzum0rj1bjmail_UNKN': 487,\n",
       " '0dimetrius0_UNKN': 488,\n",
       " '0drt0r_UNKN': 489,\n",
       " '0exm68_UNKN': 490,\n",
       " '0h_UNKN': 491,\n",
       " '0jxrgdc70lgg0lrqvtc80ymg0': 492,\n",
       " '0khqv9cw0yhquncx0l4sinc00': 493,\n",
       " '0qp46l_UNKN': 494,\n",
       " '0r_UNKN': 495,\n",
       " '0tpeжyt_UNKN': 496,\n",
       " '0varda0_UNKN': 497,\n",
       " '0w30_UNKN': 498,\n",
       " '0x00000012_UNKN': 499,\n",
       " '0x00_UNKN': 500,\n",
       " '0x1_UNKN': 501,\n",
       " '0x20_UNKN': 502,\n",
       " '0x2923143_UNKN': 503,\n",
       " '0x50_UNKN': 504,\n",
       " '0x5f3759df_UNKN': 505,\n",
       " '0x7e1_UNKN': 506,\n",
       " '0xff_UNKN': 507,\n",
       " '0а_UNKN': 508,\n",
       " '0азнь_UNKN': 509,\n",
       " '0й_UNKN': 510,\n",
       " '0лень_UNKN': 511,\n",
       " '0м_UNKN': 512,\n",
       " '0не_UNKN': 513,\n",
       " '0о_UNKN': 514,\n",
       " '0ошники_UNKN': 515,\n",
       " '0р_UNKN': 516,\n",
       " '0руб_UNKN': 517,\n",
       " '0ттбьч_UNKN': 518,\n",
       " '0х00_UNKN': 519,\n",
       " '0х2_UNKN': 520,\n",
       " '0х500_UNKN': 521,\n",
       " '0х50_UNKN': 522,\n",
       " '0ых_UNKN': 523,\n",
       " '1.000_UNKN': 524,\n",
       " '1.002_UNKN': 525,\n",
       " '1.01_UNKN': 526,\n",
       " '1.02_UNKN': 527,\n",
       " '1.07_UNKN': 528,\n",
       " '1.0_UNKN': 529,\n",
       " '1.11_UNKN': 530,\n",
       " '1.19175_UNKN': 531,\n",
       " '1.198_UNKN': 532,\n",
       " '1.1_UNKN': 533,\n",
       " '1.20_UNKN': 534,\n",
       " '1.216451_UNKN': 535,\n",
       " '1.22_UNKN': 536,\n",
       " '1.24_UNKN': 537,\n",
       " '1.25_UNKN': 538,\n",
       " '1.27_UNKN': 539,\n",
       " '1.2_UNKN': 540,\n",
       " '1.30_UNKN': 541,\n",
       " '1.35_UNKN': 542,\n",
       " '1.3763753_UNKN': 543,\n",
       " '1.38_UNKN': 544,\n",
       " '1.3_UNKN': 545,\n",
       " '1.42_UNKN': 546,\n",
       " '1.45_UNKN': 547,\n",
       " '1.498_UNKN': 548,\n",
       " '1.4_UNKN': 549,\n",
       " '1.50_UNKN': 550,\n",
       " '1.511_UNKN': 551,\n",
       " '1.51_UNKN': 552,\n",
       " '1.52_UNKN': 553,\n",
       " '1.53_UNKN': 554,\n",
       " '1.57_UNKN': 555,\n",
       " '1.5_UNKN': 556,\n",
       " '1.600_UNKN': 557,\n",
       " '1.602_UNKN': 558,\n",
       " '1.60_UNKN': 559,\n",
       " '1.65_UNKN': 560,\n",
       " '1.68_UNKN': 561,\n",
       " '1.69_UNKN': 562,\n",
       " '1.6_UNKN': 563,\n",
       " '1.70_UNKN': 564,\n",
       " '1.71_UNKN': 565,\n",
       " '1.72_UNKN': 566,\n",
       " '1.75_UNKN': 567,\n",
       " '1.783_UNKN': 568,\n",
       " '1.78_UNKN': 569,\n",
       " '1.7_UNKN': 570,\n",
       " '1.80_UNKN': 571,\n",
       " '1.82_UNKN': 572,\n",
       " '1.85_UNKN': 573,\n",
       " '1.8745_UNKN': 574,\n",
       " '1.87_UNKN': 575,\n",
       " '1.892_UNKN': 576,\n",
       " '1.8_UNKN': 577,\n",
       " '1.90_UNKN': 578,\n",
       " '1.93_UNKN': 579,\n",
       " '1.95_UNKN': 580,\n",
       " '1.96_UNKN': 581,\n",
       " '1.99_UNKN': 582,\n",
       " '1.9_UNKN': 583,\n",
       " '10.000_UNKN': 584,\n",
       " '10.00_UNKN': 585,\n",
       " '10.01_UNKN': 586,\n",
       " '10.0_UNKN': 587,\n",
       " '10.10_UNKN': 588,\n",
       " '10.12_UNKN': 589,\n",
       " '10.16_UNKN': 590,\n",
       " '10.1_UNKN': 591,\n",
       " '10.259_UNKN': 592,\n",
       " '10.25_UNKN': 593,\n",
       " '10.366_UNKN': 594,\n",
       " '10.42_UNKN': 595,\n",
       " '10.45_UNKN': 596,\n",
       " '10.4_UNKN': 597,\n",
       " '10.5_UNKN': 598,\n",
       " '10.6_UNKN': 599,\n",
       " '10.99100_UNKN': 600,\n",
       " '10.9_UNKN': 601,\n",
       " '100.000_UNKN': 602,\n",
       " '100.75_UNKN': 603,\n",
       " '10000000000_UNKN': 604,\n",
       " '1000000000_UNKN': 605,\n",
       " '1000000001_UNKN': 606,\n",
       " '100000000_UNKN': 607,\n",
       " '10000000_UNKN': 608,\n",
       " '10000001_UNKN': 609,\n",
       " '1000000_UNKN': 610,\n",
       " '1000000к_UNKN': 611,\n",
       " '10000010_UNKN': 612,\n",
       " '1000005_UNKN': 613,\n",
       " '100000640093600_UNKN': 614,\n",
       " '100000_UNKN': 615,\n",
       " '100000й_UNKN': 616,\n",
       " '100007038_UNKN': 617,\n",
       " '10000_UNKN': 618,\n",
       " '10000к_UNKN': 619,\n",
       " '10000л_UNKN': 620,\n",
       " '10000р_UNKN': 621,\n",
       " '10000т_UNKN': 622,\n",
       " '100010001000_UNKN': 623,\n",
       " '10001000_UNKN': 624,\n",
       " '1000111110100100100000110': 625,\n",
       " '10001200_UNKN': 626,\n",
       " '10001200р_UNKN': 627,\n",
       " '10001500_UNKN': 628,\n",
       " '10001500м_UNKN': 629,\n",
       " '1000192_UNKN': 630,\n",
       " '100019_UNKN': 631,\n",
       " '10001_UNKN': 632,\n",
       " '10002000_UNKN': 633,\n",
       " '10003000р_UNKN': 634,\n",
       " '10007013185_UNKN': 635,\n",
       " '1000800р_UNKN': 636,\n",
       " '1000_UNKN': 637,\n",
       " '1000btc_UNKN': 638,\n",
       " '1000в_UNKN': 639,\n",
       " '1000гр_UNKN': 640,\n",
       " '1000грн_UNKN': 641,\n",
       " '1000й_UNKN': 642,\n",
       " '1000к_UNKN': 643,\n",
       " '1000км_UNKN': 644,\n",
       " '1000кмстараюсь_UNKN': 645,\n",
       " '1000мб_UNKN': 646,\n",
       " '1000мин_UNKN': 647,\n",
       " '1000мм_UNKN': 648,\n",
       " '1000ни_UNKN': 649,\n",
       " '1000но_UNKN': 650,\n",
       " '1000р_UNKN': 651,\n",
       " '1000рмесяц_UNKN': 652,\n",
       " '1000руб_UNKN': 653,\n",
       " '1000угольное_UNKN': 654,\n",
       " '1000х2100_UNKN': 655,\n",
       " '1000х750_UNKN': 656,\n",
       " '1000х90х90мм_UNKN': 657,\n",
       " '1000ысячи_UNKN': 658,\n",
       " '10010010000_UNKN': 659,\n",
       " '100104_UNKN': 660,\n",
       " '100110_UNKN': 661,\n",
       " '100120_UNKN': 662,\n",
       " '1001230_UNKN': 663,\n",
       " '1001233_UNKN': 664,\n",
       " '100130_UNKN': 665,\n",
       " '100130т_UNKN': 666,\n",
       " '100150_UNKN': 667,\n",
       " '100150р_UNKN': 668,\n",
       " '100170_UNKN': 669,\n",
       " '1001_UNKN': 670,\n",
       " '1001мем_UNKN': 671,\n",
       " '100200_UNKN': 672,\n",
       " '100200к_UNKN': 673,\n",
       " '100200л_UNKN': 674,\n",
       " '100250_UNKN': 675,\n",
       " '1002_UNKN': 676,\n",
       " '100300_UNKN': 677,\n",
       " '1004_UNKN': 678,\n",
       " '1005000_UNKN': 679,\n",
       " '100500_UNKN': 680,\n",
       " '100500кг_UNKN': 681,\n",
       " '100500ый_UNKN': 682,\n",
       " '100500этажки_UNKN': 683,\n",
       " '100500я_UNKN': 684,\n",
       " '1005100_UNKN': 685,\n",
       " '1005_UNKN': 686,\n",
       " '10060_UNKN': 687,\n",
       " '1006_UNKN': 688,\n",
       " '1008317_UNKN': 689,\n",
       " '1009.332622_UNKN': 690,\n",
       " '100_UNKN': 691,\n",
       " '100hp_UNKN': 692,\n",
       " '100k_UNKN': 693,\n",
       " '100kg_UNKN': 694,\n",
       " '100kk_UNKN': 695,\n",
       " '100leshnica_UNKN': 696,\n",
       " '100lvl_UNKN': 697,\n",
       " '100rub_UNKN': 698,\n",
       " '100rubley_UNKN': 699,\n",
       " '100rublеy_UNKN': 700,\n",
       " '100rublеу_UNKN': 701,\n",
       " '100su_UNKN': 702,\n",
       " '100алиби_UNKN': 703,\n",
       " '100в_UNKN': 704,\n",
       " '100г100м_UNKN': 705,\n",
       " '100г500м_UNKN': 706,\n",
       " '100г_UNKN': 707,\n",
       " '100гб_UNKN': 708,\n",
       " '100главый_UNKN': 709,\n",
       " '100го_UNKN': 710,\n",
       " '100гр_UNKN': 711,\n",
       " '100градусного_UNKN': 712,\n",
       " '100грамм_UNKN': 713,\n",
       " '100грн_UNKN': 714,\n",
       " '100джек_UNKN': 715,\n",
       " '100евр_UNKN': 716,\n",
       " '100евроночь_UNKN': 717,\n",
       " '100жужжит_UNKN': 718,\n",
       " '100изменение_UNKN': 719,\n",
       " '100й_UNKN': 720,\n",
       " '100к1_UNKN': 721,\n",
       " '100к200к_UNKN': 722,\n",
       " '100к_UNKN': 723,\n",
       " '100ка_UNKN': 724,\n",
       " '100кб_UNKN': 725,\n",
       " '100кг_UNKN': 726,\n",
       " '100кило_UNKN': 727,\n",
       " '100килограммовому_UNKN': 728,\n",
       " '100килограмовая_UNKN': 729,\n",
       " '100кк_UNKN': 730,\n",
       " '100км_UNKN': 731,\n",
       " '100кмес_UNKN': 732,\n",
       " '100кмин_UNKN': 733,\n",
       " '100кмч_UNKN': 734,\n",
       " '100крат_UNKN': 735,\n",
       " '100л_UNKN': 736,\n",
       " '100лет_UNKN': 737,\n",
       " '100летие_UNKN': 738,\n",
       " '100летию_UNKN': 739,\n",
       " '100летними_UNKN': 740,\n",
       " '100литров_UNKN': 741,\n",
       " '100м2_UNKN': 742,\n",
       " '100м36524200057м_UNKN': 743,\n",
       " '100м_UNKN': 744,\n",
       " '100максимум_UNKN': 745,\n",
       " '100мб_UNKN': 746,\n",
       " '100мбит_UNKN': 747,\n",
       " '100мбс_UNKN': 748,\n",
       " '100мбсек_UNKN': 749,\n",
       " '100мегатонных_UNKN': 750,\n",
       " '100местное_UNKN': 751,\n",
       " '100метров_UNKN': 752,\n",
       " '100мигабитный_UNKN': 753,\n",
       " '100мл_UNKN': 754,\n",
       " '100млн_UNKN': 755,\n",
       " '100мм_UNKN': 756,\n",
       " '100ная_UNKN': 757,\n",
       " '100но_UNKN': 758,\n",
       " '100ный_UNKN': 759,\n",
       " '100ого_UNKN': 760,\n",
       " '100отка_UNKN': 761,\n",
       " '100пакетиков_UNKN': 762,\n",
       " '100прикинь_UNKN': 763,\n",
       " '100пудов_UNKN': 764,\n",
       " '100пудово_UNKN': 765,\n",
       " '100р_UNKN': 766,\n",
       " '100ркг_UNKN': 767,\n",
       " '100рсутки_UNKN': 768,\n",
       " '100руб_UNKN': 769,\n",
       " '100рублей_UNKN': 770,\n",
       " '100рштука_UNKN': 771,\n",
       " '100с_UNKN': 772,\n",
       " '100см100см_UNKN': 773,\n",
       " '100см_UNKN': 774,\n",
       " '100сработает_UNKN': 775,\n",
       " '100стопроцентной_UNKN': 776,\n",
       " '100т_UNKN': 777,\n",
       " '100тб_UNKN': 778,\n",
       " '100то_UNKN': 779,\n",
       " '100тонн_UNKN': 780,\n",
       " '100тыс_UNKN': 781,\n",
       " '100тыщ_UNKN': 782,\n",
       " '100тыщъпятистаэташки_UNKN': 783,\n",
       " '100уровень_UNKN': 784,\n",
       " '100хп_UNKN': 785,\n",
       " '100шт_UNKN': 786,\n",
       " '100штук_UNKN': 787,\n",
       " '100я_UNKN': 788,\n",
       " '10100111_UNKN': 789,\n",
       " '10100_UNKN': 790,\n",
       " '10101010101010101010_UNKN': 791,\n",
       " '10101030_UNKN': 792,\n",
       " '1010400153_UNKN': 793,\n",
       " '1010_UNKN': 794,\n",
       " '10110000_UNKN': 795,\n",
       " '10110_UNKN': 796,\n",
       " '10111110_UNKN': 797,\n",
       " '101111_UNKN': 798,\n",
       " '1011_UNKN': 799,\n",
       " '1011к_UNKN': 800,\n",
       " '1011лет_UNKN': 801,\n",
       " '1012_UNKN': 802,\n",
       " '1013_UNKN': 803,\n",
       " '1013к_UNKN': 804,\n",
       " '1014_UNKN': 805,\n",
       " '10155_UNKN': 806,\n",
       " '1015_UNKN': 807,\n",
       " '1015к_UNKN': 808,\n",
       " '1015кг_UNKN': 809,\n",
       " '1015р_UNKN': 810,\n",
       " '1016_UNKN': 811,\n",
       " '1017_UNKN': 812,\n",
       " '10180_UNKN': 813,\n",
       " '1018_UNKN': 814,\n",
       " '1019_UNKN': 815,\n",
       " '101_UNKN': 816,\n",
       " '101вый_UNKN': 817,\n",
       " '101й_UNKN': 818,\n",
       " '101р_UNKN': 819,\n",
       " '10200_UNKN': 820,\n",
       " '1020_UNKN': 821,\n",
       " '1020вт_UNKN': 822,\n",
       " '1020к_UNKN': 823,\n",
       " '1020кбсек_UNKN': 824,\n",
       " '1021_UNKN': 825,\n",
       " '1023_UNKN': 826,\n",
       " '1024450_UNKN': 827,\n",
       " '102456_UNKN': 828,\n",
       " '1024_UNKN': 829,\n",
       " '1024гб_UNKN': 830,\n",
       " '1024х768_UNKN': 831,\n",
       " '10251220_UNKN': 832,\n",
       " '10255_UNKN': 833,\n",
       " '1025_UNKN': 834,\n",
       " '1028_UNKN': 835,\n",
       " '102_UNKN': 836,\n",
       " '102х77_UNKN': 837,\n",
       " '102х_UNKN': 838,\n",
       " '103.4_UNKN': 839,\n",
       " '1030_UNKN': 840,\n",
       " '1030²24500метров_UNKN': 841,\n",
       " '1030к_UNKN': 842,\n",
       " '103101.34_UNKN': 843,\n",
       " '10320_UNKN': 844,\n",
       " '1032_UNKN': 845,\n",
       " '1034_UNKN': 846,\n",
       " '10350_UNKN': 847,\n",
       " '10376_UNKN': 848,\n",
       " '1039_UNKN': 849,\n",
       " '103_UNKN': 850,\n",
       " '103й_UNKN': 851,\n",
       " '1040_UNKN': 852,\n",
       " '1040к_UNKN': 853,\n",
       " '1045_UNKN': 854,\n",
       " '1048576_UNKN': 855,\n",
       " '1048_UNKN': 856,\n",
       " '104_UNKN': 857,\n",
       " '104а_UNKN': 858,\n",
       " '105.1_UNKN': 859,\n",
       " '105.3_UNKN': 860,\n",
       " '1050.3_UNKN': 861,\n",
       " '10500_UNKN': 862,\n",
       " '10500рсмены_UNKN': 863,\n",
       " '10501060_UNKN': 864,\n",
       " '1050_UNKN': 865,\n",
       " '1050ti_UNKN': 866,\n",
       " '1050ти_UNKN': 867,\n",
       " '1052_UNKN': 868,\n",
       " '10532_UNKN': 869,\n",
       " '105520_UNKN': 870,\n",
       " '105570773_UNKN': 871,\n",
       " '1055_UNKN': 872,\n",
       " '1056090_UNKN': 873,\n",
       " '10568_UNKN': 874,\n",
       " '1056_UNKN': 875,\n",
       " '1058_UNKN': 876,\n",
       " '10597_UNKN': 877,\n",
       " '105_UNKN': 878,\n",
       " '105кг_UNKN': 879,\n",
       " '105ст_UNKN': 880,\n",
       " '10609_UNKN': 881,\n",
       " '1060_UNKN': 882,\n",
       " '1060ti_UNKN': 883,\n",
       " '106400_UNKN': 884,\n",
       " '10660_UNKN': 885,\n",
       " '106_UNKN': 886,\n",
       " '106к_UNKN': 887,\n",
       " '106сильный_UNKN': 888,\n",
       " '1070_UNKN': 889,\n",
       " '1070ti_UNKN': 890,\n",
       " '107218кмч_UNKN': 891,\n",
       " '1074_UNKN': 892,\n",
       " '1075932mail_UNKN': 893,\n",
       " '1075_UNKN': 894,\n",
       " '10770_UNKN': 895,\n",
       " '1078620368940859_UNKN': 896,\n",
       " '1079_UNKN': 897,\n",
       " '107_UNKN': 898,\n",
       " '107ми_UNKN': 899,\n",
       " '10800_UNKN': 900,\n",
       " '1080_UNKN': 901,\n",
       " '1080gtx_UNKN': 902,\n",
       " '1080i_UNKN': 903,\n",
       " '1080p_UNKN': 904,\n",
       " '1080ti_UNKN': 905,\n",
       " '1080р_UNKN': 906,\n",
       " '1080ти_UNKN': 907,\n",
       " '1081_UNKN': 908,\n",
       " '1084885492244773410223_UN': 909,\n",
       " '10862_UNKN': 910,\n",
       " '108659_UNKN': 911,\n",
       " '1086_UNKN': 912,\n",
       " '1089_UNKN': 913,\n",
       " '108_UNKN': 914,\n",
       " '109110_UNKN': 915,\n",
       " '1093_UNKN': 916,\n",
       " '1096_UNKN': 917,\n",
       " '109_UNKN': 918,\n",
       " '109к_UNKN': 919,\n",
       " '109лс_UNKN': 920,\n",
       " '10_UNKN': 921,\n",
       " '10g_UNKN': 922,\n",
       " '10h_UNKN': 923,\n",
       " '10k_UNKN': 924,\n",
       " '10minutemail_UNKN': 925,\n",
       " '10p_UNKN': 926,\n",
       " '10x8_UNKN': 927,\n",
       " '10xx_UNKN': 928,\n",
       " '10³⁰⁰_UNKN': 929,\n",
       " '10¹⁸_UNKN': 930,\n",
       " '10а_UNKN': 931,\n",
       " '10б_UNKN': 932,\n",
       " '10бальной_UNKN': 933,\n",
       " '10бесконечность_UNKN': 934,\n",
       " '10воды10песка1цемент_UNKN': 935,\n",
       " '10вт_UNKN': 936,\n",
       " '10га_UNKN': 937,\n",
       " '10гб_UNKN': 938,\n",
       " '10го_UNKN': 939,\n",
       " '10грамм_UNKN': 940,\n",
       " '10десять_UNKN': 941,\n",
       " '10е_UNKN': 942,\n",
       " '10есяток_UNKN': 943,\n",
       " '10есять_UNKN': 944,\n",
       " '10есятью_UNKN': 945,\n",
       " '10и15и_UNKN': 946,\n",
       " '10и_UNKN': 947,\n",
       " '10из10_UNKN': 948,\n",
       " '10й_UNKN': 949,\n",
       " '10к_UNKN': 950,\n",
       " '10ка_UNKN': 951,\n",
       " '10кбитс_UNKN': 952,\n",
       " '10кв_UNKN': 953,\n",
       " '10кг_UNKN': 954,\n",
       " '10кгсм³_UNKN': 955,\n",
       " '10ке_UNKN': 956,\n",
       " '10ки_UNKN': 957,\n",
       " '10кило_UNKN': 958,\n",
       " '10кк_UNKN': 959,\n",
       " '10ккк_UNKN': 960,\n",
       " '10км_UNKN': 961,\n",
       " '10кой_UNKN': 962,\n",
       " '10коп_UNKN': 963,\n",
       " '10копеечные_UNKN': 964,\n",
       " '10копеечными_UNKN': 965,\n",
       " '10которые_UNKN': 966,\n",
       " '10кратную_UNKN': 967,\n",
       " '10ку_UNKN': 968,\n",
       " '10л_UNKN': 969,\n",
       " '10лет_UNKN': 970,\n",
       " '10летие_UNKN': 971,\n",
       " '10летия_UNKN': 972,\n",
       " '10летнего_UNKN': 973,\n",
       " '10летней_UNKN': 974,\n",
       " '10летний_UNKN': 975,\n",
       " '10летним_UNKN': 976,\n",
       " '10летних_UNKN': 977,\n",
       " '10летняя_UNKN': 978,\n",
       " '10м1атм_UNKN': 979,\n",
       " '10м3_UNKN': 980,\n",
       " '10м_UNKN': 981,\n",
       " '10мб_UNKN': 982,\n",
       " '10мбитс_UNKN': 983,\n",
       " '10мбс_UNKN': 984,\n",
       " '10мг_UNKN': 985,\n",
       " '10месячный_UNKN': 986,\n",
       " '10метров_UNKN': 987,\n",
       " '10метровую_UNKN': 988,\n",
       " '10метровый_UNKN': 989,\n",
       " '10миль_UNKN': 990,\n",
       " '10мин_UNKN': 991,\n",
       " '10минут_UNKN': 992,\n",
       " '10минутах_UNKN': 993,\n",
       " '10млн_UNKN': 994,\n",
       " '10мм_UNKN': 995,\n",
       " '10можно_UNKN': 996,\n",
       " '10нм_UNKN': 997,\n",
       " '10но_UNKN': 998,\n",
       " '10ну_UNKN': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voc1.char2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3341ef90e824ef0a14e27bf90c3918d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[169730   6934  91754 ...      4      4      4]\n",
      " [169730   6934  91754 ...      4      4      4]\n",
      " [169730   6934  91754 ...      4      4      4]\n",
      " ...\n",
      " [ 52516  99766  57344 ...      4      4      4]\n",
      " [ 52516  99766  57344 ...      4      4      4]\n",
      " [ 52516  99766  57344 ...      4      4      4]]\n",
      "[[ 99766 149023 105369 ...      4      4      4]\n",
      " [188210 181013  45919 ...      4      4      4]\n",
      " [ 36253  56266 126309 ...      4      4      4]\n",
      " ...\n",
      " [154435 114713  71694 ...      4      4      4]\n",
      " [ 71694  97254 114713 ...      4      4      4]\n",
      " [188210 182252 152074 ...      4      4      4]]\n",
      "[[152074 164183  67894 ...      4      4      4]\n",
      " [154435 114713  71694 ...      4      4      4]\n",
      " [154435 114713  71694 ...      4      4      4]\n",
      " ...\n",
      " [126392   4256 328320 ...      4      4      4]\n",
      " [ 37280 153558 149710 ...      4      4      4]\n",
      " [138822 353514 119067 ...      4      4      4]]\n",
      "[[110355 172004 407414 ...      4      4      4]\n",
      " [110355 172004 407414 ...      4      4      4]\n",
      " [110355 172004 407414 ...      4      4      4]\n",
      " ...\n",
      " [100444 150936 132021 ...      4      4      4]\n",
      " [172618  69450 180630 ...      4      4      4]\n",
      " [150861  44902 118985 ...      4      4      4]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "voc1.indx_tokenize_from_files(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "424590"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_arr = None\n",
    "for i in range(1):\n",
    "    with np.load(f'tokenized_files/np_tokens{i}.npz') as data:\n",
    "        if np_arr is not None:\n",
    "            np_arr = np.append(np_arr,data['arr'],axis=0)\n",
    "        else:\n",
    "            np_arr = data['arr']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_arr = None\n",
    "for i in range(1):\n",
    "    with np.load(f'indexed_token_files/indx_tokens{i}.npz') as data:\n",
    "        if np_arr is not None:\n",
    "            np_arr = np.append(np_arr,data['arr'],axis=0)\n",
    "        else:\n",
    "            np_arr = data['arr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000000, 40)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "320.0"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_arr.nbytes/1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[128897, 155086, 150350, ...,      4,      4,      4],\n",
       "       [111158,   2771,  42423, ...,  11618, 115962,  42423],\n",
       "       [158623, 111158,  73738, ...,      4,      4,      4],\n",
       "       ...,\n",
       "       [160165, 132439,  89121, ...,      4,      4,      4],\n",
       "       [ 89121, 188210, 123192, ...,      4,      4,      4],\n",
       "       [ 54942, 170538, 172776, ...,      4,      4,      4]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 264 ms, sys: 10.4 ms, total: 274 ms\n",
      "Wall time: 260 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'139_UNKN'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time np.unique(np_context)[100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = KeyedVectors.load_word2vec_format('../../model.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = './model'\n",
    "vocab_size = len(voc)\n",
    "sentence_size = 40\n",
    "embedding_size = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_fn(x,  params, is_training):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(x)\n",
    "\n",
    "    if is_training:\n",
    "        dataset = dataset.shuffle(buffer_size=params['buffer_size'])\n",
    "        dataset = dataset.repeat(count=params['num_epochs'])\n",
    "\n",
    "    dataset = dataset.batch(params['batch_size'])\n",
    "    dataset = dataset.prefetch(buffer_size=2)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    return iterator.get_next()\n",
    "    #return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def initializer(shape=None, dtype=None, partition_info=None):    \n",
    "    vocab_dict = count_v.vocabulary_\n",
    "    embedding_matrix = np.random.uniform(-1, 1, size=(vocab_size+1, embedding_size))\n",
    "    num_loaded = 0\n",
    "    for w, i in vocab_dict.items():\n",
    "        v = None\n",
    "        try:\n",
    "            v = word2vec[w]\n",
    "        except KeyError: # не нашли такой токен в словаре\n",
    "                pass\n",
    "        if v is not None :\n",
    "            embedding_matrix[i+1] = v\n",
    "            num_loaded += 1\n",
    "   \n",
    "    embedding_matrix = embedding_matrix.astype(np.float32)\n",
    "    embedding_matrix[0] = np.zeros(300)\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn(features, labels, mode, params):  \n",
    "    \n",
    "    # Compute predictions.\n",
    "    net = params['net']\n",
    "    \n",
    "    encoded_features = {}  \n",
    "    \n",
    "    with tf.variable_scope('encoder'):\n",
    "        encoded_features['anchor'] = net(features['anchor'])\n",
    "    with tf.variable_scope('encoder', reuse=True):\n",
    "        encoded_features['positive'] = net(features['positive'])\n",
    "    with tf.variable_scope('encoder', reuse=True):\n",
    "        encoded_features['negative'] = net(features['negative'])\n",
    "    \n",
    "    \n",
    "    #predicted_classes = tf.argmax(logits, 1)\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        predictions = {\n",
    "            'class_ids': predicted_classes[:, tf.newaxis],\n",
    "            'probabilities': tf.nn.softmax(logits),\n",
    "            'logits': logits,\n",
    "        }\n",
    "        return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
    "    \n",
    "    # Compute loss.\n",
    "    loss = my_triplet_loss(encoded_features,params['margin'])\n",
    "    \n",
    "#     # Compute evaluation metrics.\n",
    "#     accuracy = tf.metrics.accuracy(labels=labels,\n",
    "#                                predictions=predicted_classes,\n",
    "#                                name='acc_op')\n",
    "#     f1 = tf_metrics.f1(labels=labels,\n",
    "#                                predictions=predicted_classes,num_classes=3,average='micro')\n",
    "    \n",
    "#     metrics = {'accuracy': accuracy,'f1':f1}\n",
    "#     tf.summary.scalar('accuracy', accuracy[1])\n",
    "    \n",
    "#     # Compute evaluation\n",
    "#     if mode == tf.estimator.ModeKeys.EVAL:\n",
    "#         return tf.estimator.EstimatorSpec(\n",
    "#             mode, loss=loss, eval_metric_ops=metrics)\n",
    "    \n",
    "    optimizer = tf.train.AdagradOptimizer(learning_rate=0.1)\n",
    "    \n",
    "    train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())\n",
    "    \n",
    "    return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
