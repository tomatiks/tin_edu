{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Машинное обучение в диалоговых системах\n",
    "\n",
    "Рады вас видеть на курсе по разработке диалоговых систем от Тинькофф Финтех школы! \n",
    "\n",
    "В рамках курса мы рассмотрим, как **обработка естественного языка** и **машинное обучение** используются для построения чат-бот систем, от начала и до конца."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://media.giphy.com/media/aaZzZWqycDujC/giphy.gif\" width=\"400\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Эта лекция вводная, в ней мы рассмотрим:\n",
    "* предобработку текста\n",
    "* представление текста\n",
    "* понятие эмбеддинга\n",
    "* текстовую классификацию"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Предобработка текста\n",
    "\n",
    "Текст на естественном языке, который нужно обрабатывать в задачах машинного обучения, сильно зависит от источника. Пример:\n",
    "\n",
    "Википедия\n",
    "> Литературный язык — обработанная часть общенародного языка, обладающая в большей или меньшей степени письменно закреплёнными нормами; язык всех проявлений культуры, выражающихся в словесной форме.\n",
    "\n",
    "Твиттер\n",
    "> Если у вас в компании есть люди, которые целый день сидят в чатиках и смотрят видосики, то, скорее всего, это ДАТАСАЕНТИСТЫ и у них ОБУЧАЕТСЯ\n",
    "\n",
    "Ответы@Mail.ru\n",
    "> как пишется \"Вообщем лето было отличное\" раздельно или слитно слово ВОобщем?? ?\n",
    "\n",
    "В связи с этим, возникает задача предобработки (или нормализации) текста, то есть приведения к некоторому единому виду.\n",
    "\n",
    "**Quiz: Какие шаги/действия можно производить и что это даст?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "text = 'купил таблетки от тупости, но не смог открыть банку,ЧТО ДЕЛАТЬ???'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### 1.1 Приведение текста к нижнему регистру."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'купил таблетки от тупости, но не смог открыть банку,что делать???'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = text.lower()\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 Удаление неинформативных символов.\n",
    "\n",
    "Такими символами могут быть символы пунктуации, спец-символы, повторяющиеся символы, цифры. Для удаления подобных символов можно пользоваться стандартной библиотекой для [регулярных выражений](https://docs.python.org/3/library/re.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'купил таблетки от тупости, но не смог открыть банку,что делать?'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regex = re.compile(r'(\\W)\\1+')\n",
    "regex.sub(r'\\1', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'купил таблетки от тупости  но не смог открыть банку что делать'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regex = re.compile(r'[^\\w\\s]')\n",
    "regex.sub(r' ', text).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 Удаление неинформативных слов.\n",
    "\n",
    "Плохие слова:\n",
    "* Слишком частые\n",
    "<br> &nbsp; русский язык: и, но, я, ты, ...\n",
    "<br> &nbsp; английский язык: a, the, I, ...\n",
    "<br> &nbsp; специфичные для коллекции: \"сообщать\" в новостях\n",
    "* Слишком редкие\n",
    "* Стоп-слова\n",
    "<br> &nbsp; Предлоги, междометия, частицы, цифры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/dmitriy/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"isn't\", 'y', 'couldn', 'the', 'whom', 've']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "sw_eng = set(stopwords.words('english'))\n",
    "list(sw_eng)[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['был', 'всех', 'есть', 'для', 'во', 'ни']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "sw_ru = set(stopwords.words('russian'))\n",
    "list(sw_ru)[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "До 111 слов\n",
      "После 60 слов\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Разница прежним теперешнем состоянием состояла том, прежде, забывал то, ним, то, говорили, он, страдальчески сморщив лоб, пытался мог разглядеть чего-то, далеко отстоящего него. Теперь также забывал то, говорили, то, ним; заметной, насмешливой, улыбкой самое, ним, вслушивался то, говорили, хотя очевидно видел слышал что-то другое... Теперь улыбка радости жизни постоянно играла рта, глазах светилось участие людям - вопрос: довольны же, он?'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = 'Разница между прежним и теперешнем его состоянием состояла в том, \\\n",
    "        что прежде, когда он забывал то, что было перед ним, то, что ему \\\n",
    "        говорили, он, страдальчески сморщив лоб, как будто пытался и не мог\\\n",
    "        разглядеть чего-то, далеко отстоящего от него. Теперь он также \\\n",
    "        забывал то, что ему говорили, и то, что было перед ним; но теперь \\\n",
    "        с чуть заметной, как будто насмешливой, улыбкой он в то самое, \\\n",
    "        что было перед ним, вслушивался в то, что ему говорили, хотя \\\n",
    "        очевидно видел и слышал что-то совсем другое...\\\n",
    "        Теперь улыбка радости жизни постоянно играла у его рта, и в глазах \\\n",
    "        его светилось участие к людям - вопрос: довольны ли они так же, как он?'\n",
    "clean_sent = ' '.join([word for word in sent.split() if not word in sw_ru])\n",
    "print('До {} слов'.format(len(sent.split())))\n",
    "print('После {} слов'.format(len(clean_sent.split())))\n",
    "clean_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sent = 'быть или не быть'\n",
    "clean_sent = ' '.join([word for word in sent.split() if not word in sw_ru])\n",
    "print(clean_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Разбиение текста на смысловые единицы (токенизация)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Купите кружку-термос \"Hello Kitty\" на 0.5л (64см³) за 3 рубля. До 01.01.2050.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Самый простой подход к токенизации - это разбиение по текста по пробельным символам. \n",
    "\n",
    "**Quiz: Какая у этого подхода есть проблема?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Купите',\n",
       " 'кружку-термос',\n",
       " '\"Hello',\n",
       " 'Kitty\"',\n",
       " 'на',\n",
       " '0.5л',\n",
       " '(64см³)',\n",
       " 'за',\n",
       " '3',\n",
       " 'рубля.',\n",
       " 'До',\n",
       " '01.01.2050.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В библиотеке для морфологического анализа для русского языка [`pymorphy2`](https://pymorphy2.readthedocs.io/en/latest/) есть простая вспомогательная функция для токенизации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Купите',\n",
       " 'кружку-термос',\n",
       " '\"',\n",
       " 'Hello',\n",
       " 'Kitty',\n",
       " '\"',\n",
       " 'на',\n",
       " '0',\n",
       " '.',\n",
       " '5л',\n",
       " '(',\n",
       " '64см³',\n",
       " ')',\n",
       " 'за',\n",
       " '3',\n",
       " 'рубля',\n",
       " '.',\n",
       " 'До',\n",
       " '01',\n",
       " '.',\n",
       " '01',\n",
       " '.',\n",
       " '2050',\n",
       " '.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pymorphy2.tokenizers import simple_word_tokenize\n",
    "\n",
    "simple_word_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Более сложной метод токенизации представлен в [`nltk`](https://www.nltk.org/): библиотеке для общего NLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Купите',\n",
       " 'кружку-термос',\n",
       " '``',\n",
       " 'Hello',\n",
       " 'Kitty',\n",
       " \"''\",\n",
       " 'на',\n",
       " '0.5л',\n",
       " '(',\n",
       " '64см³',\n",
       " ')',\n",
       " 'за',\n",
       " '3',\n",
       " 'рубля',\n",
       " '.',\n",
       " 'До',\n",
       " '01.01.2050',\n",
       " '.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.word_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для русского языка также есть новая специализированная библиотека [`razdel`](https://github.com/natasha/razdel)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Купите',\n",
       " 'кружку-термос',\n",
       " '\"',\n",
       " 'Hello',\n",
       " 'Kitty',\n",
       " '\"',\n",
       " 'на',\n",
       " '0.5',\n",
       " 'л',\n",
       " '(',\n",
       " '64',\n",
       " 'см³',\n",
       " ')',\n",
       " 'за',\n",
       " '3',\n",
       " 'рубля',\n",
       " '.',\n",
       " 'До',\n",
       " '01.01.2050',\n",
       " '.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import razdel\n",
    "\n",
    "\n",
    "def tokenize_with_razdel(text):\n",
    "    return [token.text for token in razdel.tokenize(text)]\n",
    "\n",
    "\n",
    "tokenize_with_razdel(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Привет', ' Ты видел мр', 'Смита сегодня утром', '']\n"
     ]
    }
   ],
   "source": [
    "hard_string = 'Привет. Ты видел мр.Смита сегодня утром?'\n",
    "tmp_string = re.split(r'[!.?]', hard_string)\n",
    "print(tmp_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Привет.', 'Ты видел мр.Смита сегодня утром?']\n"
     ]
    }
   ],
   "source": [
    "hard_string = 'Привет. Ты видел мр.Смита сегодня утром?'\n",
    "exp = r'(?<!\\w\\.\\w.)(?<![А-Я][а-я]\\.)(?<=\\.|\\?)\\s'\n",
    "tmp_string = re.split(exp, hard_string)\n",
    "print(tmp_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Приведение слов к нормальной форме (стемминг, лемматизация)\n",
    "\n",
    "**Стемминг - это нормализация слова путём отбрасывания окончания по правилам языка.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Такая нормализация хорошо подходит для языков с небольшим разнообразием словоформ, например, для английского. В библиотеке [nltk](https://www.nltk.org/) есть несколько реализаций стеммеров:\n",
    " - [Porter stemmer](http://tartarus.org/martin/PorterStemmer/)\n",
    " - [Snowball stemmer](http://snowball.tartarus.org/)\n",
    " - [Lancaster stemmer](http://www.nltk.org/_modules/nltk/stem/lancaster.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'run'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "SnowballStemmer(language='english').stem('running')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'write wrote written'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "SnowballStemmer(language='english').stem('write wrote written')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для русского языка этот подход не очень подходит, поскольку в русском есть падежные формы, время у глаголов и т.д."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'бежа'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SnowballStemmer(language='russian').stem('бежать')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Лемматизация - приведение слов к начальной морфологической форме (с помощью словаря и грамматики языка).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Вспомогательная задача\n",
    "from nltk import wordnet, pos_tag\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    my_switch = {\n",
    "        'J': wordnet.wordnet.ADJ,\n",
    "        'V': wordnet.wordnet.VERB,\n",
    "        'N': wordnet.wordnet.NOUN,\n",
    "        'R': wordnet.wordnet.ADV,\n",
    "    }\n",
    "    for key, item in my_switch.items():\n",
    "        if treebank_tag.startswith(key):\n",
    "            return item\n",
    "    return wordnet.wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet.wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('George', 'NNP'), ('admitted', 'VBD'), ('the', 'DT'), ('talks', 'NNS'), ('happened', 'VBD')]\n"
     ]
    }
   ],
   "source": [
    "sent = 'George admitted the talks happened'.split()\n",
    "pos_tagged = pos_tag(sent)\n",
    "print(pos_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import WordNetLemmatizer\n",
    "def my_lemmatizer(sent):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokenized_sent = sent.split()\n",
    "    pos_tagged = [(word, get_wordnet_pos(tag))\n",
    "                 for word, tag in pos_tag(tokenized_sent)]\n",
    "    print(pos_tagged)\n",
    "    return ' '.join([lemmatizer.lemmatize(word, tag)\n",
    "                    for word, tag in pos_tagged])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('write', 'a'), ('wrote', 'v'), ('written', 'v')]\n",
      "write write write\n"
     ]
    }
   ],
   "source": [
    "sent = 'write wrote written'\n",
    "print(my_lemmatizer(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для лемматизации русских слов есть несколько библиотек в свободном доступе:\n",
    "- [pymorphy2](https://pymorphy2.readthedocs.io/en/latest/)\n",
    "- [mystem3](https://tech.yandex.ru/mystem/)\n",
    "- [maru](https://github.com/chomechome/maru)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Самый простой подход к лемматизации - словарный. Здесь не учитывается контекст слова, поэтому для омонимов такой подход работает не всегда. Такой подход применяет библиотека `pymorphy2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pymorphy2 import MorphAnalyzer\n",
    "\n",
    "pymorphy = MorphAnalyzer()\n",
    "\n",
    "\n",
    "def lemmatize_with_pymorphy(tokens):\n",
    "    return [pymorphy.parse(token)[0].normal_form for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['мама', 'мыло', 'рам']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatize_with_pymorphy(['мама', 'мыла', 'раму'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Библиотека от Яндекса `mystem3` обходит это ограничение и рассматривает контекст слова, используя статистику и правила."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem\n",
    "\n",
    "mystem = Mystem()\n",
    "\n",
    "\n",
    "def lemmatize_with_mystem(text):\n",
    "    return [lemma for lemma in mystem.lemmatize(text) if not lemma.isspace()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['мама', 'мыть', 'рама']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatize_with_mystem('мама мыла раму')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но на более сложных примерах такой подход тоже может сойтись к самому частотному варианту."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['на', 'завод', 'становиться', 'увидеть', 'вид', 'становиться']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatize_with_mystem('на заводе стали увидел виды стали')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Библиотека `maru` использует машинное обучение и нейросети для разрешения омонимии."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'maru'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-3038c743c21d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmaru\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmaru_rnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaru\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_analyzer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rnn'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'maru'"
     ]
    }
   ],
   "source": [
    "import maru\n",
    "\n",
    "maru_rnn = maru.get_analyzer('rnn')\n",
    "\n",
    "\n",
    "def lemmatize_with_maru(tokens):\n",
    "    return [morph.lemma for morph in maru_rnn.analyze(tokens)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['на', 'завод', 'сталь', 'увидеть', 'вид', 'сталь']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatize_with_maru(['на', 'заводе', 'стали', 'увидел', 'виды', 'стали'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Представление текста\n",
    "\n",
    "**Quiz: Как можно использовать токенизированные тексты в задачах NLP? Какие варианты представления текста можете назвать?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 One-Hot Encoding\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1200/1*YEJf9BQQh0ma1ECs6x_7yQ.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz: Что такое разреженная матрица?**\n",
    "\n",
    "**Quiz: Какие есть плюсы и минусы у one-hot представления?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как получить one-hot? \n",
    "\n",
    "Сначала нам нужно каждому слову поставить в соответствие номер, а затем перевести их в бинарные вектора. \n",
    "\n",
    "Используем библиотеку [`scikit-learn`](https://scikit-learn.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-30T11:44:20.487638Z",
     "start_time": "2019-01-30T11:44:20.482337Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 1])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "words = ['NLP', 'is', 'awesome']\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "corpus_encoded = label_encoder.fit_transform(words)\n",
    "corpus_encoded # какой вывод?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['NLP', 'awesome', 'is'], dtype='<U7')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_encoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "onehot_encoder.fit_transform(corpus_encoded.reshape(-1, 1)) # какой вывод?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В библиотеке для построения нейросетей [`keras`](https://keras.io/) есть более удобная функция для такого кодирования."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras.utils\n",
    "\n",
    "keras.utils.to_categorical(corpus_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Bag-of-words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-30T12:05:25.824178Z",
     "start_time": "2019-01-30T12:05:25.820140Z"
    }
   },
   "source": [
    "**BoW** - \"мешок слов\". \n",
    "\n",
    "**Quiz: Что будет, если мы сложим все one-hot вектора слов в тексте?**\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://slideplayer.com/slide/7073400/24/images/15/The+Bag+of+Words+Representation.jpg)\n",
    "Посчитаем количество слов в текстах"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-30T11:51:36.904566Z",
     "start_time": "2019-01-30T11:51:36.901906Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    'Дочь бьет Марго',\n",
    "    'Тот кто бьет, не знает кто бьет его',\n",
    "    'Кто кого бьет?',\n",
    "    'Марго бьет дочь?',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-30T11:51:44.357709Z",
     "start_time": "2019-01-30T11:51:44.350257Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1, 1, 0, 0, 0, 0, 1, 0, 0],\n",
       "        [2, 0, 1, 1, 0, 2, 0, 1, 1],\n",
       "        [1, 0, 0, 0, 1, 1, 0, 0, 0],\n",
       "        [1, 1, 0, 0, 0, 0, 1, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit_transform(corpus).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'дочь': 1,\n",
       " 'бьет': 0,\n",
       " 'марго': 6,\n",
       " 'тот': 8,\n",
       " 'кто': 5,\n",
       " 'не': 7,\n",
       " 'знает': 3,\n",
       " 'его': 2,\n",
       " 'кого': 4}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz: Какие минусы у такого представления?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 TF-IDF\n",
    "\n",
    "**Term Frequency**  $tf(w,d)$ - сколько раз слово $w$ встретилось в документе $d$\n",
    "\n",
    "**Document Frequency** $df(w)$ - сколько документов содержат слово $w$\n",
    "\n",
    "**Inverse Document Frequency** $idf(w) = log_2(N/df(w))$  — обратная документная частотность. \n",
    "\n",
    "**TF-IDF**=$tf(w,d)*idf(w)$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.42389674, 0.64043405, 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.64043405, 0.        , 0.        ],\n",
       "        [0.37919167, 0.        , 0.36332075, 0.36332075, 0.        ,\n",
       "         0.5728925 , 0.        , 0.36332075, 0.36332075],\n",
       "        [0.37919167, 0.        , 0.        , 0.        , 0.72664149,\n",
       "         0.5728925 , 0.        , 0.        , 0.        ],\n",
       "        [0.42389674, 0.64043405, 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.64043405, 0.        , 0.        ]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "idf_vectorizer = TfidfVectorizer()\n",
    "idf_vectorizer.fit_transform(corpus).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'дочь': 1,\n",
       " 'бьет': 0,\n",
       " 'марго': 6,\n",
       " 'тот': 8,\n",
       " 'кто': 5,\n",
       " 'не': 7,\n",
       " 'знает': 3,\n",
       " 'его': 2,\n",
       " 'кого': 4}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Классификация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz: Что это за задача? Какие ещё задачи решаются в машинном обучении?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы попробуем применить описание методы предобработки и представления текста на примере анализа тональности текста. В качестве данных будем использовать небольшой датасет твитов. Всего в данных 2 класса: позитив и негатив."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Загрузка тренировочных и тестовых данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz: Зачем нам разделять данные на тренировочные и тестовые?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим тренировочные и тестовые данные при помощи библиотеки [`pandas`](https://pandas.pydata.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6929, 2)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv('train.csv')\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>positive</td>\n",
       "      <td>эти розы для прекрасной мамочки)))=_=]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>negative</td>\n",
       "      <td>И да, у меня в этом году серьезные проблемы со...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>positive</td>\n",
       "      <td>♥Обожаю людей, которые заставляют меня смеятьс...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>Вчера нашла в почтовом ящике пустую упаковку и...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>positive</td>\n",
       "      <td>очень долгожданный и хороший день был)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                               text\n",
       "0  positive            эти розы для прекрасной мамочки)))=_=]]\n",
       "1  negative  И да, у меня в этом году серьезные проблемы со...\n",
       "2  positive  ♥Обожаю людей, которые заставляют меня смеятьс...\n",
       "3  negative  Вчера нашла в почтовом ящике пустую упаковку и...\n",
       "4  positive             очень долгожданный и хороший день был)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive    4635\n",
       "negative    2294\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(794, 2)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv('test.csv')\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>positive</td>\n",
       "      <td>#ахахах ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>negative</td>\n",
       "      <td>я очень устал в этом году. очень.  расскажите,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negative</td>\n",
       "      <td>НЕ ТОРОПИТЕСЬ ЖИТЬ!!! сегодня на моих глазах о...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>positive</td>\n",
       "      <td>Когда я его обнимаю, я закрываю глаза, уткнувш...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>меня надо усыпить О_О</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                               text\n",
       "0  positive                                        #ахахах ...\n",
       "1  negative  я очень устал в этом году. очень.  расскажите,...\n",
       "2  negative  НЕ ТОРОПИТЕСЬ ЖИТЬ!!! сегодня на моих глазах о...\n",
       "3  positive  Когда я его обнимаю, я закрываю глаза, уткнувш...\n",
       "4  negative                              меня надо усыпить О_О"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive    536\n",
       "negative    258\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Оценка качества"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz: Как оценить качество модели в задаче классификации?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<table style=\"width:40%\">\n",
    "  <tr>\n",
    "    <th></th>\n",
    "    <th> y = 1 </th> \n",
    "    <th> y = 0 </th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th> a(x) = 1 </th>\n",
    "    <td> True Positive (TP) </td> \n",
    "    <td> False Positive (FP) </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th> a(x) = 0 </th>\n",
    "    <td> False Negative (FN) </td> \n",
    "    <td> True Negative (TN) </td>\n",
    "  </tr>\n",
    "</table>\n",
    "<br>\n",
    "$Accuracy = \\frac{TP + TN}{TP + TN + FP + FN}$\n",
    "<br><br>\n",
    "$Precision = \\frac{TP}{TP + FP}$\n",
    "<br><br>\n",
    "$Recall = \\frac{TP}{TP + FN}$\n",
    "<br><br>\n",
    "$F1 = 2 * \\frac{Precision * Recall}{Precision + Recall}$\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Построение модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем функцию для оценки векторизатора. В качестве модели будем использовать линейный SVM, он хорошо работает для определения тональности.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz: Какая модель машинного обучения называется линейной? Почему мы используем линейную модель в задаче?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import tqdm\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "def evaluate_vectorizer(vectorizer):\n",
    "    X = vectorizer.fit_transform(tqdm.tqdm_notebook(train.text, desc='Vectorizing train:'))\n",
    "\n",
    "    model = LinearSVC(random_state=42)\n",
    "    model.fit(X, train.label)\n",
    "    \n",
    "    X_test = vectorizer.transform(tqdm.tqdm_notebook(test.text, desc='Vectorizing test:'))\n",
    "    predictions = model.predict(X_test)\n",
    "    \n",
    "    print(classification_report(test.label, predictions))\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Сравнение способов представления текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bedd0e6577e3467a8240867076ec9567",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Vectorizing train:', max=6929, style=ProgressStyle(descriptio…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbd73b86d7eb47a6ae28817724d3d437",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Vectorizing test:', max=794, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.73      0.62      0.67       258\n",
      "    positive       0.83      0.89      0.86       536\n",
      "\n",
      "   micro avg       0.80      0.80      0.80       794\n",
      "   macro avg       0.78      0.75      0.76       794\n",
      "weighted avg       0.80      0.80      0.80       794\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_vectorizer(CountVectorizer(min_df=2));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e16ff59fa42e43d6800c625a16e2b103",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Vectorizing train:', max=6929, style=ProgressStyle(descriptio…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61214cef8be945f7b4c0a5441e858bb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Vectorizing test:', max=794, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.76      0.65      0.70       258\n",
      "    positive       0.84      0.90      0.87       536\n",
      "\n",
      "   micro avg       0.82      0.82      0.82       794\n",
      "   macro avg       0.80      0.78      0.79       794\n",
      "weighted avg       0.82      0.82      0.82       794\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_vectorizer(TfidfVectorizer(min_df=2));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65f1bd312f6447edad37b78e32e8cea2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Vectorizing train:', max=6929, style=ProgressStyle(descriptio…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52356e1dcf434e7c9fd9e17d0787305d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Vectorizing test:', max=794, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.78      0.76      0.77       258\n",
      "    positive       0.89      0.90      0.89       536\n",
      "\n",
      "   micro avg       0.85      0.85      0.85       794\n",
      "   macro avg       0.83      0.83      0.83       794\n",
      "weighted avg       0.85      0.85      0.85       794\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_vectorizer(TfidfVectorizer(min_df=2, tokenizer=tokenize_with_razdel));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7405ea0fabed4c85a5441252a493dc80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Vectorizing train:', max=6929, style=ProgressStyle(descriptio…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bcabbc4bf224a0086325ffbb8a664fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Vectorizing test:', max=794, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.80      0.81       258\n",
      "    positive       0.91      0.92      0.91       536\n",
      "\n",
      "   micro avg       0.88      0.88      0.88       794\n",
      "   macro avg       0.87      0.86      0.86       794\n",
      "weighted avg       0.88      0.88      0.88       794\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    min_df=2, \n",
    "    tokenizer=lambda text: [lemma for lemma in mystem.lemmatize(text) if not lemma.isspace()])\n",
    "\n",
    "predictions = evaluate_vectorizer(tfidf_vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f714742af2444a1ea1eabffd9c41d216",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Vectorizing train:', max=6929, style=ProgressStyle(descriptio…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'lemmatize_with_maru' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-4c804d53aba1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlemmatize_with_maru\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenize_with_razdel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m )\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_vectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfidf_vectorizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-33-1ff6535afa70>\u001b[0m in \u001b[0;36mevaluate_vectorizer\u001b[0;34m(vectorizer)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mevaluate_vectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Vectorizing train:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLinearSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1601\u001b[0m         \"\"\"\n\u001b[1;32m   1602\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1603\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1604\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1605\u001b[0m         \u001b[0;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1030\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m-> 1032\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m   1033\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m    940\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    943\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m    326\u001b[0m                                                tokenize)\n\u001b[1;32m    327\u001b[0m             return lambda doc: self._word_ngrams(\n\u001b[0;32m--> 328\u001b[0;31m                 tokenize(preprocess(self.decode(doc))), stop_words)\n\u001b[0m\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-44-4c804d53aba1>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      1\u001b[0m tfidf_vectorizer = TfidfVectorizer(\n\u001b[1;32m      2\u001b[0m     \u001b[0mmin_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlemmatize_with_maru\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenize_with_razdel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m )\n\u001b[1;32m      5\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_vectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfidf_vectorizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lemmatize_with_maru' is not defined"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    min_df=2, \n",
    "    tokenizer=lambda text: lemmatize_with_maru(tokenize_with_razdel(text)),\n",
    ")\n",
    "predictions = evaluate_vectorizer(tfidf_vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "def plot_confusion_matrix_heatmap(true, predicted):\n",
    "    classes = true.unique()\n",
    "    matrix = confusion_matrix(true, predicted, labels=classes)\n",
    "    sns.heatmap(matrix, xticklabels=classes, yticklabels=classes, annot=True, fmt='g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_confusion_matrix_heatmap(test.label, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Embeddings (word2vec и друзья)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding - это векторное представление слова.\n",
    "\n",
    "| Слово  |Вектор          |\n",
    "|--------|---------------|\n",
    "| Щенок  |[0.9, 1.0, 0.0]|\n",
    "| Пёс    |[1.0, 0.2, 0.0]|\n",
    "| Котёнок |[0.0, 1.0, 0.9]|\n",
    "| Кот    |[0.0, 0.2, 1.0]|\n",
    "\n",
    "Например, в таблице первая компонента вектора эмбеддинга отражает \"собачность\" слова, вторая отвечает за \"молодость\", а третья - за \"кошачность\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-30T07:49:31.925225Z",
     "start_time": "2019-01-30T07:49:31.920674Z"
    }
   },
   "source": [
    "### 4.1 W2V Embeddings\n",
    "Лингвист John Firth (1957):\n",
    "> \"You shall know a word by the company it keeps\" \n",
    "> (\"Скажи мне кто друг твоего слова и скажу что это за слово\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-30T10:19:33.038840Z",
     "start_time": "2019-01-30T10:19:33.034585Z"
    }
   },
   "source": [
    "[T.Mikolov et al, 2013](https://arxiv.org/abs/1301.3781)\n",
    "<img src=\"https://adriancolyer.files.wordpress.com/2016/04/word2vec-context-words.png?w=600\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как такие вектора будут расположены в пространстве? \n",
    "\n",
    "> Встреча президента России Путина и президента США Обамы состоялась вчера в Кремле.\n",
    "\n",
    "> Президент США, Барак Обама, вчера уехал из Вашингтона.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Интерактивно на projector.tensorflow](http://projector.tensorflow.org/?config=https://gist.githubusercontent.com/akutuzov/fd57a53a6aeec15c3497c54bc42a9af8/raw/c29e04ee34dc7ffad8d8bcccc8da2d5905259fcc/tayga_none_fasttextcbow_300_10_2019_b7b71a84a9796c369d8566d6c64d75ee_config.json)\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/fogside/fintech_dev/ekb/spring_2019/01_text_preprocessing_and_representation/w2v_pca.png?token=AgsinKF23DsHdd4YfY9qINJWH56E4PlTks5cZIWSwA%3D%3D\" width=\"900\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Cosine: cos(\\theta) = \\frac{A \\cdot B}{\\lVert A \\rVert \\cdot \\lVert B \\rVert}$$\n",
    "\n",
    "\n",
    "$$Euclidean: d(A, B) = \\sqrt{\\sum{(A - B)^2}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.stack.imgur.com/QlFJt.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Встречаем **Word2Vec**, модель, с помощью которой можно обучить такие эмбедденги. \n",
    "<img src=\"https://raw.githubusercontent.com/fogside/fintech_dev/ekb/spring_2019/01_text_preprocessing_and_representation/w2v_calc.png?token=AgsinCBMS0RHNVdbwKtOyE-JvfAsMF5Aks5cZIDhwA%3D%3D\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Есть 2 взаимно-обратные архитектуры Word2Vec: CBOW и Skip-Gram:   \n",
    "<img src=\"https://cdn-images-1.medium.com/max/2000/0*o2FCVrLKtdcxPQqc.png\" width=\"600\">\n",
    "\n",
    "\n",
    "**Quiz: Как можно представить вектор размерностью 300 в виде вектора размерности 3?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Skip-Gram model\n",
    "\n",
    "[Word2Vec Tutorial - The Skip-Gram Model](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/)\n",
    "\n",
    "[Word2Vec Tutorial Part 2 - Negative Sampling](http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/)\n",
    "\n",
    "Обучающая выборка:\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1600/0*0m03CiSHWl4f2dae.png\" width=\"600\">\n",
    "\n",
    "**Quiz: В каком виде подать слово на вход нейронной сети?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель:\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1600/0*FTfdlZ7yDBoQ8c9W.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Откуда же мы получаем эмбеддинг?\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/800/0*6DOQn6gxvEoix0yn.png\" width=\"500\">\n",
    "\n",
    "<img src=\"http://mccormickml.com/assets/word2vec/matrix_mult_w_one_hot.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для обучения своей модели word2vec удобно использовать библиотеку [`gensim`](https://radimrehurek.com/gensim/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. И снова классификация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем теперь написать свой векторизатор на основе `word2vec`. Чтобы получить вектор текста, будем брать word2vec-вектора отдельных слов, суммировать в один вектор и затем этот вектор нормализовать."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для начала скачаем word2vec модель, построенную по новостным заголовкам в рамках проекта [RusVectōrēs](https://rusvectores.org/ru/models/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import gzip\n",
    "import pathlib\n",
    "import urllib.request\n",
    "\n",
    "WORD2VEC_PATH = pathlib.Path('word2vec.bin')\n",
    "\n",
    "if not WORD2VEC_PATH.exists():\n",
    "    url = 'https://rusvectores.org/static/models/rusvectores2/news_mystem_skipgram_1000_20_2015.bin.gz'\n",
    "    with urllib.request.urlopen(url) as connection:\n",
    "        compressed = connection.read()\n",
    "            \n",
    "    decompressed = gzip.GzipFile(fileobj=io.BytesIO(compressed), mode='rb').read()\n",
    "    WORD2VEC_PATH.write_bytes(decompressed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы воспользоваться векторами из этой модели, также обратимся к библиотеке `gensim`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "word2vec = KeyedVectors.load_word2vec_format(WORD2VEC_PATH, binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Эта конкретная модель обучалась не на простых токенах, а на токенах и их частеречных тегах из MyStem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('обама_S', 0.4460664987564087),\n",
       " ('барак_S', 0.37829941511154175),\n",
       " ('вашингтонский_A', 0.3590691089630127),\n",
       " ('washington_UNKN', 0.35556167364120483),\n",
       " ('президент_S', 0.33835840225219727),\n",
       " ('союзник_S', 0.3376995921134949),\n",
       " ('заокеанский_A', 0.33447808027267456),\n",
       " ('госдеп_S', 0.333746999502182),\n",
       " ('сша_S', 0.3223333954811096),\n",
       " ('путинский_A', 0.31731104850769043)]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.most_similar(positive=['путин_S', 'вашингтон_S'], negative=['москва_S'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сначала напишем функцию, которая получает частеречный разбор из MyStem и возвращает токены вида `<слово>_<часть речи>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2vec_mystem = Mystem(entire_input=False)\n",
    "\n",
    "\n",
    "def tokenize_with_mystem_pos(text):\n",
    "    result = []\n",
    "    \n",
    "    for item in word2vec_mystem.analyze(text):\n",
    "        if item['analysis']:\n",
    "            lemma = item['analysis'][0]['lex']\n",
    "            pos = re.split('[=,]', item['analysis'][0]['gr'])[0]\n",
    "            token = f'{lemma}_{pos}'\n",
    "        else:\n",
    "            token = f'{item[\"text\"]}_UNKN'\n",
    "            \n",
    "        result.append(token)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Затем напишем класс, который по списку текстов возвращает вектора, полученные с помощью word2vec модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "\n",
    "class Word2VecVectorizer(TransformerMixin):\n",
    "    def __init__(self, vectors):\n",
    "        self.vectors = vectors\n",
    "        self.zeros = np.zeros(self.vectors.vector_size)\n",
    "        \n",
    "    def _get_text_vector(self, text):\n",
    "        token_vectors = []\n",
    "        for token in tokenize_with_mystem_pos(text):\n",
    "            try:\n",
    "                token_vectors.append(self.vectors[token])\n",
    "            except KeyError: # не нашли такой токен в словаре\n",
    "                pass\n",
    "                \n",
    "        if not token_vectors:\n",
    "            return self.zeros\n",
    "\n",
    "        text_vector = np.sum(token_vectors, axis=0)\n",
    "        return text_vector / np.linalg.norm(text_vector)\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        return np.array([self._get_text_vector(text) for text in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e474891b6b324b7db309681950eab87e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afc00dd24b1240af81acef706161986b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   negative       0.77      0.67      0.72       258\n",
      "   positive       0.85      0.90      0.88       536\n",
      "\n",
      "avg / total       0.83      0.83      0.83       794\n",
      "\n"
     ]
    }
   ],
   "source": [
    "word2vec_vectorizer = Word2VecVectorizer(word2vec)\n",
    "\n",
    "evaluate_vectorizer(word2vec_vectorizer);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tf-idf и word2vec представления текста имеет смысл комбинировать при обучении линейных моделей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64ae5a6062b545c2bc720bbe1d496b85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30349bdc4ac04a148cfbf495e880ce2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   negative       0.85      0.82      0.83       258\n",
      "   positive       0.91      0.93      0.92       536\n",
      "\n",
      "avg / total       0.89      0.89      0.89       794\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "evaluate_vectorizer(\n",
    "    FeatureUnion(\n",
    "        [\n",
    "            ('tf-idf', tfidf_vectorizer),\n",
    "            ('word2vec', word2vec_vectorizer),\n",
    "        ]\n",
    "    )\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~Никакого~~ большого прироста в качестве мы не получили, но можно заметить, что полнота по негативу и точность по позитиву слегка выросли."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
