{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Машинное обучение в диалоговых системах\n",
    "\n",
    "Рады вас видеть на курсе по разработке диалоговых систем от Тинькофф Финтех школы! \n",
    "\n",
    "В рамках курса мы рассмотрим, как **обработка естественного языка** и **машинное обучение** используются для построения чат-бот систем, от начала и до конца."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://media.giphy.com/media/aaZzZWqycDujC/giphy.gif\" width=\"400\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Эта лекция вводная, в ней мы рассмотрим:\n",
    "* предобработку текста\n",
    "* представление текста\n",
    "* понятие эмбеддинга\n",
    "* текстовую классификацию"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Предобработка текста\n",
    "\n",
    "Текст на естественном языке, который нужно обрабатывать в задачах машинного обучения, сильно зависит от источника. Пример:\n",
    "\n",
    "Википедия\n",
    "> Литературный язык — обработанная часть общенародного языка, обладающая в большей или меньшей степени письменно закреплёнными нормами; язык всех проявлений культуры, выражающихся в словесной форме.\n",
    "\n",
    "Твиттер\n",
    "> Если у вас в компании есть люди, которые целый день сидят в чатиках и смотрят видосики, то, скорее всего, это ДАТАСАЕНТИСТЫ и у них ОБУЧАЕТСЯ\n",
    "\n",
    "Ответы@Mail.ru\n",
    "> как пишется \"Вообщем лето было отличное\" раздельно или слитно слово ВОобщем?? ?\n",
    "\n",
    "В связи с этим, возникает задача предобработки (или нормализации) текста, то есть приведения к некоторому единому виду.\n",
    "\n",
    "**Quiz: Какие шаги/действия можно производить и что это даст?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "text = 'купил таблетки от тупости, но не смог открыть банку,ЧТО ДЕЛАТЬ???'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### 1.1 Приведение текста к нижнему регистру."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'купил таблетки от тупости, но не смог открыть банку,что делать???'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = text.lower()\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 Удаление неинформативных символов.\n",
    "\n",
    "Такими символами могут быть символы пунктуации, спец-символы, повторяющиеся символы, цифры. Для удаления подобных символов можно пользоваться стандартной библиотекой для [регулярных выражений](https://docs.python.org/3/library/re.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'купил таблетки от тупости, но не смог открыть банку,что делать?'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regex = re.compile(r'(\\W)\\1+')\n",
    "regex.sub(r'\\1', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'купил таблетки от тупости  но не смог открыть банку что делать'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regex = re.compile(r'[^\\w\\s]')\n",
    "regex.sub(r' ', text).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 Удаление неинформативных слов.\n",
    "\n",
    "Плохие слова:\n",
    "* Слишком частые\n",
    "<br> &nbsp; русский язык: и, но, я, ты, ...\n",
    "<br> &nbsp; английский язык: a, the, I, ...\n",
    "<br> &nbsp; специфичные для коллекции: \"сообщать\" в новостях\n",
    "* Слишком редкие\n",
    "* Стоп-слова\n",
    "<br> &nbsp; Предлоги, междометия, частицы, цифры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['these', 'an', 'such', 'did', 'whom', 'there']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "sw_eng = set(stopwords.words('english'))\n",
    "list(sw_eng)[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['над', 'три', 'вот', 'и', 'всю', 'тоже']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "sw_ru = set(stopwords.words('russian'))\n",
    "list(sw_ru)[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "До 111 слов\n",
      "После 60 слов\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Разница прежним теперешнем состоянием состояла том, прежде, забывал то, ним, то, говорили, он, страдальчески сморщив лоб, пытался мог разглядеть чего-то, далеко отстоящего него. Теперь также забывал то, говорили, то, ним; заметной, насмешливой, улыбкой самое, ним, вслушивался то, говорили, хотя очевидно видел слышал что-то другое... Теперь улыбка радости жизни постоянно играла рта, глазах светилось участие людям - вопрос: довольны же, он?'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = 'Разница между прежним и теперешнем его состоянием состояла в том, \\\n",
    "        что прежде, когда он забывал то, что было перед ним, то, что ему \\\n",
    "        говорили, он, страдальчески сморщив лоб, как будто пытался и не мог\\\n",
    "        разглядеть чего-то, далеко отстоящего от него. Теперь он также \\\n",
    "        забывал то, что ему говорили, и то, что было перед ним; но теперь \\\n",
    "        с чуть заметной, как будто насмешливой, улыбкой он в то самое, \\\n",
    "        что было перед ним, вслушивался в то, что ему говорили, хотя \\\n",
    "        очевидно видел и слышал что-то совсем другое...\\\n",
    "        Теперь улыбка радости жизни постоянно играла у его рта, и в глазах \\\n",
    "        его светилось участие к людям - вопрос: довольны ли они так же, как он?'\n",
    "clean_sent = ' '.join([word for word in sent.split() if not word in sw_ru])\n",
    "print('До {} слов'.format(len(sent.split())))\n",
    "print('После {} слов'.format(len(clean_sent.split())))\n",
    "clean_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sent = 'быть или не быть'\n",
    "clean_sent = ' '.join([word for word in sent.split() if not word in sw_ru])\n",
    "print(clean_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Разбиение текста на смысловые единицы (токенизация)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = 'Купите кружку-термос \"Hello Kitty\" на 0.5л (64см³) за 3 рубля. До 01.01.2050.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Самый простой подход к токенизации - это разбиение по текста по пробельным символам. \n",
    "\n",
    "**Quiz: Какая у этого подхода есть проблема?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Купите',\n",
       " 'кружку-термос',\n",
       " '\"Hello',\n",
       " 'Kitty\"',\n",
       " 'на',\n",
       " '0.5л',\n",
       " '(64см³)',\n",
       " 'за',\n",
       " '3',\n",
       " 'рубля.',\n",
       " 'До',\n",
       " '01.01.2050.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В библиотеке для морфологического анализа для русского языка [`pymorphy2`](https://pymorphy2.readthedocs.io/en/latest/) есть простая вспомогательная функция для токенизации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Купите',\n",
       " 'кружку-термос',\n",
       " '\"',\n",
       " 'Hello',\n",
       " 'Kitty',\n",
       " '\"',\n",
       " 'на',\n",
       " '0',\n",
       " '.',\n",
       " '5л',\n",
       " '(',\n",
       " '64см³',\n",
       " ')',\n",
       " 'за',\n",
       " '3',\n",
       " 'рубля',\n",
       " '.',\n",
       " 'До',\n",
       " '01',\n",
       " '.',\n",
       " '01',\n",
       " '.',\n",
       " '2050',\n",
       " '.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pymorphy2.tokenizers import simple_word_tokenize\n",
    "\n",
    "simple_word_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Более сложной метод токенизации представлен в [`nltk`](https://www.nltk.org/): библиотеке для общего NLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Купите',\n",
       " 'кружку-термос',\n",
       " '``',\n",
       " 'Hello',\n",
       " 'Kitty',\n",
       " \"''\",\n",
       " 'на',\n",
       " '0.5л',\n",
       " '(',\n",
       " '64см³',\n",
       " ')',\n",
       " 'за',\n",
       " '3',\n",
       " 'рубля',\n",
       " '.',\n",
       " 'До',\n",
       " '01.01.2050',\n",
       " '.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.word_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для русского языка также есть новая специализированная библиотека [`razdel`](https://github.com/natasha/razdel)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Купите',\n",
       " 'кружку-термос',\n",
       " '\"',\n",
       " 'Hello',\n",
       " 'Kitty',\n",
       " '\"',\n",
       " 'на',\n",
       " '0.5',\n",
       " 'л',\n",
       " '(',\n",
       " '64',\n",
       " 'см³',\n",
       " ')',\n",
       " 'за',\n",
       " '3',\n",
       " 'рубля',\n",
       " '.',\n",
       " 'До',\n",
       " '01.01.2050',\n",
       " '.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import razdel\n",
    "\n",
    "\n",
    "def tokenize_with_razdel(text):\n",
    "    return [token.text for token in razdel.tokenize(text)]\n",
    "\n",
    "\n",
    "tokenize_with_razdel(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Привет', ' Ты видел мр', 'Смита сегодня утром', '']\n"
     ]
    }
   ],
   "source": [
    "hard_string = 'Привет. Ты видел мр.Смита сегодня утром?'\n",
    "tmp_string = re.split(r'[!.?]', hard_string)\n",
    "print(tmp_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Привет.', 'Ты видел мр.Смита сегодня утром?']\n"
     ]
    }
   ],
   "source": [
    "hard_string = 'Привет. Ты видел мр.Смита сегодня утром?'\n",
    "exp = r'(?<!\\w\\.\\w.)(?<![А-Я][а-я]\\.)(?<=\\.|\\?)\\s'\n",
    "tmp_string = re.split(exp, hard_string)\n",
    "print(tmp_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Приведение слов к нормальной форме (стемминг, лемматизация)\n",
    "\n",
    "**Стемминг - это нормализация слова путём отбрасывания окончания по правилам языка.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Такая нормализация хорошо подходит для языков с небольшим разнообразием словоформ, например, для английского. В библиотеке [nltk](https://www.nltk.org/) есть несколько реализаций стеммеров:\n",
    " - [Porter stemmer](http://tartarus.org/martin/PorterStemmer/)\n",
    " - [Snowball stemmer](http://snowball.tartarus.org/)\n",
    " - [Lancaster stemmer](http://www.nltk.org/_modules/nltk/stem/lancaster.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'run'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "SnowballStemmer(language='english').stem('running')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'write wrote written'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "SnowballStemmer(language='english').stem('write wrote written')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для русского языка этот подход не очень подходит, поскольку в русском есть падежные формы, время у глаголов и т.д."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'бежа'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SnowballStemmer(language='russian').stem('бежать')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Лемматизация - приведение слов к начальной морфологической форме (с помощью словаря и грамматики языка).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Вспомогательная задача\n",
    "from nltk import wordnet, pos_tag\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    my_switch = {\n",
    "        'J': wordnet.wordnet.ADJ,\n",
    "        'V': wordnet.wordnet.VERB,\n",
    "        'N': wordnet.wordnet.NOUN,\n",
    "        'R': wordnet.wordnet.ADV,\n",
    "    }\n",
    "    for key, item in my_switch.items():\n",
    "        if treebank_tag.startswith(key):\n",
    "            return item\n",
    "    return wordnet.wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('George', 'NNP'), ('admitted', 'VBD'), ('the', 'DT'), ('talks', 'NNS'), ('happened', 'VBD')]\n"
     ]
    }
   ],
   "source": [
    "sent = 'George admitted the talks happened'.split()\n",
    "pos_tagged = pos_tag(sent)\n",
    "print(pos_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import WordNetLemmatizer\n",
    "def my_lemmatizer(sent):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokenized_sent = sent.split()\n",
    "    pos_tagged = [(word, get_wordnet_pos(tag))\n",
    "                 for word, tag in pos_tag(tokenized_sent)]\n",
    "    return ' '.join([lemmatizer.lemmatize(word, tag)\n",
    "                    for word, tag in pos_tagged])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "write write write\n"
     ]
    }
   ],
   "source": [
    "sent = 'write wrote written'\n",
    "print(my_lemmatizer(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для лемматизации русских слов есть несколько библиотек в свободном доступе:\n",
    "- [pymorphy2](https://pymorphy2.readthedocs.io/en/latest/)\n",
    "- [mystem3](https://tech.yandex.ru/mystem/)\n",
    "- [maru](https://github.com/chomechome/maru)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Самый простой подход к лемматизации - словарный. Здесь не учитывается контекст слова, поэтому для омонимов такой подход работает не всегда. Такой подход применяет библиотека `pymorphy2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pymorphy2 import MorphAnalyzer\n",
    "\n",
    "pymorphy = MorphAnalyzer()\n",
    "\n",
    "\n",
    "def lemmatize_with_pymorphy(tokens):\n",
    "    return [pymorphy.parse(token)[0].normal_form for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['мама', 'мыло', 'рам']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatize_with_pymorphy(['мама', 'мыла', 'раму'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Библиотека от Яндекса `mystem3` обходит это ограничение и рассматривает контекст слова, используя статистику и правила."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem\n",
    "\n",
    "mystem = Mystem()\n",
    "\n",
    "\n",
    "def lemmatize_with_mystem(text):\n",
    "    return [lemma for lemma in mystem.lemmatize(text) if not lemma.isspace()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['мама', 'мыть', 'рама']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatize_with_mystem('мама мыла раму')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но на более сложных примерах такой подход тоже может сойтись к самому частотному варианту."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['на', 'завод', 'становиться', 'увидеть', 'вид', 'становиться']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatize_with_mystem('на заводе стали увидел виды стали')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Библиотека `maru` использует машинное обучение и нейросети для разрешения омонимии."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import maru\n",
    "\n",
    "maru_rnn = maru.get_analyzer('rnn')\n",
    "\n",
    "\n",
    "def lemmatize_with_maru(tokens):\n",
    "    return [morph.lemma for morph in maru_rnn.analyze(tokens)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['на', 'завод', 'сталь', 'увидеть', 'вид', 'сталь']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatize_with_maru(['на', 'заводе', 'стали', 'увидел', 'виды', 'стали'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Представление текста\n",
    "\n",
    "**Quiz: Как можно использовать токенизированные тексты в задачах NLP? Какие варианты представления текста можете назвать?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 One-Hot Encoding\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1200/1*YEJf9BQQh0ma1ECs6x_7yQ.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz: Что такое разреженная матрица?**\n",
    "\n",
    "**Quiz: Какие есть плюсы и минусы у one-hot представления?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как получить one-hot? \n",
    "\n",
    "Сначала нам нужно каждому слову поставить в соответствие номер, а затем перевести их в бинарные вектора. \n",
    "\n",
    "Используем библиотеку [`scikit-learn`](https://scikit-learn.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-30T11:44:20.487638Z",
     "start_time": "2019-01-30T11:44:20.482337Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 1])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "words = ['NLP', 'is', 'awesome']\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "corpus_encoded = label_encoder.fit_transform(words)\n",
    "corpus_encoded # какой вывод?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['NLP', 'awesome', 'is'], dtype='<U7')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_encoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "onehot_encoder.fit_transform(corpus_encoded.reshape(-1, 1)) # какой вывод?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В библиотеке для построения нейросетей [`keras`](https://keras.io/) есть более удобная функция для такого кодирования."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras.utils\n",
    "\n",
    "keras.utils.to_categorical(corpus_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Bag-of-words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-30T12:05:25.824178Z",
     "start_time": "2019-01-30T12:05:25.820140Z"
    }
   },
   "source": [
    "**BoW** - \"мешок слов\". \n",
    "\n",
    "**Quiz: Что будет, если мы сложим все one-hot вектора слов в тексте?**\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://slideplayer.com/slide/7073400/24/images/15/The+Bag+of+Words+Representation.jpg)\n",
    "Посчитаем количество слов в текстах"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-30T11:51:36.904566Z",
     "start_time": "2019-01-30T11:51:36.901906Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    'Дочь бьет Марго',\n",
    "    'Тот кто бьет, не знает кто бьет его',\n",
    "    'Кто кого бьет?',\n",
    "    'Марго бьет дочь?',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-30T11:51:44.357709Z",
     "start_time": "2019-01-30T11:51:44.350257Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1, 1, 0, 0, 0, 0, 1, 0, 0],\n",
       "        [2, 0, 1, 1, 0, 2, 0, 1, 1],\n",
       "        [1, 0, 0, 0, 1, 1, 0, 0, 0],\n",
       "        [1, 1, 0, 0, 0, 0, 1, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit_transform(corpus).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'бьет': 0,\n",
       " 'дочь': 1,\n",
       " 'его': 2,\n",
       " 'знает': 3,\n",
       " 'кого': 4,\n",
       " 'кто': 5,\n",
       " 'марго': 6,\n",
       " 'не': 7,\n",
       " 'тот': 8}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz: Какие минусы у такого представления?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 TF-IDF\n",
    "\n",
    "**Term Frequency**  $tf(w,d)$ - сколько раз слово $w$ встретилось в документе $d$\n",
    "\n",
    "**Document Frequency** $df(w)$ - сколько документов содержат слово $w$\n",
    "\n",
    "**Inverse Document Frequency** $idf(w) = log_2(N/df(w))$  — обратная документная частотность. \n",
    "\n",
    "**TF-IDF**=$tf(w,d)*idf(w)$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.42389674, 0.64043405, 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.64043405, 0.        , 0.        ],\n",
       "        [0.37919167, 0.        , 0.36332075, 0.36332075, 0.        ,\n",
       "         0.5728925 , 0.        , 0.36332075, 0.36332075],\n",
       "        [0.37919167, 0.        , 0.        , 0.        , 0.72664149,\n",
       "         0.5728925 , 0.        , 0.        , 0.        ],\n",
       "        [0.42389674, 0.64043405, 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.64043405, 0.        , 0.        ]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "idf_vectorizer = TfidfVectorizer()\n",
    "idf_vectorizer.fit_transform(corpus).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'бьет': 0,\n",
       " 'дочь': 1,\n",
       " 'его': 2,\n",
       " 'знает': 3,\n",
       " 'кого': 4,\n",
       " 'кто': 5,\n",
       " 'марго': 6,\n",
       " 'не': 7,\n",
       " 'тот': 8}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Классификация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz: Что это за задача? Какие ещё задачи решаются в машинном обучении?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы попробуем применить описание методы предобработки и представления текста на примере анализа тональности текста. В качестве данных будем использовать небольшой датасет твитов. Всего в данных 2 класса: позитив и негатив."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Загрузка тренировочных и тестовых данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz: Зачем нам разделять данные на тренировочные и тестовые?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим тренировочные и тестовые данные при помощи библиотеки [`pandas`](https://pandas.pydata.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6929, 2)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv('train.csv')\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>positive</td>\n",
       "      <td>эти розы для прекрасной мамочки)))=_=]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>negative</td>\n",
       "      <td>И да, у меня в этом году серьезные проблемы со...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>positive</td>\n",
       "      <td>♥Обожаю людей, которые заставляют меня смеятьс...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>Вчера нашла в почтовом ящике пустую упаковку и...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>positive</td>\n",
       "      <td>очень долгожданный и хороший день был)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                               text\n",
       "0  positive            эти розы для прекрасной мамочки)))=_=]]\n",
       "1  negative  И да, у меня в этом году серьезные проблемы со...\n",
       "2  positive  ♥Обожаю людей, которые заставляют меня смеятьс...\n",
       "3  negative  Вчера нашла в почтовом ящике пустую упаковку и...\n",
       "4  positive             очень долгожданный и хороший день был)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive    4635\n",
       "negative    2294\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(794, 2)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv('test.csv')\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>positive</td>\n",
       "      <td>#ахахах ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>negative</td>\n",
       "      <td>я очень устал в этом году. очень.  расскажите,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negative</td>\n",
       "      <td>НЕ ТОРОПИТЕСЬ ЖИТЬ!!! сегодня на моих глазах о...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>positive</td>\n",
       "      <td>Когда я его обнимаю, я закрываю глаза, уткнувш...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>меня надо усыпить О_О</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                               text\n",
       "0  positive                                        #ахахах ...\n",
       "1  negative  я очень устал в этом году. очень.  расскажите,...\n",
       "2  negative  НЕ ТОРОПИТЕСЬ ЖИТЬ!!! сегодня на моих глазах о...\n",
       "3  positive  Когда я его обнимаю, я закрываю глаза, уткнувш...\n",
       "4  negative                              меня надо усыпить О_О"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive    536\n",
       "negative    258\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Оценка качества"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz: Как оценить качество модели в задаче классификации?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<table style=\"width:40%\">\n",
    "  <tr>\n",
    "    <th></th>\n",
    "    <th> y = 1 </th> \n",
    "    <th> y = 0 </th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th> a(x) = 1 </th>\n",
    "    <td> True Positive (TP) </td> \n",
    "    <td> False Positive (FP) </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th> a(x) = 0 </th>\n",
    "    <td> False Negative (FN) </td> \n",
    "    <td> True Negative (TN) </td>\n",
    "  </tr>\n",
    "</table>\n",
    "<br>\n",
    "$Accuracy = \\frac{TP + TN}{TP + TN + FP + FN}$\n",
    "<br><br>\n",
    "$Precision = \\frac{TP}{TP + FP}$\n",
    "<br><br>\n",
    "$Recall = \\frac{TP}{TP + FN}$\n",
    "<br><br>\n",
    "$F1 = 2 * \\frac{Precision * Recall}{Precision + Recall}$\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Построение модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем функцию для оценки векторизатора. В качестве модели будем использовать линейный SVM, он хорошо работает для определения тональности.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz: Какая модель машинного обучения называется линейной? Почему мы используем линейную модель в задаче?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import tqdm\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "def evaluate_vectorizer(vectorizer):\n",
    "    X = vectorizer.fit_transform(tqdm.tqdm_notebook(train.text, desc='Vectorizing train:'))\n",
    "\n",
    "    model = LinearSVC(random_state=42)\n",
    "    model.fit(X, train.label)\n",
    "    \n",
    "    X_test = vectorizer.transform(tqdm.tqdm_notebook(test.text, desc='Vectorizing test:'))\n",
    "    predictions = model.predict(X_test)\n",
    "    \n",
    "    print(classification_report(test.label, predictions))\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Сравнение способов представления текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6acc59fc93004e7a88467787f83cd467",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19e1722b9ca74cd3a7ec2cf6482d6179",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   negative       0.73      0.62      0.67       258\n",
      "   positive       0.83      0.89      0.86       536\n",
      "\n",
      "avg / total       0.80      0.80      0.80       794\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_vectorizer(CountVectorizer(min_df=2));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d3ef3af00d84a74a50ca75e4117c528",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "733e58c7e8fd42eeba687c6e960451bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   negative       0.76      0.65      0.70       258\n",
      "   positive       0.84      0.90      0.87       536\n",
      "\n",
      "avg / total       0.82      0.82      0.82       794\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_vectorizer(TfidfVectorizer(min_df=2));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15fdb4ab06b742aea79ed1c3d8d52eff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abbb4783829340ba8483f898074e6ba1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   negative       0.78      0.76      0.77       258\n",
      "   positive       0.89      0.90      0.89       536\n",
      "\n",
      "avg / total       0.85      0.85      0.85       794\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_vectorizer(TfidfVectorizer(min_df=2, tokenizer=tokenize_with_razdel));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac0ecf1545fa436ab74d600ad92f444f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e902eeeab6794b9eb88b9e282abe5483",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   negative       0.85      0.79      0.82       258\n",
      "   positive       0.90      0.93      0.92       536\n",
      "\n",
      "avg / total       0.89      0.89      0.89       794\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    min_df=2, \n",
    "    tokenizer=lambda text: lemmatize_with_maru(tokenize_with_razdel(text)),\n",
    ")\n",
    "predictions = evaluate_vectorizer(tfidf_vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "def plot_confusion_matrix_heatmap(true, predicted):\n",
    "    classes = true.unique()\n",
    "    matrix = confusion_matrix(true, predicted, labels=classes)\n",
    "    sns.heatmap(matrix, xticklabels=classes, yticklabels=classes, annot=True, fmt='g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWAAAAD8CAYAAABJsn7AAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd717ff1208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_confusion_matrix_heatmap(test.label, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Embeddings (word2vec и друзья)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding - это векторное представление слова.\n",
    "\n",
    "| Слово  |Вектор          |\n",
    "|--------|---------------|\n",
    "| Щенок  |[0.9, 1.0, 0.0]|\n",
    "| Пёс    |[1.0, 0.2, 0.0]|\n",
    "| Котёнок |[0.0, 1.0, 0.9]|\n",
    "| Кот    |[0.0, 0.2, 1.0]|\n",
    "\n",
    "Например, в таблице первая компонента вектора эмбеддинга отражает \"собачность\" слова, вторая отвечает за \"молодость\", а третья - за \"кошачность\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-30T07:49:31.925225Z",
     "start_time": "2019-01-30T07:49:31.920674Z"
    }
   },
   "source": [
    "### 4.1 W2V Embeddings\n",
    "Лингвист John Firth (1957):\n",
    "> \"You shall know a word by the company it keeps\" \n",
    "> (\"Скажи мне кто друг твоего слова и скажу что это за слово\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-30T10:19:33.038840Z",
     "start_time": "2019-01-30T10:19:33.034585Z"
    }
   },
   "source": [
    "[T.Mikolov et al, 2013](https://arxiv.org/abs/1301.3781)\n",
    "<img src=\"https://adriancolyer.files.wordpress.com/2016/04/word2vec-context-words.png?w=600\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как такие вектора будут расположены в пространстве? \n",
    "\n",
    "> Встреча президента России Путина и президента США Обамы состоялась вчера в Кремле.\n",
    "\n",
    "> Президент США, Барак Обама, вчера уехал из Вашингтона.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Интерактивно на projector.tensorflow](http://projector.tensorflow.org/?config=https://gist.githubusercontent.com/akutuzov/fd57a53a6aeec15c3497c54bc42a9af8/raw/c29e04ee34dc7ffad8d8bcccc8da2d5905259fcc/tayga_none_fasttextcbow_300_10_2019_b7b71a84a9796c369d8566d6c64d75ee_config.json)\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/fogside/fintech_dev/ekb/spring_2019/01_text_preprocessing_and_representation/w2v_pca.png?token=AgsinKF23DsHdd4YfY9qINJWH56E4PlTks5cZIWSwA%3D%3D\" width=\"900\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Cosine: cos(\\theta) = \\frac{A \\cdot B}{\\lVert A \\rVert \\cdot \\lVert B \\rVert}$$\n",
    "\n",
    "\n",
    "$$Euclidean: d(A, B) = \\sqrt{\\sum{(A - B)^2}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.stack.imgur.com/QlFJt.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Встречаем **Word2Vec**, модель, с помощью которой можно обучить такие эмбедденги. \n",
    "<img src=\"https://raw.githubusercontent.com/fogside/fintech_dev/ekb/spring_2019/01_text_preprocessing_and_representation/w2v_calc.png?token=AgsinCBMS0RHNVdbwKtOyE-JvfAsMF5Aks5cZIDhwA%3D%3D\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Есть 2 взаимно-обратные архитектуры Word2Vec: CBOW и Skip-Gram:   \n",
    "<img src=\"https://cdn-images-1.medium.com/max/2000/0*o2FCVrLKtdcxPQqc.png\" width=\"600\">\n",
    "\n",
    "\n",
    "**Quiz: Как можно представить вектор размерностью 300 в виде вектора размерности 3?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Skip-Gram model\n",
    "\n",
    "[Word2Vec Tutorial - The Skip-Gram Model](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/)\n",
    "\n",
    "[Word2Vec Tutorial Part 2 - Negative Sampling](http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/)\n",
    "\n",
    "Обучающая выборка:\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1600/0*0m03CiSHWl4f2dae.png\" width=\"600\">\n",
    "\n",
    "**Quiz: В каком виде подать слово на вход нейронной сети?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель:\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1600/0*FTfdlZ7yDBoQ8c9W.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Откуда же мы получаем эмбеддинг?\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/800/0*6DOQn6gxvEoix0yn.png\" width=\"500\">\n",
    "\n",
    "<img src=\"http://mccormickml.com/assets/word2vec/matrix_mult_w_one_hot.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для обучения своей модели word2vec удобно использовать библиотеку [`gensim`](https://radimrehurek.com/gensim/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. И снова классификация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем теперь написать свой векторизатор на основе `word2vec`. Чтобы получить вектор текста, будем брать word2vec-вектора отдельных слов, суммировать в один вектор и затем этот вектор нормализовать."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для начала скачаем word2vec модель, построенную по новостным заголовкам в рамках проекта [RusVectōrēs](https://rusvectores.org/ru/models/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import gzip\n",
    "import pathlib\n",
    "import urllib.request\n",
    "\n",
    "WORD2VEC_PATH = pathlib.Path('word2vec.bin')\n",
    "\n",
    "if not WORD2VEC_PATH.exists():\n",
    "    url = 'https://rusvectores.org/static/models/rusvectores2/news_mystem_skipgram_1000_20_2015.bin.gz'\n",
    "    with urllib.request.urlopen(url) as connection:\n",
    "        compressed = connection.read()\n",
    "            \n",
    "    decompressed = gzip.GzipFile(fileobj=io.BytesIO(compressed), mode='rb').read()\n",
    "    WORD2VEC_PATH.write_bytes(decompressed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы воспользоваться векторами из этой модели, также обратимся к библиотеке `gensim`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "word2vec = KeyedVectors.load_word2vec_format(WORD2VEC_PATH, binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Эта конкретная модель обучалась не на простых токенах, а на токенах и их частеречных тегах из MyStem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('обама_S', 0.4460664987564087),\n",
       " ('барак_S', 0.37829941511154175),\n",
       " ('вашингтонский_A', 0.3590691089630127),\n",
       " ('washington_UNKN', 0.35556167364120483),\n",
       " ('президент_S', 0.33835840225219727),\n",
       " ('союзник_S', 0.3376995921134949),\n",
       " ('заокеанский_A', 0.33447808027267456),\n",
       " ('госдеп_S', 0.333746999502182),\n",
       " ('сша_S', 0.3223333954811096),\n",
       " ('путинский_A', 0.31731104850769043)]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.most_similar(positive=['путин_S', 'вашингтон_S'], negative=['москва_S'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сначала напишем функцию, которая получает частеречный разбор из MyStem и возвращает токены вида `<слово>_<часть речи>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2vec_mystem = Mystem(entire_input=False)\n",
    "\n",
    "\n",
    "def tokenize_with_mystem_pos(text):\n",
    "    result = []\n",
    "    \n",
    "    for item in word2vec_mystem.analyze(text):\n",
    "        if item['analysis']:\n",
    "            lemma = item['analysis'][0]['lex']\n",
    "            pos = re.split('[=,]', item['analysis'][0]['gr'])[0]\n",
    "            token = f'{lemma}_{pos}'\n",
    "        else:\n",
    "            token = f'{item[\"text\"]}_UNKN'\n",
    "            \n",
    "        result.append(token)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Затем напишем класс, который по списку текстов возвращает вектора, полученные с помощью word2vec модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "\n",
    "class Word2VecVectorizer(TransformerMixin):\n",
    "    def __init__(self, vectors):\n",
    "        self.vectors = vectors\n",
    "        self.zeros = np.zeros(self.vectors.vector_size)\n",
    "        \n",
    "    def _get_text_vector(self, text):\n",
    "        token_vectors = []\n",
    "        for token in tokenize_with_mystem_pos(text):\n",
    "            try:\n",
    "                token_vectors.append(self.vectors[token])\n",
    "            except KeyError: # не нашли такой токен в словаре\n",
    "                pass\n",
    "                \n",
    "        if not token_vectors:\n",
    "            return self.zeros\n",
    "\n",
    "        text_vector = np.sum(token_vectors, axis=0)\n",
    "        return text_vector / np.linalg.norm(text_vector)\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        return np.array([self._get_text_vector(text) for text in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e474891b6b324b7db309681950eab87e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afc00dd24b1240af81acef706161986b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   negative       0.77      0.67      0.72       258\n",
      "   positive       0.85      0.90      0.88       536\n",
      "\n",
      "avg / total       0.83      0.83      0.83       794\n",
      "\n"
     ]
    }
   ],
   "source": [
    "word2vec_vectorizer = Word2VecVectorizer(word2vec)\n",
    "\n",
    "evaluate_vectorizer(word2vec_vectorizer);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tf-idf и word2vec представления текста имеет смысл комбинировать при обучении линейных моделей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64ae5a6062b545c2bc720bbe1d496b85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30349bdc4ac04a148cfbf495e880ce2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   negative       0.85      0.82      0.83       258\n",
      "   positive       0.91      0.93      0.92       536\n",
      "\n",
      "avg / total       0.89      0.89      0.89       794\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "evaluate_vectorizer(\n",
    "    FeatureUnion(\n",
    "        [\n",
    "            ('tf-idf', tfidf_vectorizer),\n",
    "            ('word2vec', word2vec_vectorizer),\n",
    "        ]\n",
    "    )\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~Никакого~~ большого прироста в качестве мы не получили, но можно заметить, что полнота по негативу и точность по позитиву слегка выросли."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
