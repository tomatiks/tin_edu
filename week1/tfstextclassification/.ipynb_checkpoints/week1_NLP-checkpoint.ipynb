{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "TqdmDeprecationWarning: Please use `tqdm.pandas(...)` instead of `tqdm_pandas(tqdm, ...)`.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import cufflinks as cf\n",
    "\n",
    "from scipy.sparse import hstack\n",
    "from scipy.cluster import hierarchy\n",
    "from scipy.spatial.distance import pdist\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from sklearn.model_selection import train_test_split,TimeSeriesSplit\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "import re\n",
    "import razdel\n",
    "from pymystem3 import Mystem\n",
    "import tqdm\n",
    "tqdm.tqdm_pandas(tqdm.tqdm, leave=True)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, SpectralClustering, AffinityPropagation,DBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.metrics import silhouette_score,davies_bouldin_score\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LogisticRegressionCV,LogisticRegression\n",
    "\n",
    "from sklearn.metrics import accuracy_score,roc_auc_score, f1_score,roc_curve,auc\n",
    "\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('train.csv', encoding='utf-8',sep=',')\n",
    "df_test = pd.read_csv('test.csv', encoding='utf-8',sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 112467 entries, 0 to 112466\n",
      "Data columns (total 3 columns):\n",
      "id       112467 non-null int64\n",
      "text     112467 non-null object\n",
      "label    112467 non-null int64\n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 2.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    50000\n",
       "2    40851\n",
       "0    21616\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "sw_ru = set(stopwords.words('russian'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['have_number'] = df_train['text'].apply(lambda x: 1 if re.search(r'\\d{10}',x) else 0)\n",
    "df_train['have_eng'] = df_train['text'].apply(lambda x: 1 if re.search(r'[a-zA-Z]+',x) else 0)\n",
    "df_train['have_symbols'] = df_train['text'].apply(lambda x: 1 if re.search(r'[*\\'\\\"=]+',x) else 0)\n",
    "df_train['have_upper'] = df_train['text'].apply(lambda x: 1 if re.search(r'[А-Я][А-Я]+',x) else 0)\n",
    "\n",
    "## is_upper для знакомств\n",
    "## сокращения для знакомств"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['have_number'] = df_test['text'].apply(lambda x: 1 if re.search(r'\\d{10}',x) else 0)\n",
    "df_test['have_eng'] = df_test['text'].apply(lambda x: 1 if re.search(r'[a-zA-Z]+',x) else 0)\n",
    "df_test['have_symbols'] = df_test['text'].apply(lambda x: 1 if re.search(r'[*\\'\\\"=]+',x) else 0)\n",
    "df_test['have_upper'] = df_test['text'].apply(lambda x: 1 if re.search(r'[А-Я][А-Я]+',x) else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "mystem = Mystem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "upos_map = {'A':'ADJ','ADV':'ADV','ADVPRO':'ADV','ANUM':'ADJ','APRO':'DET','COM':'ADJ','CONJ':'SCON','INTJ':'INTJ','NONLEX':'X','NUM':'NUM','PART':'PART','PR':'ADP','S':'NOUN','SPRO':'PRON','UNKN':'X' ,'V':'VERB'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_prep(text):\n",
    "    \n",
    "    text = text.lower()\n",
    "    result = []\n",
    "    # Убираем лишние символы\n",
    "    #text = re.sub(r'[;,]',r' ',text).strip()\n",
    "    text = re.sub(r'[^\\w\\s\\.]',r'',text).strip()   \n",
    "    #text = [token.text for token in razdel.tokenize(text)]\n",
    "    # Делаем лемматизацию       \n",
    "    result = [lemma for lemma in mystem.lemmatize(text) if not lemma.isspace() #and lemma not in sw_ru\n",
    "             and lemma.strip() not in ['.','..','...']]\n",
    "    \n",
    "\n",
    "    # Лемматизатор неправильно разбивает слова с дефисом, поэтому исправляем это\n",
    "#     if '-' in text:\n",
    "#         for l in range(len(text)):\n",
    "#             if text[l] == '-':\n",
    "#                 text[l] = f'{text[l-1]}-{text[l+1]}'\n",
    "#                 text[l-1] = text[l+1] = text[l]\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_prep_tags(text):\n",
    "    \n",
    "    text = text.lower()\n",
    "    result = []\n",
    "    # Убираем лишние символы\n",
    "    #text = re.sub(r'[;,]',r' ',text).strip()\n",
    "    text = re.sub(r'[^\\w\\s\\.]',r'',text).strip()   \n",
    "    #text = [token.text for token in razdel.tokenize(text)]\n",
    "    # Делаем лемматизацию       \n",
    "#     text = [lemma for lemma in mystem.lemmatize(text) if not lemma.isspace() and lemma not in sw_ru\n",
    "#             and lemma.strip() not in ['.','..','...']]\n",
    "    \n",
    "    \n",
    "    for item in mystem.analyze(text):\n",
    "      #  print(item)\n",
    "        token = None\n",
    "        if item.get('analysis'):\n",
    "            lemma = item['analysis'][0]['lex']\n",
    "            pos = re.split('[=,]', item['analysis'][0]['gr'])[0]\n",
    "            #and lemma not in sw_ru\n",
    "            if not lemma.isspace()  and lemma.strip() not in ['.','..','...']: #and lemma not in sw_ru\n",
    "     \n",
    "                token = f'{lemma}_{upos_map[pos]}'\n",
    "        else:\n",
    "            lem_text = item[\"text\"]\n",
    "            if not lem_text.isspace() and lem_text.strip() not in ['.','..','...']: #and lem_text not in sw_ru\n",
    "            \n",
    "                token = f'{lem_text}_UNKN'\n",
    "            \n",
    "        if token:    \n",
    "            result.append(token)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Лемматизатор неправильно разбивает слова с дефисом, поэтому исправляем это\n",
    "#     if '-' in text:\n",
    "#         for l in range(len(text)):\n",
    "#             if text[l] == '-':\n",
    "#                 text[l] = f'{text[l-1]}-{text[l+1]}'\n",
    "#                 text[l-1] = text[l+1] = text[l]\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# применяем функцию к признаку\n",
    "\n",
    "#df_train['prep_text'] = df_train['text'].progress_apply(text_prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                 [44, позн, мужч, 40, год, 0678935919]\n",
       "1                             [вспомнить, это, увидеть]\n",
       "2                             [sep, decimal, изза, это]\n",
       "3                    [нужный, дева, 19л, 0637048428смс]\n",
       "4     [м54, искать, д, сп, ласка, некм, 0508665030, ...\n",
       "5                [говорить, спать, постоянно, хотеться]\n",
       "6      [познакомиться, адекватный, мужчина, 0668847450]\n",
       "7     [модер, доченька, звать, арина, папа, дима, ма...\n",
       "8                                           [это, попс]\n",
       "9                 [пар, пообщий, дева, смс, 0935695491]\n",
       "10           [смотреть, паблик, смотреть, прочий, фолд]\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.loc[:10]['text'].apply(text_prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['говорить_VERB',\n",
       " 'спать_VERB',\n",
       " 'постоянно_ADV',\n",
       " 'хотеться_VERB',\n",
       " 'мама_NOUN',\n",
       " 'приятель_NOUN']"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_prep_tags('Мне говорили от них спать постоянно хочется мама приятель')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>have_number</th>\n",
       "      <th>have_eng</th>\n",
       "      <th>have_symbols</th>\n",
       "      <th>have_upper</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>ж 44 позн с мужч от 40 лет 0678935919</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>уже и не вспомню где это увидел</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>, sep=';', decimal=',' может изза этого</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>НУЖНА ДЕВ ДЛЯ С\\\\О -19л.0637048428смс</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>.М54.ИЩУ Д.Ж.  С-П.ДЛЯ ЛАСК...НЕКМ.0508665030....</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>Мне говорили от них спать постоянно хочется</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>ПОЗНАКОМЛЮСЬ С АДЕКВАТНЫМ МУЖЧИНОЙ С/О 0668847450</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>модер ДОЧЕНЬКУ ЗОВУТ АРИНА ПАПА ДИМА МАМА НАСТ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>Но это не попса, не надо</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>пар. Пообщ. С дев. По смс. 0935695491</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                               text  label  have_number  \\\n",
       "0   0              ж 44 позн с мужч от 40 лет 0678935919      1            1   \n",
       "1   1                    уже и не вспомню где это увидел      2            0   \n",
       "2   2            , sep=';', decimal=',' может изза этого      2            0   \n",
       "3   3              НУЖНА ДЕВ ДЛЯ С\\\\О -19л.0637048428смс      1            1   \n",
       "4   4  .М54.ИЩУ Д.Ж.  С-П.ДЛЯ ЛАСК...НЕКМ.0508665030....      1            1   \n",
       "5   5        Мне говорили от них спать постоянно хочется      2            0   \n",
       "6   6  ПОЗНАКОМЛЮСЬ С АДЕКВАТНЫМ МУЖЧИНОЙ С/О 0668847450      1            1   \n",
       "7   7  модер ДОЧЕНЬКУ ЗОВУТ АРИНА ПАПА ДИМА МАМА НАСТ...      1            0   \n",
       "8   8                           Но это не попса, не надо      2            0   \n",
       "9   9              пар. Пообщ. С дев. По смс. 0935695491      1            1   \n",
       "\n",
       "   have_eng  have_symbols  have_upper  \n",
       "0         0             0           0  \n",
       "1         0             0           0  \n",
       "2         1             1           0  \n",
       "3         0             0           1  \n",
       "4         0             0           1  \n",
       "5         0             0           0  \n",
       "6         0             0           1  \n",
       "7         0             0           1  \n",
       "8         0             0           0  \n",
       "9         0             0           0  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.concat([df_train['text'],df_test['text']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Преобразуем нашу информацию в вектора методом tf-idf\n",
    "idf_vectorizer = TfidfVectorizer(min_df = 2,tokenizer=text_prep)\n",
    "\n",
    "#idf_vectorizer.fit(pd.concat([df_train['text'],df_test['text']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#idf_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(df_train,df_train['label'],test_size=0.2,random_state=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0b7817eb56b4137afcdd4e776d1c66c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=89973), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d8c2d6236c6428c859c5e02fffd6ad8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=22494), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "X_train_idf_vector = idf_vectorizer.transform(tqdm.tqdm_notebook(X_train['text']))\n",
    "X_test_idf_vector = idf_vectorizer.transform(tqdm.tqdm_notebook(X_test['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final prediction\n",
    "fin_train_idf_vector = idf_vectorizer.transform(tqdm.tqdm_notebook(df_train['text']))\n",
    "fin_test_idf_vector = idf_vectorizer.transform(tqdm.tqdm_notebook(df_test['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<112467x57519 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 773992 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fin_train_idf_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = KeyedVectors.load_word2vec_format('tayga-func_upos_skipgram_300_5_2019.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word2vec.most_similar(positive=['обама::барак_NOUN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word2vec['парень_NOUN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "\n",
    "class Word2VecVectorizer(TransformerMixin):\n",
    "    def __init__(self, vectors):\n",
    "        self.vectors = vectors\n",
    "        self.zeros = np.zeros(self.vectors.vector_size)\n",
    "        \n",
    "    def _get_text_vector(self, text):\n",
    "        token_vectors = []\n",
    "        for token in text_prep_tags(text):\n",
    "            try:\n",
    "                token_vectors.append(self.vectors[token])\n",
    "            except KeyError: # не нашли такой токен в словаре\n",
    "                pass\n",
    "                \n",
    "        if not token_vectors:\n",
    "            return self.zeros\n",
    "\n",
    "        text_vector = np.sum(token_vectors, axis=0)\n",
    "        return text_vector / np.linalg.norm(text_vector)\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        return np.array([self._get_text_vector(text) for text in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_vectorizer = Word2VecVectorizer(word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84be89a6c7af49389bc94dd90b70d634",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=89973), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "968e2f53a8744049994114902087baa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=22494), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "X_train_w2v_vector = word2vec_vectorizer.fit_transform(tqdm.tqdm_notebook(X_train['text']))\n",
    "X_test_w2v_vector = word2vec_vectorizer.fit_transform(tqdm.tqdm_notebook(X_test['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64390                         [парень_S, девушка_S, лайф_S]\n",
       "108881    [задавать_V, early_UNKN, __UNKN, stopping_UNKN...\n",
       "39023               [равный_A, колво_S, столбец_S, чтоль_S]\n",
       "15381     [привет_S, дешево_ADV, арендовать_V, сервер_S,...\n",
       "28059     [прочитывать_V, читать_V, hacking_UNKN, guide_...\n",
       "74909               [интригантка_S, чертов_A, искатьр_UNKN]\n",
       "40571     [ребята_S, правильно_ADV, писаться_V, полность...\n",
       "60915     [почему_ADVPRO, заставлять_V, страдать_V, самы...\n",
       "91119     [отношение_S, штамп_S, заканчиваться_V, это_PA...\n",
       "865       [жак_S, фреско_ADV, хороший_A, купить_V, деньг...\n",
       "15046     [м32_UNKN, познакомиться_V, дж_UNKN, 066_UNKN,...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train['text'].loc[:15046].apply(text_prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train_w2v_vector[6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Union\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e756e730b1740e98f242871cef58ec5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=112467), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a07da65b6f3f462aa5b057f8fffa755d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=89973), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00d8657f08f94d44b878d6119319729c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=22494), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "fun = FeatureUnion(\n",
    "        [\n",
    "            ('tf-idf', idf_vectorizer),\n",
    "            ('word2vec', word2vec_vectorizer),\n",
    "        ]\n",
    "    )\n",
    "fun = fun.fit(tqdm.tqdm_notebook(pd.concat([df_train['text']]))) #,df_test['text']\n",
    "X_train_fun = fun.transform(tqdm.tqdm_notebook(X_train['text']))\n",
    "X_test_fun = fun.transform(tqdm.tqdm_notebook(X_test['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2b7c45113ed43b6babd537d03f021bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=224933), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c47ed744902b483e830c9f27c9cd0ee3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=112467), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b6b99c6aa104052b919deeaf19d3c99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=112466), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# FInal version\n",
    "\n",
    "fun2 = FeatureUnion(\n",
    "        [\n",
    "            ('tf-idf', idf_vectorizer),\n",
    "            ('word2vec', word2vec_vectorizer),\n",
    "        ]\n",
    "    )\n",
    "fun2 = fun2.fit(tqdm.tqdm_notebook(pd.concat([df_train['text'],df_test['text']]))) #,df_test['text']\n",
    "fin_train_fun = fun2.transform(tqdm.tqdm_notebook(df_train['text']))\n",
    "fin_test_fun = fun2.transform(tqdm.tqdm_notebook(df_test['text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_X_train_idf_vector = hstack((X_train_idf_vector,np.array(X_train['have_symbols'])[:,None]))\n",
    "new_X_test_idf_vector = hstack((X_test_idf_vector,np.array(X_test['have_symbols'])[:,None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_X_train_fun = hstack((X_train_fun,X_train[['have_symbols','have_number','have_eng','have_upper']].values))\n",
    "new_X_test_fun = hstack((X_test_fun,X_test[['have_symbols','have_number','have_eng','have_upper']].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0],\n",
       "       [1, 0, 1, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 0, 1, 0]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[['have_symbols','have_number','have_eng','have_upper']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5774018004541697"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(new_X_train_idf_vector.getrow(1).toarray()[0][:52000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final prediction\n",
    "fin_train_idf_vector = hstack((fin_train_idf_vector,df_train[['have_symbols','have_number','have_eng','have_upper']].values))\n",
    "fin_test_idf_vector = hstack((fin_test_idf_vector,df_test[['have_symbols','have_number','have_eng','have_upper']].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final prediction\n",
    "fin_train_fun = hstack((fin_train_fun,df_train[['have_symbols','have_number','have_eng','have_upper']].values))\n",
    "fin_test_fun = hstack((fin_test_fun,df_test[['have_symbols','have_number','have_eng','have_upper']].values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = StratifiedKFold(n_splits=5, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = MLPClassifier((256,100),random_state=20,verbose=True,max_iter=1,learning_rate_init=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.31952142\n",
      "Iteration 2, loss = 0.21086383\n",
      "Iteration 3, loss = 0.16021787\n",
      "Iteration 4, loss = 0.12032247\n",
      "Iteration 5, loss = 0.08934619\n",
      "Iteration 6, loss = 0.07349143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dmitriy/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning:\n",
      "\n",
      "Stochastic Optimizer: Maximum iterations (6) reached and the optimization hasn't converged yet.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8651809400734689"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.fit(X_train_fun,y_train)\n",
    "y_pred = nn.predict(X_test_fun)\n",
    "f1_score(y_test,y_pred,average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8857310140234388"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(cross_val_score(nn,new_X_train_fun,y_train,scoring='f1_weighted',n_jobs=-1,cv=cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.30896300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dmitriy/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning:\n",
      "\n",
      "Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Final version\n",
    "nn.fit(fin_train_fun,df_train['label'])\n",
    "y_fin_pred = nn.predict(fin_test_fun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2, ..., 2, 2, 0])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_fin_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC(verbose=True,cache_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dmitriy/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:196: FutureWarning:\n",
      "\n",
      "The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dmitriy/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning:\n",
      "\n",
      "F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.27831785055244673"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc.fit(X_train_idf_vector,y_train)\n",
    "y_pred = svc.predict(X_test_idf_vector)\n",
    "f1_score(y_test,y_pred,average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cross_val_score(svc,X_train_idf_vector,y_train,scoring='f1_weighted',n_jobs=-1,cv=cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit = LogisticRegression(n_jobs=-1, random_state=20,C=1,multi_class='multinomial',solver='lbfgs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='multinomial',\n",
       "          n_jobs=-1, penalty='l2', random_state=20, solver='lbfgs',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit.fit(X_train_fun,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.875766148495799"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = logit.predict(X_test_fun)\n",
    "f1_score(y_test,y_pred,average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2, ..., 2, 1, 0])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [112467, 89973]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-02d580bab680>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogit\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfin_train_fun\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'f1_weighted'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    400\u001b[0m                                 \u001b[0mfit_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m                                 \u001b[0mpre_dispatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m                                 error_score=error_score)\n\u001b[0m\u001b[1;32m    403\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcv_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test_score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m     \"\"\"\n\u001b[0;32m--> 225\u001b[0;31m     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_cv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mindexable\u001b[0;34m(*iterables)\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m             \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 235\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [112467, 89973]"
     ]
    }
   ],
   "source": [
    "np.mean(cross_val_score(logit,new_X_train_fun,y_train,scoring='f1_weighted',n_jobs=-1,cv=cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final logit\n",
    "logit.fit(fin_train_fun,df_train['label'])\n",
    "y_fin_pred = logit.predict(fin_test_fun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Инициализируем lightGBM\n",
    "lgbcl = lgb.LGBMClassifier(\n",
    "        #num_leaves=31,\n",
    "        #max_depth = 7,\n",
    "        learning_rate=0.5,\n",
    "        n_estimators=800,\n",
    "        #subsample=.9,\n",
    "        #colsample_bytree=.9,\n",
    "        random_state=20,\n",
    "        n_jobs=-1,\n",
    "        objective='multiclass',\n",
    "        num_class=3\n",
    "    )\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's multi_logloss: 0.334903\n",
      "[200]\tvalid_0's multi_logloss: 0.335143\n",
      "Early stopping, best iteration is:\n",
      "[171]\tvalid_0's multi_logloss: 0.333562\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
       "        importance_type='split', learning_rate=0.5, max_depth=-1,\n",
       "        min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
       "        n_estimators=800, n_jobs=-1, num_class=3, num_leaves=31,\n",
       "        objective='multiclass', random_state=20, reg_alpha=0.0,\n",
       "        reg_lambda=0.0, silent=True, subsample=1.0,\n",
       "        subsample_for_bin=200000, subsample_freq=0)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Обучаем классификатор\n",
    "lgbcl.fit(\n",
    "         X_train_fun,  y_train,\n",
    "        eval_set=[( X_test_fun, y_test)],\n",
    "        early_stopping_rounds=100,\n",
    "    #categorical_feature=cat_cols,\n",
    "        verbose=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8527844966923913"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = lgbcl.predict(X_test_fun)\n",
    "f1_score(y_test,y_pred,average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8546487426045941"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(cross_val_score(lgbcl,X_train_fun,y_train,scoring='f1_weighted',n_jobs=-1,cv=cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
       "        importance_type='split', learning_rate=0.5, max_depth=7,\n",
       "        min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
       "        n_estimators=800, n_jobs=-1, num_class=3, num_leaves=31,\n",
       "        objective='multiclass', random_state=20, reg_alpha=0.0,\n",
       "        reg_lambda=0.0, silent=True, subsample=1.0,\n",
       "        subsample_for_bin=200000, subsample_freq=0)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Обучаем классификатор\n",
    "lgbcl.fit(\n",
    "        fin_train_idf_vector,  df_train['label'],\n",
    "        #eval_set=[( X_test_idf_vector, y_test)],\n",
    "        #early_stopping_rounds=100,\n",
    "    #categorical_feature=cat_cols,\n",
    "        verbose=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_fin_pred = lgbcl.predict(fin_test_idf_vector)\n",
    "#f1_score(y_test,y_pred,average='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final save to csv\n",
    "submit = pd.read_csv('sample.csv', encoding='utf-8',sep=',',index_col='id')\n",
    "submit['label'] = y_fin_pred\n",
    "submit.to_csv('predict/union_nn_256_100_1epoch_0.001_4features.csv', encoding='utf-8',sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
