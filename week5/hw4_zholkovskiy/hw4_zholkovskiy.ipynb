{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import os, sys\n",
    "import logging\n",
    "import tensorflow as tf\n",
    "import tf_metrics\n",
    "import faiss\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vectorizer import Vectorizer\n",
    "from triplet_loss import batch_all_triplet_loss, batch_hard_triplet_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(norm=True, to_subm=False):\n",
    "    vect = Vectorizer()\n",
    "\n",
    "    data_train = pd.read_csv('./data/train.csv', index_col='index')\n",
    "    n_classes = len(np.unique(data_train.labels.values)) - 1\n",
    "\n",
    "    x_train_FULL = vect.transform(tqdm(data_train.text))\n",
    "    y_train_FULL = data_train.labels.values\n",
    "\n",
    "    np.random.seed(42)\n",
    "    msk = np.random.random(len(data_train)) > 0.15\n",
    "    data_val = data_train[~msk]\n",
    "\n",
    "    data_train = data_train[msk]\n",
    "\n",
    "    data_test = pd.read_csv('./data/test.csv', engine='python', index_col='index')\n",
    "\n",
    "    x_train = vect.transform(tqdm(data_train.text))\n",
    "    y_train = data_train.labels.values\n",
    "\n",
    "    x_val = vect.transform(tqdm(data_val.text))\n",
    "    y_val = data_val.labels.values\n",
    "\n",
    "    x_test = vect.transform(data_test.text)\n",
    "    \n",
    "    if to_subm:\n",
    "        x_train = x_train_FULL\n",
    "        y_train = y_train_FULL\n",
    "        \n",
    "    if norm:\n",
    "        x_train = normalize(x_train)\n",
    "        x_val = normalize(x_val)\n",
    "        x_test = normalize(x_test)\n",
    "        \n",
    "    return (x_train, y_train), (x_val, y_val), x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e38c9a92cbc42979fca628b98815735",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=150832), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b783c72b4e5f4662804d38c28ce73c6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=128212), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f918dfcc3bd4062af267e5ed05f6d31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=22620), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_val, y_val), x_test = get_data(norm=True, to_subm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parser(x, y):\n",
    "    features = {\"x\": x}\n",
    "    return features, y\n",
    "\n",
    "def train_input_fn(params):\n",
    "    def train_get():\n",
    "        \n",
    "        train_classes = np.unique(y_train)[1:]\n",
    "        len_train = len(y_train)\n",
    "        batch_size = params['batch_size']\n",
    "        batch_n = int(len_train / batch_size)\n",
    "        \n",
    "        \n",
    "        cl_ids = defaultdict(list)\n",
    "        for j, cl in enumerate(y_train):\n",
    "            cl_ids[cl].append(j)\n",
    "        \n",
    "        for n in range(batch_n):\n",
    "            batch_classes = np.random.choice(train_classes, size=batch_size // 3, replace=False)\n",
    "\n",
    "            #class_idx = [j for j, cl in enumerate(y_train) if cl in batch_classes] # this is super slow\n",
    "            class_idx = []\n",
    "            for cl in batch_classes:\n",
    "                class_idx.extend(cl_ids[cl])\n",
    "            other_idx = list(set(range(len_train)) - set(class_idx))\n",
    "            add_idx = list(np.random.choice(other_idx, size=(batch_size-len(class_idx)), replace=False))\n",
    "            batch_idx = class_idx + add_idx\n",
    "            yield ({'x': x_train[batch_idx]}, y_train[batch_idx])\n",
    "\n",
    "    dataset =  tf.data.Dataset.from_generator(train_get, ({'x': tf.float32}, tf.int64), output_shapes=({'x': [None, 300]}, [None]))\n",
    "    dataset = dataset.repeat()\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    return iterator.get_next()\n",
    "\n",
    "def eval_input_fn(params):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "    dataset = dataset.batch(params['batch_size'])\n",
    "    dataset = dataset.map(parser)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    return iterator.get_next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_inf_input_fn(params):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "    dataset = dataset.batch(params['batch_size'])\n",
    "    dataset = dataset.map(parser)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    return iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(estimator):\n",
    "    val_embs = np.array([x['emb'] for x in estimator.predict(eval_input_fn)])\n",
    "    train_embs = np.array([x['emb'] for x in estimator.predict(train_inf_input_fn)])\n",
    "    \n",
    "    emb_size = train_embs.shape[1]\n",
    "    index = faiss.IndexFlat(emb_size)\n",
    "\n",
    "    index.verbose = True  # to see progress\n",
    "    index.add(normalize(train_embs))\n",
    "\n",
    "    D, I = index.search(normalize(val_embs), k=1)\n",
    "    I = I.ravel()\n",
    "    D = D.ravel()\n",
    "    \n",
    "    y_val_gt = np.array([y if y in y_train else -1 for y in y_val])\n",
    "\n",
    "    f = []\n",
    "    thrs = np.linspace(0, 1, 100)\n",
    "    for thr in tqdm(thrs):\n",
    "        y_pred = [y_train[i] if d < thr else -1 for (i, d) in zip(I, D)]\n",
    "\n",
    "        f.append(f1_score(y_val_gt, y_pred, average='macro'))\n",
    "    return max(f), thrs[np.argmax(f)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValidationHook(tf.train.SessionRunHook):\n",
    "    def __init__(self, model_fn, params, checkpoint_dir,\n",
    "                 every_n_secs=None, every_n_steps=None):\n",
    "        self._iter_count = 0\n",
    "        self._estimator = tf.estimator.Estimator(\n",
    "            model_fn=model_fn,\n",
    "            params=params,\n",
    "            model_dir=checkpoint_dir\n",
    "        )\n",
    "        self._timer = tf.train.SecondOrStepTimer(every_n_secs, every_n_steps)\n",
    "        self._should_trigger = False\n",
    "        self.summary_writer = tf.summary.FileWriter(checkpoint_dir)\n",
    "\n",
    "    def begin(self):\n",
    "        self._timer.reset()\n",
    "        self._iter_count = 0\n",
    "\n",
    "    def before_run(self, run_context):\n",
    "        self._should_trigger = self._timer.should_trigger_for_step(self._iter_count)\n",
    "\n",
    "    def after_run(self, run_context, run_values):\n",
    "        if self._should_trigger:\n",
    "            f1 = evaluate(self._estimator)\n",
    "            print (f1)\n",
    "            summary = tf.Summary()\n",
    "            summary.value.add(tag='f1', simple_value = f1)\n",
    "            self.summary_writer.add_summary(summary, self._estimator.get_variable_value('global_step'))\n",
    "            \n",
    "            self._timer.update_last_triggered_step(self._iter_count)\n",
    "        self._iter_count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dir = './models/'\n",
    "logs_dir = './logs/'\n",
    "\n",
    "def train_model(forward_fn, name, epochs=10, batch_size=100):\n",
    "    params = {\n",
    "        'batch_size': batch_size,\n",
    "        'train_size': x_train.shape[0],\n",
    "        'val_size': x_test.shape[0],\n",
    "        'num_epochs': epochs,\n",
    "        'forward': forward_fn,\n",
    "    }\n",
    "    model_dir = os.path.join(models_dir, name)\n",
    "    log_dir = os.path.join(logs_dir, name)\n",
    "    logger = logging.getLogger('tensorflow')\n",
    "    logger.handlers = []\n",
    "\n",
    "    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "    log_file = os.path.join(logs_dir, '{}.log'.format(name))\n",
    "    print ('log output: {}'.format(log_file))\n",
    "    \n",
    "    fh = logging.FileHandler(log_file)\n",
    "    fh.setLevel(logging.DEBUG)\n",
    "    fh.setFormatter(formatter)\n",
    "    logger.addHandler(fh)\n",
    "\n",
    "    sh = logging.StreamHandler(sys.stdout)\n",
    "    sh.setLevel(logging.CRITICAL)\n",
    "    logger.addHandler(sh)\n",
    "    \n",
    "    config = tf.estimator.RunConfig(\n",
    "        save_checkpoints_steps = int(params['train_size'] / params['batch_size']),\n",
    "        model_dir = model_dir\n",
    "    )\n",
    "    classifier = tf.estimator.Estimator(\n",
    "        model_fn = model_fn,\n",
    "        params = params,\n",
    "        config = config,\n",
    "    )\n",
    "    \n",
    "    train_spec = tf.estimator.TrainSpec(\n",
    "        input_fn = lambda: train_input_fn(params), \n",
    "        max_steps=int(params['train_size'] / params['batch_size'] * params['num_epochs'])\n",
    "    )\n",
    "\n",
    "    eval_spec = tf.estimator.EvalSpec(\n",
    "        input_fn = lambda: eval_input_fn(params),\n",
    "        steps = int(params['val_size'] / params['batch_size']),\n",
    "        throttle_secs = 0, \n",
    "#         hooks = [ValidationHook(\n",
    "#             model_fn = model_fn,\n",
    "#             params = params,\n",
    "#             checkpoint_dir = model_dir,\n",
    "#             every_n_steps = int(params['train_size'] / params['batch_size'] * 10)\n",
    "#         )]\n",
    "    )\n",
    "    try:\n",
    "        val_scores = tf.estimator.train_and_evaluate(\n",
    "            classifier, \n",
    "            train_spec, \n",
    "            eval_spec\n",
    "        )\n",
    "    except KeyboardInterrupt:\n",
    "        print('Interrupted')\n",
    "\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn(features, labels, mode, params):   \n",
    "    emb = params['forward'](features)\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        predictions = {\n",
    "            'emb': emb,\n",
    "          #  'prob': prob\n",
    "        }\n",
    "        return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
    "\n",
    "    loss = batch_hard_triplet_loss(labels, emb, 1, 'l2')\n",
    "    if type(loss) == tuple:\n",
    "        loss = loss[0]\n",
    "        \n",
    "    if mode == tf.estimator.ModeKeys.EVAL:\n",
    "        \n",
    "        return tf.estimator.EstimatorSpec(\n",
    "            mode, loss=loss)\n",
    "        \n",
    "    optimizer = tf.train.AdagradOptimizer(learning_rate=0.3)\n",
    "    train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())\n",
    "    \n",
    "    return tf.estimator.EstimatorSpec(\n",
    "        mode, loss=loss, train_op=train_op)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_00(features):\n",
    "    inputs = features['x']\n",
    "    print ('*')\n",
    "    out = tf.layers.dense(inputs, 512, activation=tf.nn.tanh)\n",
    "    out = tf.layers.dropout(out)\n",
    "    out = tf.layers.batch_normalization(out)\n",
    "\n",
    "\n",
    "    emb = tf.layers.dense(out, units=512)\n",
    "\n",
    "    return emb#, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log output: ./logs/l2_normed.log\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n"
     ]
    }
   ],
   "source": [
    "estimator = train_model(forward_00, 'l2_normed', epochs=50, batch_size=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*\n",
      "*\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "091b4c0804cb4e7e980da69ab11d7153",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5757690006395229, 0.27272727272727276)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(estimator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# make submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_full_inf_input_fn(params):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "    dataset = dataset.batch(params['batch_size'])\n",
    "    dataset = dataset.map(parser)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    return iterator.get_next()\n",
    "\n",
    "def test_inf_input_fn(params):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x_test, np.zeros_like(x_test)))\n",
    "    dataset = dataset.batch(params['batch_size'])\n",
    "    dataset = dataset.map(parser)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    return iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*\n",
      "*\n"
     ]
    }
   ],
   "source": [
    "test_embs = np.array([x['emb'] for x in estimator.predict(test_inf_input_fn)])\n",
    "train_FULL_embs = np.array([x['emb'] for x in estimator.predict(train_full_inf_input_fn)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating KNN\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "emb_size = train_FULL_embs.shape[1]\n",
    "index = faiss.IndexFlat(emb_size)\n",
    "\n",
    "index.verbose = True  # to see progress\n",
    "index.add(normalize(train_FULL_embs))\n",
    "\n",
    "print('Calculating KNN')\n",
    "D, I = index.search(normalize(test_embs), k=1)\n",
    "I = I.ravel()\n",
    "D = D.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_thr = 0.273\n",
    "y_subm = [y_train[i] if d < best_thr else -1 for (i, d) in zip(I, D)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./subm/1.csv', 'w') as f:\n",
    "    f.write('index,labels\\n' )\n",
    "    for j, y in enumerate(y_subm):\n",
    "        f.write('{},{}\\n'.format(j,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
